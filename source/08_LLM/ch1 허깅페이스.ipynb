{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4d09903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:99% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.input_prompt{padding:0px;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:24pt;}\n",
       "div.text_cell_render.rendered_html{font-size:20pt;}\n",
       "div.text_cell_render ul li, div.text_cell_render ol li p, code{font-size:22pt; line-height:30px;}\n",
       "div.output {font-size:24pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:24pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{font-size:24pt;padding:5px;}\n",
       "table.dataframe{font-size:24px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:99% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.input_prompt{padding:0px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:24pt;}\n",
    "div.text_cell_render.rendered_html{font-size:20pt;}\n",
    "div.text_cell_render ul li, div.text_cell_render ol li p, code{font-size:22pt; line-height:30px;}\n",
    "div.output {font-size:24pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:24pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:24pt;padding:5px;}\n",
    "table.dataframe{font-size:24px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17948ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import logging\n",
    "# 경고 제거\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# transformers 로깅 레벨 조정\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# Hugging Face symlink 경고 제거\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# from transformers import pipeline, logging as hf_logging\n",
    "# hf_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8996e87",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">ch1. 허깅페이스</span>\n",
    "- Inference API 이용 : 모델의 결과를 surver에서\n",
    "- pipeline()이용 : 모델을 다운로드받아 모델의 결과를 local에서\n",
    "    * raw text -> tokenizer -> model -> [0.11, 0.55, 0.xx, ~] logits값으로 prediction 결과 출력\n",
    "```\n",
    "허깅페이스 transformers에서 지원하는 task\n",
    "\"sentiment-analysis\" : \"text-classification\"의 별칭(감정분석 전용으로 사용)\n",
    "\"text-classification\" : 감정분석, 뉴스분류, 리뷰 분류 등 일반적인 문장 분류\n",
    "\"zero-shot-classification\" : 레이블을 학습 없이 주어진 후보군 중에서 분류\n",
    "\"token-classification\" : 개체명 인식(NER : Named Entity Recognition) 등 단위 라벨링\n",
    "\"ner\" : \"token-classification\"의 별칭\n",
    "\"fill-mask\": 빈칸 채우기 \n",
    "\"text-generation\" : 텍스트 생성 (GPT류 모델에 사용)\n",
    "\"text2text-generation\" : 번역, 요약 등 입력 -> 출력 변환\n",
    "\"translation\" : 번역\n",
    "\"summerization\" : 텍스트요약\n",
    "\"question-answering\" : 주어진 context를 보고 질문에 답하기.\n",
    "\"image-to-text\" : 그림을 설명\n",
    "\"image-classification\" : 이미지 분류\n",
    "\n",
    "```\n",
    "\n",
    "## 1. 텍스트 기반 감정분석(긍정/부정)\n",
    "- C:\\Users\\Admin\\.cache\\huggingface\\hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "207a65b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9333938956260681}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(task=\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cfc601c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9982663989067078}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(task=\"text-classification\",\n",
    "                     model = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# 감정분석시 내용이 많으면 list로\n",
    "classifier([\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "    \"I hate this so much!\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "068b68e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.857815682888031}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"이 영화 정말 최고였어요. 감동적이고 연기가 대단해\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af06a9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.8577604293823242}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"이 물건 정말 사고 싶어요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fc7d497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998695850372314},\n",
       " {'label': 'NEGATIVE', 'score': 0.9991129040718079},\n",
       " {'label': 'NEGATIVE', 'score': 0.599323034286499},\n",
       " {'label': 'POSITIVE', 'score': 0.5370621085166931}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier([\"I like you\", \"I hate you\", \"나 너가 싫어\", \"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a44b5b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(task = \"sentiment-analysis\",\n",
    "                     model=\"matthewburke/korean_sentiment\")\n",
    "texts = ['나는 너가 좋아', \"당신이 싫어요\", \"힘들어요\",\"오늘 기분이 최고야\"]\n",
    "\n",
    "result = classifier(texts)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0965d2a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "654c5265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9557897448539734},\n",
       " {'label': 'LABEL_0', 'score': 0.9092598557472229},\n",
       " {'label': 'LABEL_0', 'score': 0.9140233397483826},\n",
       " {'label': 'LABEL_1', 'score': 0.9714491367340088}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b11d335c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9557897448539734},\n",
       " {'label': 'LABEL_0', 'score': 0.9092598557472229},\n",
       " {'label': 'LABEL_0', 'score': 0.9140233397483826},\n",
       " {'label': 'LABEL_1', 'score': 0.9714491367340088}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(task=\"sentiment-analysis\",\n",
    "                     model=\"matthewburke/korean_sentiment\")\n",
    "classifier(['나는 너가 좋아', \"당신이 싫어요\", \"힘들어요\", \"오늘 기분이 최고야\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2db9bfe",
   "metadata": {},
   "source": [
    "## 2. 제로샷분류(Zero-shot분류)\n",
    "- 기계학습 및 자연어처리에서 각 개별 작업에 대한 특정 교육없이 작업을 수행할 수 있는 모형(비지도학습)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1202e756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c26fa3d15cc4890b5aeb9272db23142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1dc1f9851194194ba7bd4c1681a079b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfbc83882cc433d81051d1a9938934c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a19fb844de4258b8fe8b4552dd4cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d209664e5f48338e07939d3f618e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18900676ce4a496fbf926df7ce9bc17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'I have a problem with my iphone that needs to be resolved asap!',\n",
       " 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'],\n",
       " 'scores': [0.5227580070495605,\n",
       "  0.45814019441604614,\n",
       "  0.0142647260800004,\n",
       "  0.0026850001886487007,\n",
       "  0.002152054337784648]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "classifier(\n",
    "    \"I have a problem with my iphone that needs to be resolved asap!\",\n",
    "    candidate_labels=[\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9037d9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'One day i will see the world',\n",
       " 'labels': ['travel', 'dancing', 'cooking'],\n",
       " 'scores': [0.9939888715744019, 0.0032361410558223724, 0.002775018336251378]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_to_classify = \"One day i will see the world\"\n",
    "candidate_labels = ['travel', 'cooking', 'dancing']\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9108ef",
   "metadata": {},
   "source": [
    "# 3. text 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa46284c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456fc42cd06445efa0248a87fce1d2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6cd3bb7484a44399b2bf00008f8c646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fea4fd5af824c63aefee1b58f8a3200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d853bcdf0f432fab0ac453879fdbf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d51133196c432d854e761cf3978cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e434e355b4a5408bb109dadb7f5b19bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2014f87e416d4e528cab279d2d69e0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'in this course. We will teach you how to create and sell your own digital products, to build a business where you can sell your own products, and to learn how to grow your business.\\n\\nWe will teach you how to create and sell your own digital products, to build a business where you can sell your own products, and to learn how to grow your business. We will provide you with the tools you need to become a successful entrepreneur. You will learn the basic principles of how to build a business and how to get it done.\\n\\nYou will learn the basic principles of how to build a business and how to get it done. You will learn the basics of how to build a business and how to get it done. We will show you how to create a business from the ground up and how you can create a business from the ground up and how you can build a company.\\n\\nWe will show you how to create a business from the ground up and how you can create a company from the ground up and how you can build a company. We will demonstrate how to build a company from the ground up and how you can build a company from the ground up.\\n\\nHow to build your own online business with Google\\n\\nWe will show you how to build your own online business with Google.'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline, set_seed\n",
    "# set_seed(2)\n",
    "generation = pipeline(\"text-generation\", \"gpt2\") # 텍스트 생성 gpt3부터는 허깅페이스없음\n",
    "generation(\n",
    "    \"in this course. We will teach you how to\",\n",
    "    pad_token_id=generation.tokenizer.eos_token_id\n",
    ") # pad_token_id 경고를 없애려고 setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b3a4b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "480bf772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6107d4a82ff4c0bb5d791248ea85c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa7dfe2eec946e9be4fa2e65b37a8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5e385e825a4c0da270b7f9575a10b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3a2ae74e0b49f8bcb2891da4e1a320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['no_repeat_ngram_szie'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      2\u001b[0m generation \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskt/kogpt2-base-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgeneration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m이 과정은 다음과 같은 방법을 알려드려요. \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 생성할 최대 길이(생성할 토큰 수)\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 생성할 문장 갯수\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# 다양할 샘플 사용\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# top-k 샘플링(확률 높은 상위 50개 토큰만 사용)\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# 창의성 조절(낮을 수록 보수적)\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_repeat_ngram_szie\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 반복 방지\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:332\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\transformers\\pipelines\\base.py:1467\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1460\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1461\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1464\u001b[0m         )\n\u001b[0;32m   1465\u001b[0m     )\n\u001b[0;32m   1466\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\transformers\\pipelines\\base.py:1474\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1473\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1474\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1475\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1476\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\transformers\\pipelines\\base.py:1374\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1372\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1373\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1374\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1375\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:432\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[0;32m    430\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m--> 432\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[0;32m    435\u001b[0m     generated_sequence \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\transformers\\generation\\utils.py:2388\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2384\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2385\u001b[0m     \u001b[38;5;66;03m# type() required to access the unbound class-level method\u001b[39;00m\n\u001b[0;32m   2386\u001b[0m     decoding_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m), GENERATION_MODES_MAPPING[generation_mode])\n\u001b[1;32m-> 2388\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2389\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_generation_mode(generation_mode, generation_config, generation_mode_kwargs)\n\u001b[0;32m   2391\u001b[0m \u001b[38;5;66;03m# Deprecation-related step: set Hub repo for deprecated strategies.\u001b[39;00m\n\u001b[0;32m   2392\u001b[0m \u001b[38;5;66;03m# NOTE: This must come after initializing generation_config, since we need it to determine if this is a deprecated mode.\u001b[39;00m\n\u001b[0;32m   2393\u001b[0m \u001b[38;5;66;03m# It must also be before any preparation steps, since Hub repos expect to be loaded before preparation steps.\u001b[39;00m\n\u001b[0;32m   2394\u001b[0m \u001b[38;5;66;03m# TODO joao, manuel: remove this in v4.62.0\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\transformers\\generation\\utils.py:1599\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[1;34m(self, model_kwargs)\u001b[0m\n\u001b[0;32m   1596\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[0;32m   1598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[1;32m-> 1599\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1600\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1601\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1602\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['no_repeat_ngram_szie'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generation = pipeline(\"text-generation\", \"skt/kogpt2-base-v2\")\n",
    "result = generation(\n",
    "    \"이 과정은 다음과 같은 방법을 알려드려요. \",\n",
    "    pad_token_id = generation.tokenizer.eos_token_id,\n",
    "    max_new_tokens = 100, # 생성할 최대 길이(생성할 토큰 수)\n",
    "    num_return_sequences=1, # 생성할 문장 갯수\n",
    "    do_sample=True,          # 다양할 샘플 사용\n",
    "    top_k = 50,         # top-k 샘플링(확률 높은 상위 50개 토큰만 사용)\n",
    "    top_p = 0.95,\n",
    "    temperature = 1.2,           # 창의성 조절(낮을 수록 보수적)\n",
    "    no_repeat_ngram_szie = 2 # 반복 방지\n",
    ")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bb8df2",
   "metadata": {},
   "source": [
    "## 4. 마스크(빈칸)채우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "64ae707e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.19275707006454468,\n",
       "  'token': 3299,\n",
       "  'token_str': ' doctor',\n",
       "  'sequence': \"I'm going to hospital and meet a doctor\"},\n",
       " {'score': 0.06794589757919312,\n",
       "  'token': 27321,\n",
       "  'token_str': ' psychiatrist',\n",
       "  'sequence': \"I'm going to hospital and meet a psychiatrist\"}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(task='fill-mask',\n",
    "                   model='distilbert/distilroberta-base') # 마스크 채우기\n",
    "unmasker(\"I'm going to hospital and meet a <mask>\", top_k=2) # top_k 기본값 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1ceae8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.0629730075597763,\n",
       "  'token': 265,\n",
       "  'token_str': ' business',\n",
       "  'sequence': \"Hello, I'm a business model.\"},\n",
       " {'score': 0.038101598620414734,\n",
       "  'token': 18150,\n",
       "  'token_str': ' freelance',\n",
       "  'sequence': \"Hello, I'm a freelance model.\"},\n",
       " {'score': 0.03764132782816887,\n",
       "  'token': 774,\n",
       "  'token_str': ' role',\n",
       "  'sequence': \"Hello, I'm a role model.\"},\n",
       " {'score': 0.037326786667108536,\n",
       "  'token': 2734,\n",
       "  'token_str': ' fashion',\n",
       "  'sequence': \"Hello, I'm a fashion model.\"},\n",
       " {'score': 0.026023676618933678,\n",
       "  'token': 24526,\n",
       "  'token_str': ' Playboy',\n",
       "  'sequence': \"Hello, I'm a Playboy model.\"}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "unmasker(\"Hello, I'm a <mask> model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "92c5da1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.14130638539791107,\n",
       "  'token': 35,\n",
       "  'token_str': ':',\n",
       "  'sequence': '안녕하세요? 나는: 모델이예요.'},\n",
       " {'score': 0.1223798543214798,\n",
       "  'token': 116,\n",
       "  'token_str': '?',\n",
       "  'sequence': '안녕하세요? 나는? 모델이예요.'},\n",
       " {'score': 0.08188082277774811,\n",
       "  'token': 328,\n",
       "  'token_str': '!',\n",
       "  'sequence': '안녕하세요? 나는! 모델이예요.'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "unmasker(\"안녕하세요? 나는 <mask> 모델이예요.\", top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fdefb4",
   "metadata": {},
   "source": [
    " ## InferenceAPI 사용\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8d27f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "# os.environ['HF_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0352fd68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96886b46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed242257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c77e8c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d77924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dl-nlp(ipykernel)",
   "language": "python",
   "name": "ml-dl-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
