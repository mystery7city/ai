{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4decb793",
   "metadata": {},
   "source": [
    "# RAGAS Evaluation Notebook (Clean)\n",
    "\n",
    "This notebook evaluates **baseline vs experiment** RAG pipelines using RAGAS and saves **summary/detail** outputs per run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf76203-b1ff-4ee3-b17e-06004982adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdaa3541-ee50-497b-81cd-91c2274ba4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1b5f49e-4fde-42f4-a42a-40dd814c571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys, subprocess, textwrap\n",
    "\n",
    "# def sh(cmd):\n",
    "#     print(\">\", cmd)\n",
    "#     r = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "#     print(r.stdout)\n",
    "#     if r.stderr.strip():\n",
    "#         print(\"[stderr]\")\n",
    "#         print(r.stderr)\n",
    "\n",
    "# print(\"python:\", sys.executable)\n",
    "# print(\"version:\", sys.version)\n",
    "\n",
    "# # í˜„ì¬ íŒ¨í‚¤ì§€ ìƒíƒœ í™•ì¸\n",
    "# sh(\"python -c \\\"import numpy; print('numpy', numpy.__version__)\\\"\")\n",
    "# sh(\"python -c \\\"import pyarrow; print('pyarrow', pyarrow.__version__)\\\"\")\n",
    "# sh(\"python -c \\\"import datasets; print('datasets', datasets.__version__)\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b2f1ee4-078f-4f37-9e75-6da6b135d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys, subprocess\n",
    "\n",
    "# def pip(cmd):\n",
    "#     print(\">\", cmd)\n",
    "#     r = subprocess.run([sys.executable, \"-m\", \"pip\"] + cmd.split(), capture_output=True, text=True)\n",
    "#     print(r.stdout)\n",
    "#     if r.stderr.strip():\n",
    "#         print(\"[stderr]\")\n",
    "#         print(r.stderr)\n",
    "\n",
    "# # 1) ì œê±°\n",
    "# pip(\"uninstall -y pyarrow datasets numpy\")\n",
    "\n",
    "# # 2) ì¬ì„¤ì¹˜: numpy<2 + ìµœì‹  pyarrow + datasets(ë„ˆê°€ ì“°ë˜ ë²„ì „)\n",
    "# pip(\"install numpy<2 pyarrow>=14 datasets==2.19.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f6d4dab-9416-4c6e-b037-2526c17623ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ai\\source\\chatbot_app\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… METRICS: ['ContextPrecision', 'ContextRecall', 'Faithfulness']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15916\\1708373533.py:6: DeprecationWarning: Importing context_precision from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_precision\n",
      "  from ragas.metrics import context_precision, context_recall, faithfulness\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15916\\1708373533.py:6: DeprecationWarning: Importing context_recall from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_recall\n",
      "  from ragas.metrics import context_precision, context_recall, faithfulness\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15916\\1708373533.py:6: DeprecationWarning: Importing faithfulness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import faithfulness\n",
      "  from ragas.metrics import context_precision, context_recall, faithfulness\n"
     ]
    }
   ],
   "source": [
    "# ---- RAGAS metrics: version-tolerant loader ----\n",
    "def build_metrics():\n",
    "    # Aì•ˆ: embeddings ì˜ì¡´ ê°€ëŠ¥ì„±ì´ í° AnswerRelevancyëŠ” ë¹¼ê³  \"ì™„ì£¼\"ë¶€í„°\n",
    "    # 1) í•¨ìˆ˜í˜• metric\n",
    "    try:\n",
    "        from ragas.metrics import context_precision, context_recall, faithfulness\n",
    "        return [context_precision, context_recall, faithfulness]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) í´ë˜ìŠ¤í˜• metric\n",
    "    try:\n",
    "        from ragas.metrics import ContextPrecision, ContextRecall, Faithfulness\n",
    "        return [ContextPrecision(), ContextRecall(), Faithfulness()]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) fallback íƒìƒ‰\n",
    "    import ragas.metrics as m\n",
    "    wanted = [\"ContextPrecision\", \"ContextRecall\", \"Faithfulness\"]\n",
    "    found = []\n",
    "    for name in wanted:\n",
    "        if hasattr(m, name):\n",
    "            found.append(getattr(m, name)())\n",
    "    if found:\n",
    "        return found\n",
    "\n",
    "    raise ImportError(\n",
    "        \"RAGAS metrics import failed for A-plan (without AnswerRelevancy). \"\n",
    "        \"Paste `pip show ragas` and `python -c \\\"import ragas; print(ragas.__version__)\\\"`.\"\n",
    "    )\n",
    "\n",
    "METRICS = build_metrics()\n",
    "print(\"âœ… METRICS:\", [getattr(x, '__name__', x.__class__.__name__) for x in METRICS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24085385-5708-4be6-8d18-1c17f11f456d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ragas version: 0.4.3\n"
     ]
    }
   ],
   "source": [
    "import ragas\n",
    "print(\"ragas version:\", getattr(ragas, \"__version__\", \"unknown\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85cba8ec-b4a6-4dcf-8fda-13b78e62e686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… METRICS: ['ContextPrecision', 'ContextRecall', 'Faithfulness', 'AnswerRelevancy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15916\\3044559028.py:1: DeprecationWarning: Importing context_precision from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_precision\n",
      "  from ragas.metrics import context_precision, context_recall, faithfulness, answer_relevancy\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15916\\3044559028.py:1: DeprecationWarning: Importing context_recall from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_recall\n",
      "  from ragas.metrics import context_precision, context_recall, faithfulness, answer_relevancy\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15916\\3044559028.py:1: DeprecationWarning: Importing faithfulness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import faithfulness\n",
      "  from ragas.metrics import context_precision, context_recall, faithfulness, answer_relevancy\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15916\\3044559028.py:1: DeprecationWarning: Importing answer_relevancy from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import answer_relevancy\n",
      "  from ragas.metrics import context_precision, context_recall, faithfulness, answer_relevancy\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import context_precision, context_recall, faithfulness, answer_relevancy\n",
    "\n",
    "METRICS = [context_precision, context_recall, faithfulness, answer_relevancy]\n",
    "print(\"âœ… METRICS:\", [getattr(m, \"__name__\", m.__class__.__name__) for m in METRICS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ac8615-1f58-462f-8f2e-811d673d8d56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22cee99-4aaa-40fb-bbbb-451aa89c8c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53db9df-4693-41e1-b255-bebb90c4be53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "663e3c80",
   "metadata": {},
   "source": [
    "## 0) Environment & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3772574e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: C:\\ai\\source\\chatbot_app\n",
      "TESTSET_PATH: C:\\ai\\source\\chatbot_app\\ragas_testset_mini_0001.jsonl\n",
      "exists: True\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PATH CONFIG (only this cell is modified)\n",
    "# ============================================================\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys, importlib\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# âœ… í”„ë¡œì íŠ¸ ë£¨íŠ¸ (ìƒˆ ê²½ë¡œ)\n",
    "PROJECT_ROOT = Path(r\"C:\\ai\\source\\chatbot_app\")\n",
    "\n",
    "# âœ… ëª¨ë“ˆ ê²½ë¡œ (ì›ë˜ ì“°ë˜ êµ¬ì¡° ê·¸ëŒ€ë¡œ)\n",
    "MODULE_DIR = PROJECT_ROOT / \"modules\"\n",
    "\n",
    "# âœ… í™˜ê²½ë³€ìˆ˜\n",
    "ENV_PATH = PROJECT_ROOT / \".env\"\n",
    "\n",
    "# âœ… ê²°ê³¼ ì €ì¥ ë£¨íŠ¸\n",
    "RUNS_DIR = PROJECT_ROOT / \"results\" / \"ragas_runs\"\n",
    "\n",
    "# â­•ï¸ ì—¬ê¸°ì„œ ì–´ë–¤ í…ŒìŠ¤íŠ¸ì…‹ ì“¸ì§€ ë„¤ê°€ ì§ì ‘ ì„ íƒ\n",
    "# TESTSET_PATH = PROJECT_ROOT / \"ragas_testset_single.jsonl\"\n",
    "# TESTSET_PATH = PROJECT_ROOT / \"ragas_testset_v1_from_docx.jsonl\"\n",
    "TESTSET_PATH = PROJECT_ROOT / \"ragas_testset_mini_0001.jsonl\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# setup\n",
    "# ------------------------------------------------------------\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "sys.path.insert(0, str(MODULE_DIR))\n",
    "importlib.invalidate_caches()\n",
    "\n",
    "if ENV_PATH.exists():\n",
    "    load_dotenv(ENV_PATH)\n",
    "\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"TESTSET_PATH:\", TESTSET_PATH)\n",
    "print(\"exists:\", TESTSET_PATH.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc0093e",
   "metadata": {},
   "source": [
    "## 1) Load testset (JSONL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8108580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… rows: 3\n",
      "âœ… keys example: dict_keys(['question_id', 'question', 'ground_truth', 'reference'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mini-001</td>\n",
       "      <td>ì§‘ì£¼ì¸ì´ ì•„ë¬´ ë§ë„ ì•ˆ í–ˆëŠ”ë° ê³„ì•½ê¸°ê°„ì´ ëë‚¬ì–´ìš”. ì œê°€ ê³„ì† ì‚´ë©´ì„œ ì›”ì„¸ë„ ëƒˆë‹¤ë©´...</td>\n",
       "      <td>ì„ëŒ€ì°¨ ê¸°ê°„ ë§Œë£Œ í›„ ì„ì°¨ì¸ì´ ê³„ì† ê±°ì£¼í•˜ë©° ì°¨ì„ì„ ì§€ê¸‰í•˜ê³ , ì„ëŒ€ì¸ì´ ìƒë‹¹ ê¸°ê°„ ...</td>\n",
       "      <td>ì£¼íƒì„ëŒ€ì°¨ì—ì„œ ë¬µì‹œì  ê°±ì‹ ì˜ ìš”ê±´ê³¼ íš¨ê³¼, ê°±ì‹ ê±°ì ˆÂ·í•´ì§€ í†µì§€ ê¸°ê°„ ê·œì •</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mini-002</td>\n",
       "      <td>ì „ì…ì‹ ê³ ì™€ í™•ì •ì¼ìëŠ” ë°›ì•„ë‘ì—ˆëŠ”ë°, ì§‘ì£¼ì¸ì´ ê°™ì€ ì§‘ìœ¼ë¡œ ë‹¤ë¥¸ ì„ì°¨ì¸ê³¼ ì´ì¤‘ê³„ì•½ì„ ...</td>\n",
       "      <td>ìš°ì„ ë³€ì œê¶Œì€ ì£¼íƒì„ëŒ€ì°¨ì—ì„œ ëŒ€í•­ë ¥ê³¼ í™•ì •ì¼ìê°€ ê²°í•©ë˜ì–´ ì¸ì •ëœë‹¤. ëŒ€í•­ë ¥ì€ ì£¼íƒì˜ ...</td>\n",
       "      <td>ëŒ€í•­ë ¥Â·í™•ì •ì¼ìÂ·ìš°ì„ ë³€ì œê¶Œì˜ ë°œìƒ ìš”ê±´ê³¼ ê²½ë§¤ ë°°ë‹¹ ìˆœìœ„ì— ê´€í•œ ì£¼íƒì„ëŒ€ì°¨ ê·œì •</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mini-003</td>\n",
       "      <td>ì„ëŒ€ì°¨ ê³„ì•½ì„œì— íŠ¹ì•½ìœ¼ë¡œ ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­ì´ ìˆëŠ”ë°, ì´ë¥¼ ì–´ê¸°ê³  ë°˜ë ¤ë™ë¬¼ì„ í‚¤ìš°...</td>\n",
       "      <td>ê³„ì•½ì„œì˜ íŠ¹ì•½ì„ ìœ„ë°˜í–ˆë‹¤ê³  í•´ì„œ ê³§ë°”ë¡œ ê³„ì•½í•´ì§€ê°€ ì¸ì •ë˜ëŠ” ê²ƒì€ ì•„ë‹ˆë©°, ìœ„ë°˜ì˜ ì •...</td>\n",
       "      <td>ì„ëŒ€ì°¨ íŠ¹ì•½ ìœ„ë°˜ ì‹œ í•´ì§€ íŒë‹¨ ê¸°ì¤€(ì‹ ë¢°ê´€ê³„ íŒŒê´´)ê³¼ ë³´ì¦ê¸ˆ ë°˜í™˜Â·ì†í•´ë°°ìƒ ì›ì¹™</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question_id                                           question  \\\n",
       "0    mini-001  ì§‘ì£¼ì¸ì´ ì•„ë¬´ ë§ë„ ì•ˆ í–ˆëŠ”ë° ê³„ì•½ê¸°ê°„ì´ ëë‚¬ì–´ìš”. ì œê°€ ê³„ì† ì‚´ë©´ì„œ ì›”ì„¸ë„ ëƒˆë‹¤ë©´...   \n",
       "1    mini-002  ì „ì…ì‹ ê³ ì™€ í™•ì •ì¼ìëŠ” ë°›ì•„ë‘ì—ˆëŠ”ë°, ì§‘ì£¼ì¸ì´ ê°™ì€ ì§‘ìœ¼ë¡œ ë‹¤ë¥¸ ì„ì°¨ì¸ê³¼ ì´ì¤‘ê³„ì•½ì„ ...   \n",
       "2    mini-003  ì„ëŒ€ì°¨ ê³„ì•½ì„œì— íŠ¹ì•½ìœ¼ë¡œ ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­ì´ ìˆëŠ”ë°, ì´ë¥¼ ì–´ê¸°ê³  ë°˜ë ¤ë™ë¬¼ì„ í‚¤ìš°...   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0  ì„ëŒ€ì°¨ ê¸°ê°„ ë§Œë£Œ í›„ ì„ì°¨ì¸ì´ ê³„ì† ê±°ì£¼í•˜ë©° ì°¨ì„ì„ ì§€ê¸‰í•˜ê³ , ì„ëŒ€ì¸ì´ ìƒë‹¹ ê¸°ê°„ ...   \n",
       "1  ìš°ì„ ë³€ì œê¶Œì€ ì£¼íƒì„ëŒ€ì°¨ì—ì„œ ëŒ€í•­ë ¥ê³¼ í™•ì •ì¼ìê°€ ê²°í•©ë˜ì–´ ì¸ì •ëœë‹¤. ëŒ€í•­ë ¥ì€ ì£¼íƒì˜ ...   \n",
       "2  ê³„ì•½ì„œì˜ íŠ¹ì•½ì„ ìœ„ë°˜í–ˆë‹¤ê³  í•´ì„œ ê³§ë°”ë¡œ ê³„ì•½í•´ì§€ê°€ ì¸ì •ë˜ëŠ” ê²ƒì€ ì•„ë‹ˆë©°, ìœ„ë°˜ì˜ ì •...   \n",
       "\n",
       "                                       reference  \n",
       "0       ì£¼íƒì„ëŒ€ì°¨ì—ì„œ ë¬µì‹œì  ê°±ì‹ ì˜ ìš”ê±´ê³¼ íš¨ê³¼, ê°±ì‹ ê±°ì ˆÂ·í•´ì§€ í†µì§€ ê¸°ê°„ ê·œì •  \n",
       "1   ëŒ€í•­ë ¥Â·í™•ì •ì¼ìÂ·ìš°ì„ ë³€ì œê¶Œì˜ ë°œìƒ ìš”ê±´ê³¼ ê²½ë§¤ ë°°ë‹¹ ìˆœìœ„ì— ê´€í•œ ì£¼íƒì„ëŒ€ì°¨ ê·œì •  \n",
       "2  ì„ëŒ€ì°¨ íŠ¹ì•½ ìœ„ë°˜ ì‹œ í•´ì§€ íŒë‹¨ ê¸°ì¤€(ì‹ ë¢°ê´€ê³„ íŒŒê´´)ê³¼ ë³´ì¦ê¸ˆ ë°˜í™˜Â·ì†í•´ë°°ìƒ ì›ì¹™  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TESTSET_JSONL = PROJECT_ROOT / \"ragas_testset_mini_0001.jsonl\"  # change if needed\n",
    "assert TESTSET_JSONL.exists(), f\"âŒ JSONL not found: {TESTSET_JSONL}\"\n",
    "\n",
    "rows = []\n",
    "with open(TESTSET_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        rows.append(json.loads(line))\n",
    "\n",
    "print(\"âœ… rows:\", len(rows))\n",
    "print(\"âœ… keys example:\", rows[0].keys())\n",
    "pd.DataFrame(rows[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347b7dac-4985-472c-a539-581bb0105eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd5607-f5ef-4c8c-a324-21e45c1831e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec1529b4",
   "metadata": {},
   "source": [
    "## 2) Define baseline & experiment configs\n",
    "\n",
    "- Keep **base_cfg** stable.\n",
    "- Only put **changed knobs** in `exp_cfg = replace(base_cfg, ...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69c6ca41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RAGConfig(normalize_model='solar-pro2', generation_model='gpt-4o-mini', temperature=0.1, normalize_temperature=0.0, embedding_backend='upstage', embedding_model='solar-embedding-1-large-passage', k_law=5, k_rule=5, k_case=3, search_multiplier=2, enable_bm25=True, sparse_mode='auto', sparse_k_law=None, sparse_k_rule=None, sparse_k_case=None, bm25_algorithm='okapi', bm25_k1=1.5, bm25_b=0.75, bm25_use_kiwi=True, bm25_max_doc_chars=2400, enable_bm25_title=True, bm25_title_field='title', bm25_title_max_chars=512, hybrid_sparse_title_ratio=0.35, hybrid_fusion='rrf', hybrid_dense_weight=0.6, hybrid_sparse_weight=0.4, rrf_k=60, enable_rerank=True, rerank_threshold=0.2, rerank_model='rerank-multilingual-v3.0', rerank_max_documents=20, rerank_doc_max_chars=2400, case_candidate_k=40, case_expand_top_n=None, case_context_top_k=50, dedupe_key_fields=('chunk_id', 'id')),\n",
       " RAGConfig(normalize_model='solar-pro2', generation_model='gpt-4o-mini', temperature=0.1, normalize_temperature=0.0, embedding_backend='upstage', embedding_model='solar-embedding-1-large-passage', k_law=5, k_rule=5, k_case=3, search_multiplier=2, enable_bm25=True, sparse_mode='auto', sparse_k_law=None, sparse_k_rule=None, sparse_k_case=None, bm25_algorithm='okapi', bm25_k1=1.5, bm25_b=0.75, bm25_use_kiwi=True, bm25_max_doc_chars=2400, enable_bm25_title=True, bm25_title_field='title', bm25_title_max_chars=512, hybrid_sparse_title_ratio=0.35, hybrid_fusion='rrf', hybrid_dense_weight=0.6, hybrid_sparse_weight=0.4, rrf_k=60, enable_rerank=True, rerank_threshold=0.22, rerank_model='rerank-multilingual-v3.0', rerank_max_documents=20, rerank_doc_max_chars=2400, case_candidate_k=40, case_expand_top_n=None, case_context_top_k=50, dedupe_key_fields=('chunk_id', 'id')))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import replace\n",
    "from rag_module import RAGConfig\n",
    "\n",
    "# =========================\n",
    "# Base config (edit as needed)\n",
    "# =========================\n",
    "base_cfg = RAGConfig(\n",
    "    # ---- LLM ----\n",
    "    normalize_model=\"solar-pro2\",\n",
    "    generation_model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    normalize_temperature=0.0,\n",
    "\n",
    "    # ---- Embedding ----\n",
    "    embedding_backend=\"upstage\",\n",
    "    embedding_model=\"solar-embedding-1-large-passage\",\n",
    "\n",
    "    # ---- Dense Retrieval ----\n",
    "    k_law=5,\n",
    "    k_rule=5,\n",
    "    k_case=3,\n",
    "    search_multiplier=2,\n",
    "\n",
    "    # ---- BM25 / Sparse ----\n",
    "    enable_bm25=True,\n",
    "\n",
    "    # ---- Rerank ----\n",
    "    enable_rerank=True,\n",
    "    rerank_threshold=0.2,\n",
    "    rerank_max_documents=20,\n",
    "\n",
    "    # ---- Output trimming ----\n",
    "    bm25_max_doc_chars=2400,\n",
    "    rerank_doc_max_chars=2400,\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Experiment config (only diffs here)\n",
    "# =========================\n",
    "exp_cfg = replace(base_cfg,\n",
    "    rerank_threshold=0.22,\n",
    "    rerank_max_documents=20,\n",
    ")\n",
    "base_cfg, exp_cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162fbbb5",
   "metadata": {},
   "source": [
    "## 3) Build pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba0f69f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 16:32:56,763 - rag_module - INFO - ğŸ”— Pinecone 3ì¤‘ ì¸ë±ìŠ¤ ì—°ê²° ì¤‘...\n",
      "2026-02-02 16:32:59,406 - rag_module - INFO - âœ… [Law / Rule / Case] 3ê°œ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ!\n",
      "2026-02-02 16:33:00,175 - rag_module - INFO - âœ… Kiwi í† í¬ë‚˜ì´ì € ì‚¬ìš© (BM25)\n",
      "2026-02-02 16:33:01,615 - rag_module - INFO - ğŸ”— Pinecone 3ì¤‘ ì¸ë±ìŠ¤ ì—°ê²° ì¤‘...\n",
      "2026-02-02 16:33:01,618 - rag_module - INFO - âœ… [Law / Rule / Case] 3ê°œ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ!\n",
      "2026-02-02 16:33:01,999 - rag_module - INFO - âœ… Kiwi í† í¬ë‚˜ì´ì € ì‚¬ìš© (BM25)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… pipelines ready\n"
     ]
    }
   ],
   "source": [
    "from rag_module import create_pipeline\n",
    "\n",
    "base_pipe = create_pipeline(config=base_cfg)\n",
    "exp_pipe  = create_pipeline(config=exp_cfg)\n",
    "\n",
    "print(\"âœ… pipelines ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6959059d-0ad1-4770-a338-6be9623871f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8fd6fa-516a-44f7-aa82-3095b2320555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e6dfb91",
   "metadata": {},
   "source": [
    "## 4) (Optional) Quick trace sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85371f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ Skip or customize depending on your pipeline API.\n"
     ]
    }
   ],
   "source": [
    "# If your rag_module exposes a trace / debug method, call it here.\n",
    "# Otherwise you can skip this cell.\n",
    "\n",
    "# Example (adjust to your actual API):\n",
    "# ans, trace = base_pipe.answer_with_trace(\"í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ...\")\n",
    "# display(trace)\n",
    "\n",
    "print(\"â„¹ï¸ Skip or customize depending on your pipeline API.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16739f05",
   "metadata": {},
   "source": [
    "## 5) Build RAGAS samples from your pipeline outputs\n",
    "\n",
    "This converts each testset row into the RAGAS format:\n",
    "- `question`\n",
    "- `answer`\n",
    "- `contexts` (list[str])\n",
    "- `ground_truth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a8d6173",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 16:33:03,700 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:33:03,710 - rag_module - INFO - ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: ì„ëŒ€ì¸(ì§‘ì£¼ì¸)ì´ ì•„ë¬´ ë§ë„ ì•ˆ í–ˆëŠ”ë° ê³„ì•½ê¸°ê°„ì´ ëë‚¬ì–´ìš”. ì œê°€ ê³„ì† ì‚´ë©´ì„œ ì°¨ì„(ì›”ì„¸)ë„ ëƒˆë‹¤ë©´ ë¬µì‹œì ê°±ì‹ (ìë™ì—°ì¥)ì´ ëœ ê±´ê°€ìš”? ë§Œì•½ ì„ëŒ€ì¸(ì§‘ì£¼ì¸)ì´ ëª…ë„(ë‚˜ê°€ë¼ê³  í•¨)í•˜ë¼ê³  í•˜ë©´ ì–¸ì œê¹Œì§€ ê³„ì•½í•´ì§€(í†µë³´)ë¥¼ í•´ì•¼ í•˜ë‚˜ìš”?\n",
      "2026-02-02 16:33:03,711 - rag_module - INFO - ğŸ” [Hybrid Retrieval] query='ì„ëŒ€ì¸(ì§‘ì£¼ì¸)ì´ ì•„ë¬´ ë§ë„ ì•ˆ í–ˆëŠ”ë° ê³„ì•½ê¸°ê°„ì´ ëë‚¬ì–´ìš”. ì œê°€ ê³„ì† ì‚´ë©´ì„œ ì°¨ì„(ì›”ì„¸)ë„ ëƒˆë‹¤ë©´ ë¬µì‹œì ê°±ì‹ (ìë™ì—°ì¥)ì´ ëœ ê±´ê°€ìš”? ë§Œì•½ ì„ëŒ€ì¸(ì§‘ì£¼ì¸)ì´ ëª…ë„(ë‚˜ê°€ë¼ê³  í•¨)í•˜ë¼ê³  í•˜ë©´ ì–¸ì œê¹Œì§€ ê³„ì•½í•´ì§€(í†µë³´)ë¥¼ í•´ì•¼ í•˜ë‚˜ìš”?'\n",
      "2026-02-02 16:33:04,316 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:33:06,131 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:33:08,335 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:33:13,279 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:33:13,285 - rag_module - INFO - ğŸ“Œ Rerank selected=1 (threshold=0.2)\n",
      "2026-02-02 16:33:13,286 - rag_module - INFO - ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n",
      "2026-02-02 16:33:22,346 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:33:23,218 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:33:23,221 - rag_module - INFO - ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: ì£¼ë¯¼ë“±ë¡ê³¼ í™•ì •ì¼ì(í™•ì •ì¼ì)ë¥¼ ë°›ì•„ë‘ì—ˆëŠ”ë°, ì„ëŒ€ì¸(ì„ëŒ€ì¸)ì´ ê°™ì€ ì„ì°¨ì£¼íƒ(ì„ì°¨ì£¼íƒ)ìœ¼ë¡œ ë‹¤ë¥¸ ì„ì°¨ì¸(ì„ì°¨ì¸)ê³¼ ì´ì¤‘ê³„ì•½ì„ í–ˆë‹¤ê³  í•©ë‹ˆë‹¤. ì´ ê²½ìš° ì €ëŠ” ìš°ì„ ë³€ì œê¶Œ(ìš°ì„ ë³€ì œê¶Œ)ì´ ìƒê¸´ ê±´ê°€ìš”? íš¨ë ¥ì€ ì–¸ì œë¶€í„° ë°œìƒí•˜ê³ , ê²½ë§¤ì ˆì°¨(ê²½ë§¤ì ˆì°¨)ì— ë„˜ì–´ê°€ë©´ ì–´ë–¤ ìˆœì„œë¡œ ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ(ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ)ì„ ëŒë ¤ë°›ê²Œ ë˜ë‚˜ìš”?\n",
      "2026-02-02 16:33:23,222 - rag_module - INFO - ğŸ” [Hybrid Retrieval] query='ì£¼ë¯¼ë“±ë¡ê³¼ í™•ì •ì¼ì(í™•ì •ì¼ì)ë¥¼ ë°›ì•„ë‘ì—ˆëŠ”ë°, ì„ëŒ€ì¸(ì„ëŒ€ì¸)ì´ ê°™ì€ ì„ì°¨ì£¼íƒ(ì„ì°¨ì£¼íƒ)ìœ¼ë¡œ ë‹¤ë¥¸ ì„ì°¨ì¸(ì„ì°¨ì¸)ê³¼ ì´ì¤‘ê³„ì•½ì„ í–ˆë‹¤ê³  í•©ë‹ˆë‹¤. ì´ ê²½ìš° ì €ëŠ” ìš°ì„ ë³€ì œê¶Œ(ìš°ì„ ë³€ì œê¶Œ)ì´ ìƒê¸´ ê±´ê°€ìš”? íš¨ë ¥ì€ ì–¸ì œë¶€í„° ë°œìƒí•˜ê³ , ê²½ë§¤ì ˆì°¨(ê²½ë§¤ì ˆì°¨)ì— ë„˜ì–´ê°€ë©´ ì–´ë–¤ ìˆœì„œë¡œ ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ(ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ)ì„ ëŒë ¤ë°›ê²Œ ë˜ë‚˜ìš”?'\n",
      "2026-02-02 16:33:23,675 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:33:24,352 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:33:25,236 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:33:27,287 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:33:27,289 - rag_module - INFO - ğŸ“Œ Rerank selected=6 (threshold=0.2)\n",
      "2026-02-02 16:33:27,290 - rag_module - INFO - ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n",
      "2026-02-02 16:33:41,662 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:33:43,260 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:33:43,263 - rag_module - INFO - ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œì— íŠ¹ì•½ì‚¬í•­ìœ¼ë¡œ ë°˜ë ¤ë™ë¬¼íŠ¹ì•½(ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­)ì´ ìˆëŠ” ê²½ìš°, ì´ë¥¼ ìœ„ë°˜í•˜ì—¬ ë°˜ë ¤ë™ë¬¼íŠ¹ì•½(ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­)ì„ ì–´ê¸°ê³  ë°˜ë ¤ë™ë¬¼íŠ¹ì•½(ë°˜ë ¤ë™ë¬¼)ì„ í‚¤ìš°ë‹¤ ì ë°œë˜ë©´ ë°”ë¡œ ê³„ì•½í•´ì§€(ê³„ì•½í•´ì§€)ë¥¼ ë‹¹í•˜ë‚˜ìš”? ì´ ê²½ìš° ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ(ë³´ì¦ê¸ˆ)ë„ ë³´ì¦ê¸ˆë°˜í™˜(ëŒë ¤ë°›ì§€) ëª»í•  ìˆ˜ ìˆë‚˜ìš”?  \n",
      "\n",
      "â€» ì°¸ê³ : ì›ë¬¸ì˜ 'íŠ¹ì•½'ì€ ìš©ì–´ ì‚¬ì „ì— ëª…ì‹œëœ ë§¤í•‘ ëŒ€ìƒì´ ì•„ë‹ˆë¯€ë¡œ ë³€ê²½í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ë¬¸ë§¥ìƒ 'íŠ¹ì•½ì‚¬í•­'ìœ¼ë¡œì˜ ë³€í™˜ì´ ê°€ëŠ¥í•˜ë‚˜, ì‚¬ì „ ê¸°ì¤€ ë¯¸í¬í•¨ìœ¼ë¡œ ì¸í•´ ì›ë¬¸ ìœ ì§€í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
      "2026-02-02 16:33:43,264 - rag_module - INFO - ğŸ” [Hybrid Retrieval] query='ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œì— íŠ¹ì•½ì‚¬í•­ìœ¼ë¡œ ë°˜ë ¤ë™ë¬¼íŠ¹ì•½(ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­)ì´ ìˆëŠ” ê²½ìš°, ì´ë¥¼ ìœ„ë°˜í•˜ì—¬ ë°˜ë ¤ë™ë¬¼íŠ¹ì•½(ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­)ì„ ì–´ê¸°ê³  ë°˜ë ¤ë™ë¬¼íŠ¹ì•½(ë°˜ë ¤ë™ë¬¼)ì„ í‚¤ìš°ë‹¤ ì ë°œë˜ë©´ ë°”ë¡œ ê³„ì•½í•´ì§€(ê³„ì•½í•´ì§€)ë¥¼ ë‹¹í•˜ë‚˜ìš”? ì´ ê²½ìš° ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ(ë³´ì¦ê¸ˆ)ë„ ë³´ì¦ê¸ˆë°˜í™˜(ëŒë ¤ë°›ì§€) ëª»í•  ìˆ˜ ìˆë‚˜ìš”?  \n",
      "\n",
      "â€» ì°¸ê³ : ì›ë¬¸ì˜ 'íŠ¹ì•½'ì€ ìš©ì–´ ì‚¬ì „ì— ëª…ì‹œëœ ë§¤í•‘ ëŒ€ìƒì´ ì•„ë‹ˆë¯€ë¡œ ë³€ê²½í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ë¬¸ë§¥ìƒ 'íŠ¹ì•½ì‚¬í•­'ìœ¼ë¡œì˜ ë³€í™˜ì´ ê°€ëŠ¥í•˜ë‚˜, ì‚¬ì „ ê¸°ì¤€ ë¯¸í¬í•¨ìœ¼ë¡œ ì¸í•´ ì›ë¬¸ ìœ ì§€í•˜ì˜€ìŠµë‹ˆë‹¤.'\n",
      "2026-02-02 16:33:43,889 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:33:44,760 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:33:45,619 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:33:47,513 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:33:47,519 - rag_module - INFO - ğŸ“Œ Rerank selected=20 (threshold=0.2)\n",
      "2026-02-02 16:33:47,520 - rag_module - INFO - ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n",
      "2026-02-02 16:33:58,046 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:33:58,855 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:33:58,856 - rag_module - INFO - ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: ì„ëŒ€ì¸(ì§‘ì£¼ì¸)ì´ ì•„ë¬´ ë§ë„ ì•ˆ í–ˆëŠ”ë° ê³„ì•½ê¸°ê°„ì´ ëë‚¬ì–´ìš”. ì œê°€ ê³„ì† ì‚´ë©´ì„œ ì°¨ì„(ì›”ì„¸)ë„ ëƒˆë‹¤ë©´ ë¬µì‹œì ê°±ì‹ (ìë™ì—°ì¥)ì´ ëœ ê±´ê°€ìš”? ë§Œì•½ ì„ëŒ€ì¸(ì§‘ì£¼ì¸)ì´ ëª…ë„(ë‚˜ê°€ë¼ê³  í•¨)í•˜ë¼ê³  í•˜ë©´ ì–¸ì œê¹Œì§€ ê³„ì•½ê°±ì‹ ê±°ì ˆ(í†µë³´)í•´ì•¼ í•˜ë‚˜ìš”?\n",
      "2026-02-02 16:33:58,857 - rag_module - INFO - ğŸ” [Hybrid Retrieval] query='ì„ëŒ€ì¸(ì§‘ì£¼ì¸)ì´ ì•„ë¬´ ë§ë„ ì•ˆ í–ˆëŠ”ë° ê³„ì•½ê¸°ê°„ì´ ëë‚¬ì–´ìš”. ì œê°€ ê³„ì† ì‚´ë©´ì„œ ì°¨ì„(ì›”ì„¸)ë„ ëƒˆë‹¤ë©´ ë¬µì‹œì ê°±ì‹ (ìë™ì—°ì¥)ì´ ëœ ê±´ê°€ìš”? ë§Œì•½ ì„ëŒ€ì¸(ì§‘ì£¼ì¸)ì´ ëª…ë„(ë‚˜ê°€ë¼ê³  í•¨)í•˜ë¼ê³  í•˜ë©´ ì–¸ì œê¹Œì§€ ê³„ì•½ê°±ì‹ ê±°ì ˆ(í†µë³´)í•´ì•¼ í•˜ë‚˜ìš”?'\n",
      "2026-02-02 16:33:59,305 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:34:01,353 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:34:03,601 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:34:08,380 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:34:08,383 - rag_module - INFO - ğŸ“Œ Rerank selected=1 (threshold=0.22)\n",
      "2026-02-02 16:34:08,384 - rag_module - INFO - ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n",
      "2026-02-02 16:34:19,120 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:34:20,006 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:34:20,008 - rag_module - INFO - ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: ì£¼ë¯¼ë“±ë¡ê³¼ í™•ì •ì¼ì(í™•ì •ì¼ì)ë¥¼ ë°›ì•„ë‘ì—ˆëŠ”ë°, ì„ëŒ€ì¸(ì„ëŒ€ì¸)ì´ ê°™ì€ ì„ì°¨ì£¼íƒ(ì„ì°¨ì£¼íƒ)ìœ¼ë¡œ ë‹¤ë¥¸ ì„ì°¨ì¸(ì„ì°¨ì¸)ê³¼ ì´ì¤‘ê³„ì•½ì„ í–ˆë‹¤ê³  í•©ë‹ˆë‹¤. ì´ ê²½ìš° ì €ëŠ” ìš°ì„ ë³€ì œê¶Œ(ìš°ì„ ë³€ì œê¶Œ)ì´ ìƒê¸´ ê±´ê°€ìš”? íš¨ë ¥ì€ ì–¸ì œë¶€í„° ë°œìƒí•˜ê³ , ê²½ë§¤ì ˆì°¨(ê²½ë§¤ì ˆì°¨)ì— ë„˜ì–´ê°€ë©´ ì–´ë–¤ ìˆœì„œë¡œ ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ(ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ)ì„ ëŒë ¤ë°›ê²Œ ë˜ë‚˜ìš”?\n",
      "2026-02-02 16:34:20,009 - rag_module - INFO - ğŸ” [Hybrid Retrieval] query='ì£¼ë¯¼ë“±ë¡ê³¼ í™•ì •ì¼ì(í™•ì •ì¼ì)ë¥¼ ë°›ì•„ë‘ì—ˆëŠ”ë°, ì„ëŒ€ì¸(ì„ëŒ€ì¸)ì´ ê°™ì€ ì„ì°¨ì£¼íƒ(ì„ì°¨ì£¼íƒ)ìœ¼ë¡œ ë‹¤ë¥¸ ì„ì°¨ì¸(ì„ì°¨ì¸)ê³¼ ì´ì¤‘ê³„ì•½ì„ í–ˆë‹¤ê³  í•©ë‹ˆë‹¤. ì´ ê²½ìš° ì €ëŠ” ìš°ì„ ë³€ì œê¶Œ(ìš°ì„ ë³€ì œê¶Œ)ì´ ìƒê¸´ ê±´ê°€ìš”? íš¨ë ¥ì€ ì–¸ì œë¶€í„° ë°œìƒí•˜ê³ , ê²½ë§¤ì ˆì°¨(ê²½ë§¤ì ˆì°¨)ì— ë„˜ì–´ê°€ë©´ ì–´ë–¤ ìˆœì„œë¡œ ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ(ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ)ì„ ëŒë ¤ë°›ê²Œ ë˜ë‚˜ìš”?'\n",
      "2026-02-02 16:34:20,441 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:34:21,256 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:34:23,180 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:34:25,027 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:34:25,033 - rag_module - INFO - ğŸ“Œ Rerank selected=6 (threshold=0.22)\n",
      "2026-02-02 16:34:25,033 - rag_module - INFO - ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n",
      "2026-02-02 16:34:35,643 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:34:36,941 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:34:36,944 - rag_module - INFO - ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œì— íŠ¹ì•½ì‚¬í•­ìœ¼ë¡œ ë°˜ë ¤ë™ë¬¼íŠ¹ì•½(ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­)ì´ ìˆëŠ” ê²½ìš°, ì´ë¥¼ ìœ„ë°˜í•˜ì—¬ ë°˜ë ¤ë™ë¬¼íŠ¹ì•½(ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­)ì„ ì–´ê¸°ê³  ë°˜ë ¤ë™ë¬¼íŠ¹ì•½(ë°˜ë ¤ë™ë¬¼)ì„ í‚¤ìš°ë‹¤ ì ë°œë˜ë©´ ë°”ë¡œ ê³„ì•½í•´ì§€(ê³„ì•½í•´ì§€)ë¥¼ ë‹¹í•˜ë‚˜ìš”? ì´ ê²½ìš° ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ(ë³´ì¦ê¸ˆ)ë„ ë³´ì¦ê¸ˆë°˜í™˜(ëŒë ¤ë°›ì§€) ëª»í•  ìˆ˜ ìˆë‚˜ìš”?  \n",
      "\n",
      "â€» ì°¸ê³ : ì›ë¬¸ì˜ 'íŠ¹ì•½'ì€ ìš©ì–´ ì‚¬ì „ì— ëª…ì‹œëœ ë§¤í•‘ ëŒ€ìƒì´ ì•„ë‹ˆë¯€ë¡œ ë³€ê²½í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ë¬¸ë§¥ìƒ 'íŠ¹ì•½ì‚¬í•­'ìœ¼ë¡œì˜ ë³€í™˜ì´ ê°€ëŠ¥í•˜ë‚˜, ì‚¬ì „ ê¸°ì¤€ ë¯¸í¬í•¨ìœ¼ë¡œ ì¸í•´ ì›ë¬¸ ìœ ì§€í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
      "2026-02-02 16:34:36,945 - rag_module - INFO - ğŸ” [Hybrid Retrieval] query='ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œì— íŠ¹ì•½ì‚¬í•­ìœ¼ë¡œ ë°˜ë ¤ë™ë¬¼íŠ¹ì•½(ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­)ì´ ìˆëŠ” ê²½ìš°, ì´ë¥¼ ìœ„ë°˜í•˜ì—¬ ë°˜ë ¤ë™ë¬¼íŠ¹ì•½(ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­)ì„ ì–´ê¸°ê³  ë°˜ë ¤ë™ë¬¼íŠ¹ì•½(ë°˜ë ¤ë™ë¬¼)ì„ í‚¤ìš°ë‹¤ ì ë°œë˜ë©´ ë°”ë¡œ ê³„ì•½í•´ì§€(ê³„ì•½í•´ì§€)ë¥¼ ë‹¹í•˜ë‚˜ìš”? ì´ ê²½ìš° ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ(ë³´ì¦ê¸ˆ)ë„ ë³´ì¦ê¸ˆë°˜í™˜(ëŒë ¤ë°›ì§€) ëª»í•  ìˆ˜ ìˆë‚˜ìš”?  \n",
      "\n",
      "â€» ì°¸ê³ : ì›ë¬¸ì˜ 'íŠ¹ì•½'ì€ ìš©ì–´ ì‚¬ì „ì— ëª…ì‹œëœ ë§¤í•‘ ëŒ€ìƒì´ ì•„ë‹ˆë¯€ë¡œ ë³€ê²½í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ë¬¸ë§¥ìƒ 'íŠ¹ì•½ì‚¬í•­'ìœ¼ë¡œì˜ ë³€í™˜ì´ ê°€ëŠ¥í•˜ë‚˜, ì‚¬ì „ ê¸°ì¤€ ë¯¸í¬í•¨ìœ¼ë¡œ ì¸í•´ ì›ë¬¸ ìœ ì§€í•˜ì˜€ìŠµë‹ˆë‹¤.'\n",
      "2026-02-02 16:34:37,553 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:34:38,218 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:34:39,026 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:34:40,937 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:34:40,944 - rag_module - INFO - ğŸ“Œ Rerank selected=20 (threshold=0.22)\n",
      "2026-02-02 16:34:40,945 - rag_module - INFO - ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n",
      "2026-02-02 16:34:51,991 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BASE_SAMPLES: 3\n",
      "âœ… EXP_SAMPLES : 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ì§‘ì£¼ì¸ì´ ì•„ë¬´ ë§ë„ ì•ˆ í–ˆëŠ”ë° ê³„ì•½ê¸°ê°„ì´ ëë‚¬ì–´ìš”. ì œê°€ ê³„ì† ì‚´ë©´ì„œ ì›”ì„¸ë„ ëƒˆë‹¤ë©´...</td>\n",
       "      <td>A. ë„¤, ì„ëŒ€ì¸ì´ ì•„ë¬´ ë§ë„ í•˜ì§€ ì•Šì•˜ë‹¤ë©´ ë¬µì‹œì  ê°±ì‹ ì´ ëœ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.\\n...</td>\n",
       "      <td>[page_content='â‘  ì„ëŒ€ì¸ì´ ì„ëŒ€ì°¨ê¸°ê°„ì´ ëë‚˜ê¸° 6ê°œì›” ì „ë¶€í„° 2ê°œì›” ì „...</td>\n",
       "      <td>ì„ëŒ€ì°¨ ê¸°ê°„ ë§Œë£Œ í›„ ì„ì°¨ì¸ì´ ê³„ì† ê±°ì£¼í•˜ë©° ì°¨ì„ì„ ì§€ê¸‰í•˜ê³ , ì„ëŒ€ì¸ì´ ìƒë‹¹ ê¸°ê°„ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  ì§‘ì£¼ì¸ì´ ì•„ë¬´ ë§ë„ ì•ˆ í–ˆëŠ”ë° ê³„ì•½ê¸°ê°„ì´ ëë‚¬ì–´ìš”. ì œê°€ ê³„ì† ì‚´ë©´ì„œ ì›”ì„¸ë„ ëƒˆë‹¤ë©´...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  A. ë„¤, ì„ëŒ€ì¸ì´ ì•„ë¬´ ë§ë„ í•˜ì§€ ì•Šì•˜ë‹¤ë©´ ë¬µì‹œì  ê°±ì‹ ì´ ëœ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.\\n...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [page_content='â‘  ì„ëŒ€ì¸ì´ ì„ëŒ€ì°¨ê¸°ê°„ì´ ëë‚˜ê¸° 6ê°œì›” ì „ë¶€í„° 2ê°œì›” ì „...   \n",
       "\n",
       "                                        ground_truth  \n",
       "0  ì„ëŒ€ì°¨ ê¸°ê°„ ë§Œë£Œ í›„ ì„ì°¨ì¸ì´ ê³„ì† ê±°ì£¼í•˜ë©° ì°¨ì„ì„ ì§€ê¸‰í•˜ê³ , ì„ëŒ€ì¸ì´ ìƒë‹¹ ê¸°ê°„ ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def shrink_contexts(ctxs, max_chars=2400, max_contexts=30):\n",
    "    out = []\n",
    "    for c in (ctxs or []):\n",
    "        if c is None:\n",
    "            continue\n",
    "        s = str(c).strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        out.append(s[:max_chars])\n",
    "        if len(out) >= max_contexts:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "def row_get_ground_truth(r: dict):\n",
    "    return r.get(\"ground_truth\") or r.get(\"reference\") or r.get(\"gt\") or r.get(\"answer\")\n",
    "\n",
    "def run_pipe_to_samples(pipe, rows, max_chars=2400, max_contexts=30, limit=None):\n",
    "    samples = []\n",
    "    n = len(rows) if limit is None else min(limit, len(rows))\n",
    "\n",
    "    for i in range(n):\n",
    "        r = rows[i]\n",
    "        q = r.get(\"question\") or r.get(\"query\")\n",
    "        if not q:\n",
    "            continue\n",
    "\n",
    "        # âœ… ë„¤ íŒŒì´í”„ë¼ì¸ì€ ì´ê±¸ë¡œ í˜¸ì¶œí•´ì•¼ í•¨\n",
    "        out = pipe.answer_with_trace(q)\n",
    "\n",
    "        # outì´ dictì¼ ìˆ˜ë„ ìˆê³ , (answer, ctxs, trace) íŠœí”Œì¼ ìˆ˜ë„ ìˆì–´ì„œ ì•ˆì „ ì²˜ë¦¬\n",
    "        ans, ctxs, trace = \"\", [], None\n",
    "\n",
    "        if isinstance(out, dict):\n",
    "            ans = out.get(\"answer\") or out.get(\"result\") or out.get(\"output\") or out.get(\"text\") or \"\"\n",
    "            ctxs = out.get(\"contexts\") or out.get(\"context\") or out.get(\"docs\") or []\n",
    "            trace = out.get(\"trace\") or out.get(\"debug\") or out.get(\"meta\")\n",
    "        elif isinstance(out, tuple):\n",
    "            # í”í•œ íŒ¨í„´ë“¤ ëŒ€ì‘\n",
    "            if len(out) == 3:\n",
    "                ans, ctxs, trace = out\n",
    "            elif len(out) == 2:\n",
    "                ans, ctxs = out\n",
    "            elif len(out) == 1:\n",
    "                ans = out[0]\n",
    "        else:\n",
    "            ans = str(out)\n",
    "\n",
    "        samples.append({\n",
    "            \"question\": q,\n",
    "            \"answer\": ans or \"\",\n",
    "            \"contexts\": shrink_contexts(ctxs, max_chars=max_chars, max_contexts=max_contexts),\n",
    "            \"ground_truth\": row_get_ground_truth(r) or \"\",\n",
    "            \"_trace\": trace,  # âœ… traceë„ ê°™ì´ ë³´ê´€(ì›í•˜ë©´ ì €ì¥ ê°€ëŠ¥)\n",
    "        })\n",
    "\n",
    "    return samples\n",
    "\n",
    "# âœ… ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸: 1ë¬¸ì œë§Œ\n",
    "BASE_SAMPLES = run_pipe_to_samples(base_pipe, rows, limit=None)\n",
    "EXP_SAMPLES = run_pipe_to_samples(exp_pipe, rows, limit=None)\n",
    "\n",
    "print(\"âœ… BASE_SAMPLES:\", len(BASE_SAMPLES))\n",
    "print(\"âœ… EXP_SAMPLES :\", len(EXP_SAMPLES))\n",
    "pd.DataFrame([{k:v for k,v in BASE_SAMPLES[0].items() if k != \"_trace\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286a3c26",
   "metadata": {},
   "source": [
    "## 6) RAGAS evaluation (prepared cell)\n",
    "\n",
    "- Creates per-sample detail dataframe (when supported by your RAGAS version)\n",
    "- Creates summary dataframe (mean over samples)\n",
    "- Keeps timing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10a15582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… METRICS: ['ContextPrecision', 'ContextRecall', 'Faithfulness', 'AnswerRelevancy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15916\\3329233587.py:30: DeprecationWarning: Importing context_precision from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_precision\n",
      "  from ragas.metrics import context_precision, context_recall, faithfulness\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15916\\3329233587.py:30: DeprecationWarning: Importing context_recall from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_recall\n",
      "  from ragas.metrics import context_precision, context_recall, faithfulness\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15916\\3329233587.py:30: DeprecationWarning: Importing faithfulness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import faithfulness\n",
      "  from ragas.metrics import context_precision, context_recall, faithfulness\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15916\\3329233587.py:34: DeprecationWarning: Importing answer_relevancy from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import answer_relevancy\n",
      "  from ragas.metrics import answer_relevancy\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# RAGAS compare + clean saving (ragas==0.3.2 compatible)\n",
    "# FIXES:\n",
    "#  1) detail.csvì—ì„œ _trace ì œê±° (traceëŠ” trace.jsonlë¡œë§Œ)\n",
    "#  2) samplesì— run_tagë¥¼ ë¯¸ë¦¬ ì£¼ì…í•´ì„œ trace.jsonlì— íƒœê·¸ê°€ ë‚¨ë„ë¡\n",
    "#  3) ground_truths=[...] ì•ˆì „ì¥ì¹˜ ì¶”ê°€ (ë²„ì „/í™˜ê²½ í˜¸í™˜ì„±â†‘)\n",
    "#  4) samples/detail ì»¬ëŸ¼ ì¶©ëŒ ë°©ì§€(ê°€ëŠ¥í•œ í•œ ì•ˆì „í•˜ê²Œ merge)\n",
    "# ============================================================\n",
    "\n",
    "import time, json, re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.llms import llm_factory\n",
    "\n",
    "# ----------------------------\n",
    "# LLM + METRICS (ragas 0.3.2)\n",
    "# ----------------------------\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()  # OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©\n",
    "llm = llm_factory(\"gpt-4o-mini\", client=client)\n",
    "\n",
    "def build_metrics_032():\n",
    "    from ragas.metrics import context_precision, context_recall, faithfulness\n",
    "    metrics = [context_precision, context_recall, faithfulness]\n",
    "    # answer_relevancyëŠ” í™˜ê²½ì— ë”°ë¼ ì—†ì„ ìˆ˜ ìˆì–´ optional\n",
    "    try:\n",
    "        from ragas.metrics import answer_relevancy\n",
    "        metrics.append(answer_relevancy)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    return metrics\n",
    "\n",
    "METRICS = build_metrics_032()\n",
    "print(\"âœ… METRICS:\", [getattr(m, \"__name__\", m.__class__.__name__) for m in METRICS])\n",
    "\n",
    "# ----------------------------\n",
    "# utils\n",
    "# ----------------------------\n",
    "def _json_safe(obj):\n",
    "    \"\"\"Make config/meta safe to dump to json.\"\"\"\n",
    "    try:\n",
    "        json.dumps(obj, ensure_ascii=False)\n",
    "        return obj\n",
    "    except TypeError:\n",
    "        if hasattr(obj, \"model_dump\"):\n",
    "            return obj.model_dump()\n",
    "        if hasattr(obj, \"dict\"):\n",
    "            return obj.dict()\n",
    "        if hasattr(obj, \"__dict__\"):\n",
    "            return obj.__dict__\n",
    "        return str(obj)\n",
    "\n",
    "def _write_json(path: Path, data):\n",
    "    path.write_text(json.dumps(_json_safe(data), ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def _write_jsonl(path: Path, rows):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(_json_safe(r), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def _next_run_dir(project_root: Path, prefix: str):\n",
    "    runs_root = Path(project_root) / \"results\" / \"ragas_runs\"\n",
    "    runs_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pat = re.compile(rf\"^{re.escape(prefix)}_(\\d{{4}})_\")\n",
    "    nums = []\n",
    "    for p in runs_root.iterdir():\n",
    "        if p.is_dir():\n",
    "            m = pat.match(p.name)\n",
    "            if m:\n",
    "                nums.append(int(m.group(1)))\n",
    "    next_idx = (max(nums) + 1) if nums else 1\n",
    "\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = runs_root / f\"{prefix}_{next_idx:04d}_{ts}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=False)\n",
    "    return run_dir, next_idx, ts\n",
    "\n",
    "def _strip_trace(samples):\n",
    "    \"\"\"detail.csvì—ëŠ” _traceë¥¼ ë„£ì§€ ì•Šê¸°(íŒŒì¼ í­ë°œ ë°©ì§€).\"\"\"\n",
    "    out = []\n",
    "    for s in samples:\n",
    "        if isinstance(s, dict):\n",
    "            out.append({k: v for k, v in s.items() if k != \"_trace\"})\n",
    "        else:\n",
    "            out.append(s)\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# core eval\n",
    "# ----------------------------\n",
    "def eval_ragas_with_details(samples, run_tag: str):\n",
    "    # âœ… samplesì— run_tagë¥¼ ë¯¸ë¦¬ ì£¼ì… (trace.jsonlì—ì„œ íƒœê·¸ ìœ ì§€)\n",
    "    for s in samples:\n",
    "        if isinstance(s, dict):\n",
    "            s[\"run_tag\"] = run_tag\n",
    "            # âœ… ì•ˆì „ì¥ì¹˜: ground_truthsë„ í•¨ê»˜\n",
    "            if \"ground_truths\" not in s:\n",
    "                gt = s.get(\"ground_truth\") or \"\"\n",
    "                s[\"ground_truths\"] = [gt] if isinstance(gt, str) else (gt or [])\n",
    "\n",
    "    ds = Dataset.from_list(samples)\n",
    "\n",
    "    t0 = time.time()\n",
    "    res = evaluate(dataset=ds, metrics=METRICS, llm=llm)  # âœ… 0.3.2 ì•ˆì „ íŒ¨í„´\n",
    "    eval_sec = time.time() - t0\n",
    "\n",
    "    t1 = time.time()\n",
    "    detail_df = res.to_pandas() if hasattr(res, \"to_pandas\") else pd.DataFrame()\n",
    "    to_pandas_sec = time.time() - t1\n",
    "\n",
    "    # âœ… detailì—ëŠ” _trace ì œì™¸\n",
    "    samples_df = pd.DataFrame(_strip_trace(samples))\n",
    "\n",
    "    # merge per-sample metrics back onto samples (ê¸¸ì´ ë™ì¼í•  ë•Œë§Œ)\n",
    "    if len(detail_df) == len(samples_df) and len(detail_df) > 0:\n",
    "        # ì¶©ëŒ ì»¬ëŸ¼ ë°©ì§€: detail_dfì˜ ì»¬ëŸ¼ì´ samples_dfì— ì´ë¯¸ ìˆìœ¼ë©´ prefix\n",
    "        overlap = set(samples_df.columns) & set(detail_df.columns)\n",
    "        if overlap:\n",
    "            detail_df = detail_df.rename(columns={c: f\"metric__{c}\" for c in overlap})\n",
    "\n",
    "        out_detail = pd.concat(\n",
    "            [samples_df.reset_index(drop=True), detail_df.reset_index(drop=True)],\n",
    "            axis=1\n",
    "        )\n",
    "    else:\n",
    "        out_detail = samples_df.copy()\n",
    "\n",
    "    out_detail[\"eval_seconds\"] = round(eval_sec, 3)\n",
    "    out_detail[\"to_pandas_seconds\"] = round(to_pandas_sec, 3)\n",
    "\n",
    "    # summary (mean of numeric metric columns if available)\n",
    "    summary = {}\n",
    "    if len(detail_df) > 0:\n",
    "        summary = detail_df.mean(numeric_only=True).to_dict()\n",
    "    elif isinstance(res, dict):\n",
    "        summary = {k: float(v) for k, v in res.items() if isinstance(v, (int, float))}\n",
    "\n",
    "    summary[\"run_tag\"] = run_tag\n",
    "    summary[\"eval_seconds\"] = round(eval_sec, 3)\n",
    "    summary[\"to_pandas_seconds\"] = round(to_pandas_sec, 3)\n",
    "\n",
    "    return res, out_detail, pd.DataFrame([summary])\n",
    "\n",
    "# ----------------------------\n",
    "# compare + save (clean)\n",
    "# ----------------------------\n",
    "def run_compare_and_save(\n",
    "    base_samples,\n",
    "    exp_samples,\n",
    "    project_root: Path,\n",
    "    prefix=\"ragas_compare\",\n",
    "    base_cfg=None,\n",
    "    exp_cfg=None,\n",
    "):\n",
    "    # --- sanity ---\n",
    "    print(f\"âœ… base_samples: {len(base_samples)} | exp_samples: {len(exp_samples)}\")\n",
    "\n",
    "    base_res, base_detail_df, base_summary_df = eval_ragas_with_details(base_samples, \"baseline\")\n",
    "    exp_res,  exp_detail_df,  exp_summary_df  = eval_ragas_with_details(exp_samples,  \"experiment\")\n",
    "\n",
    "    summary_df = pd.concat([base_summary_df, exp_summary_df], ignore_index=True)\n",
    "    detail_df  = pd.concat([base_detail_df,  exp_detail_df],  ignore_index=True)\n",
    "\n",
    "    run_dir, run_id, ts = _next_run_dir(project_root, prefix)\n",
    "\n",
    "    out_summary = run_dir / \"summary.csv\"\n",
    "    out_detail  = run_dir / \"detail.csv\"\n",
    "    out_meta    = run_dir / \"meta.json\"\n",
    "    out_config  = run_dir / \"config.json\"\n",
    "    out_base_in = run_dir / \"samples_base.jsonl\"\n",
    "    out_exp_in  = run_dir / \"samples_exp.jsonl\"\n",
    "    out_trace   = run_dir / \"trace.jsonl\"\n",
    "\n",
    "    summary_df.to_csv(out_summary, index=False, encoding=\"utf-8-sig\")\n",
    "    detail_df.to_csv(out_detail, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # config snapshot (best-effort)\n",
    "    cfg_payload = {\n",
    "        \"base_cfg\": _json_safe(base_cfg) if base_cfg is not None else None,\n",
    "        \"exp_cfg\":  _json_safe(exp_cfg)  if exp_cfg  is not None else None,\n",
    "        \"llm\": {\"model\": \"gpt-4o-mini\"},\n",
    "        \"metrics\": [getattr(m, \"__name__\", m.__class__.__name__) for m in METRICS],\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "    }\n",
    "    _write_json(out_config, cfg_payload)\n",
    "\n",
    "    # input snapshots (ì›ë³¸ ìœ ì§€: _trace í¬í•¨)\n",
    "    _write_jsonl(out_base_in, base_samples)\n",
    "    _write_jsonl(out_exp_in,  exp_samples)\n",
    "\n",
    "    # trace snapshot (best-effort) - samplesì˜ _traceë§Œ ëª¨ì•„ì„œ ì €ì¥\n",
    "    trace_rows = []\n",
    "    for s in list(base_samples) + list(exp_samples):\n",
    "        if isinstance(s, dict) and (\"_trace\" in s) and (s.get(\"_trace\") is not None):\n",
    "            trace_rows.append({\n",
    "                \"run_tag\": s.get(\"run_tag\"),\n",
    "                \"question\": s.get(\"question\"),\n",
    "                \"_trace\": s.get(\"_trace\"),\n",
    "            })\n",
    "\n",
    "    # traceê°€ ì•„ì˜ˆ ì—†ìœ¼ë©´ ìµœì†Œ ì •ë³´ë¼ë„ ë‚¨ê¹€\n",
    "    if not trace_rows:\n",
    "        cols = [c for c in [\"run_tag\", \"question\", \"eval_seconds\"] if c in detail_df.columns]\n",
    "        trace_rows = detail_df[cols].to_dict(orient=\"records\") if cols else []\n",
    "\n",
    "    _write_jsonl(out_trace, trace_rows)\n",
    "\n",
    "    meta = {\n",
    "        \"created_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"ragas_version\": \"0.3.2\",\n",
    "        \"run_id\": run_id,\n",
    "        \"timestamp\": ts,\n",
    "        \"run_dir\": str(run_dir),\n",
    "        \"prefix\": prefix,\n",
    "        \"n_base_samples\": len(base_samples),\n",
    "        \"n_exp_samples\": len(exp_samples),\n",
    "        \"saved\": {\n",
    "            \"summary\": str(out_summary),\n",
    "            \"detail\": str(out_detail),\n",
    "            \"meta\": str(out_meta),\n",
    "            \"config\": str(out_config),\n",
    "            \"samples_base\": str(out_base_in),\n",
    "            \"samples_exp\": str(out_exp_in),\n",
    "            \"trace\": str(out_trace),\n",
    "        },\n",
    "    }\n",
    "    _write_json(out_meta, meta)\n",
    "\n",
    "    print(f\"âœ… Saved to: {run_dir}\")\n",
    "    print(f\"   - summary: {out_summary.name}\")\n",
    "    print(f\"   - detail : {out_detail.name}\")\n",
    "    print(f\"   - meta   : {out_meta.name}\")\n",
    "    print(f\"   - config : {out_config.name}\")\n",
    "    print(f\"   - inputs : {out_base_in.name}, {out_exp_in.name}\")\n",
    "    print(f\"   - trace  : {out_trace.name}\")\n",
    "\n",
    "    return {\n",
    "        \"base_res\": base_res,\n",
    "        \"exp_res\": exp_res,\n",
    "        \"summary_df\": summary_df,\n",
    "        \"detail_df\": detail_df,\n",
    "        \"run_dir\": run_dir,\n",
    "        \"out_summary\": out_summary,\n",
    "        \"out_detail\": out_detail,\n",
    "        \"out_meta\": out_meta,\n",
    "        \"out_config\": out_config,\n",
    "        \"out_samples_base\": out_base_in,\n",
    "        \"out_samples_exp\": out_exp_in,\n",
    "        \"out_trace\": out_trace,\n",
    "        \"run_id\": run_id,\n",
    "    }\n",
    "\n",
    "# ============================\n",
    "# USAGE (ì˜ˆì‹œ)\n",
    "# ============================\n",
    "# result = run_compare_and_save(\n",
    "#     base_samples=BASE_SAMPLES,\n",
    "#     exp_samples=EXP_SAMPLES,\n",
    "#     project_root=PROJECT_ROOT,\n",
    "#     prefix=\"ragas_compare\",\n",
    "#     base_cfg=base_cfg,\n",
    "#     exp_cfg=exp_cfg,\n",
    "# )\n",
    "# display(result[\"summary_df\"])\n",
    "# display(result[\"detail_df\"].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee50b62",
   "metadata": {},
   "source": [
    "## 7) Run + compare + save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f39454-f368-4272-a7ed-116ed98f2ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29525fd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15916\\269887898.py:34: DeprecationWarning: Importing context_precision from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_precision\n",
      "  from ragas.metrics import context_precision, context_recall, faithfulness\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15916\\269887898.py:34: DeprecationWarning: Importing context_recall from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_recall\n",
      "  from ragas.metrics import context_precision, context_recall, faithfulness\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15916\\269887898.py:34: DeprecationWarning: Importing faithfulness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import faithfulness\n",
      "  from ragas.metrics import context_precision, context_recall, faithfulness\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15916\\269887898.py:37: DeprecationWarning: Importing answer_relevancy from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import answer_relevancy\n",
      "  from ragas.metrics import answer_relevancy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… METRICS: ['ContextPrecision', 'ContextRecall', 'Faithfulness', 'AnswerRelevancy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|                                                                               | 0/12 [00:00<?, ?it/s]2026-02-02 16:34:56,510 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:35:05,070 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:35:14,105 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:35:41,490 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:35:43,916 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:35:44,514 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2026-02-02 16:35:45,023 - ragas.executor - ERROR - Exception raised in Job[3]: AttributeError('OpenAIEmbeddings' object has no attribute 'embed_query')\n",
      "2026-02-02 16:35:48,749 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:35:52,509 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:35:55,409 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:35:58,311 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:36:01,548 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:36:04,965 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:36:12,412 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:36:20,107 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:36:43,903 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:36:46,120 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:36:46,646 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2026-02-02 16:36:47,158 - ragas.executor - ERROR - Exception raised in Job[7]: AttributeError('OpenAIEmbeddings' object has no attribute 'embed_query')\n",
      "2026-02-02 16:36:49,474 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:36:52,364 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:36:55,495 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:36:58,555 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:37:01,992 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:37:05,077 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:37:08,949 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:37:12,828 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:37:16,303 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:37:19,715 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:37:27,745 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:37:37,095 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:38:19,470 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:39:05,147 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:39:31,670 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:39:31,678 - ragas.executor - ERROR - Exception raised in Job[10]: InstructorRetryException(<failed_attempts>\n",
      "\n",
      "<generation number=\"1\">\n",
      "<exception>\n",
      "    The output is incomplete due to a max_tokens length limit.\n",
      "</exception>\n",
      "<completion>\n",
      "    ChatCompletion(id='chatcmpl-D4in4BB9yXbGGwO4lIWuDWdMCpXvY', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n    \"statements\": [\\n        {\\n            \"statement\": \"ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­ì„ ìœ„ë°˜í•˜ë©´ ê³„ì•½ í•´ì§€ ë° ë³´ì¦ê¸ˆ ë°˜í™˜ì— ë¶ˆì´ìµì´ ìˆì„ ìˆ˜ ìˆë‹¤.\",\\n            \"reason\": \"The context discusses the implications of violating terms related to deposits and contracts, which can include penalties or issues with deposit returns.\",\\n            \"verdict\": 1\\n        },\\n        {\\n            \"statement\": \"ì„ëŒ€ì°¨ê³„ì•½ì„œì˜ íŠ¹ì•½ì‚¬í•­ì„ ë‹¤ì‹œ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n            \"reason\": \"The context implies the importance of understanding the terms of the lease agreement, including any special provisions.\",\\n            \"verdict\": 1\\n        },\\n        {\\n            \"statement\": \"ì„ëŒ€ì¸ê³¼ì˜ ëŒ€í™”ì—ì„œ ë°˜ë ¤ë™ë¬¼ ê´€ë ¨ ì¡°í•­ì— ëŒ€í•´ ëª…í™•íˆ í•´ì•¼ í•œë‹¤.\",\\n            \"reason\": \"The context suggests that clarity in communication regarding lease terms, including pet policies, is important.\",\\n            \"verdict\": 1\\n        },\\n        {\\n            \"statement\": \"ë°˜ë ¤ë™ë¬¼ ê´€ë ¨ ë¬¸ì œ ë°œìƒ ì‹œ, ì¦ê±°ë¥¼ ë‚¨ê²¨ì•¼ í•œë‹¤.\",\\n            \"reason\": \"The context implies that having evidence is crucial in disputes regarding lease terms, including those related to pets.\",\\n            \"verdict\": 1\\n        },\\n        {\\n            \"statement\": \"í•„ìš”ì‹œ ë²•ë¥  ìƒë‹´ì„ ê³ ë ¤í•´ì•¼ í•œë‹¤.\",\\n            \"reason\": \"The context suggests that legal advice may be necessary in disputes or issues arising from lease agreements.\",\\n            \"verdict\": 1\\n        },\\n        {\\n            \"statement\": \"ë³´ì¦ê¸ˆ ë°˜í™˜ ì¡°ê±´ì„ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n            \"reason\": \"The context emphasizes the importance of understanding the conditions under which a deposit can be returned.\",\\n            \"verdict\": 1\\n        },\\n        {\\n            \"statement\": \"ë¯¼ë²• ì œ629ì¡°ì— ë”°ë¥´ë©´ ì„ì°¨ì¸ì€ ì„ëŒ€ì¸ì˜ ë™ì˜ ì—†ì´ ê·¸ ê¶Œë¦¬ë¥¼ ì–‘ë„í•˜ê±°ë‚˜ ì„ì°¨ë¬¼ì„ ì „ëŒ€í•˜ì§€ ëª»í•œë‹¤.\",\\n            \"reason\": \"The context includes legal references that support this statement regarding tenant rights and obligations.\",\\n            \"verdict\": 1\\n        },\\n        {\\n            \"statement\": \"ë¯¼ë²• ì œ625ì¡°ì— ë”°ë¥´ë©´ ì„ëŒ€ì¸ì´ ì„ì°¨ì¸ì˜ ì˜ì‚¬ì— ë°˜í•˜ì—¬ ë³´ì¡´í–‰ìœ„ë¥¼ í•˜ëŠ” ê²½ìš°ì— ì„ì°¨ì¸ì€ ê³„ì•½ì„ í•´ì§€í•  ìˆ˜ ìˆë‹¤.\",\\n            \"reason\": \"The context includes legal references that support this statement regarding tenant rights in relation to landlord actions.\",\\n            \"verdict\": 1\\n        },\\n        {\\n            \"statement\": \"ê³„ì•½ í•´ì§€ ì‹œ, ì„ëŒ€ì¸ì€ í•´ì§€ ì‚¬ìœ ë¥¼ ì„œë©´ìœ¼ë¡œ í†µì§€í•´ì•¼ í•œë‹¤.\",\\n            \"reason\": \"The context implies that proper notification procedures are necessary when terminating a lease.\",\\n            \"verdict\": 1\\n        },\\n        {\\n            \"statement\": \"ë³´ì¦ê¸ˆ ë°˜í™˜ì„ ìœ„í•´ì„œëŠ” ê³„ì•½ì„œì— ëª…ì‹œëœ ì¡°ê±´ì„ ì¶©ì¡±í•´ì•¼ í•˜ë©°, í•´ì§€ ì‚¬ìœ ê°€ ì •ë‹¹í•œ ê²½ìš° ë³´ì¦ê¸ˆ ë°˜í™˜ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆë‹¤.\",\\n            \"reason\": \"The context discusses the conditions under which a deposit may be returned, including the validity of termination reasons.\",\\n            \"verdict\": 1\\n        },\\n        {\\n            \"statement\": \"ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­ì„ ìœ„ë°˜í•œ ê²½ìš°, ê³„ì•½ í•´ì§€ ë° ë³´ì¦ê¸ˆ ë°˜í™˜ ê±°ë¶€ ì‚¬ë¡€ê°€ ìˆë‹¤.\",\\n            \"reason\": \"The context implies that violations of lease terms, such as pet policies, can lead to termination and issues with deposit returns.\",\\n            \"verdict\": 1\\n        },\\n        {\\n            \"statement\": \"ê° ì‚¬ê±´ì˜ êµ¬ì²´ì ì¸ ìƒí™©ì— ë”°ë¼ ê²°ê³¼ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤.\",\\n            \"reason\": \"The context suggests that outcomes can vary based on specific circumstances surrounding lease agreements.\",\\n            \"verdict\": 1\\n        },\\n        {\\n            \"statement\": \"ê³„ì•½ì„œì— ëª…ì‹œëœ ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­ì˜ êµ¬ì²´ì ì¸ ë‚´ìš©ì€ ë¬´ì—‡ì¸ì§€ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n            \"reason\": \"The context implies the necessity of understanding specific terms in the lease agreement, including pet restrictions.\",\\n            \"verdict\": 1\\n        },\\n        {\\n            \"statement\": \"ì„ëŒ€ì¸ì´ ë°˜ë ¤ë™ë¬¼ ê´€ë ¨ ìœ„ë°˜ ì‚¬ì‹¤ì„ ì–´ë–»ê²Œ ì•Œê²Œ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n            \"reason\": \"The context suggests that understanding how violations are identified is important in disputes.\",\\n            \"verdict\": 1\\n        },\\n        {\\n            \"statement\": \"ê³„ì•½ í•´ì§€ í†µë³´ë¥¼ ë°›ì€ ì ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n            \"reason\": \"The context implies that awareness of any termination notices is crucial for tenants.\",\\n            \"verdict\": 1\\n        },\\n        {\\n            \"statement\": \"ë³´ì¦ê¸ˆ ë°˜í™˜ì— ëŒ€í•œ ì„ëŒ€ì¸ì˜ ì…ì¥ì€ ë¬´ì—‡ì¸ì§€ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n            \"reason\": \"The context suggests that understanding the landlord\\'s position on deposit returns is important.\",\\n            \"verdict\": 1\\n        },\\n        {\\n            \"statement\": \"ì´ ì •ë³´ëŠ” ì¼ë°˜ì ì¸ ë²•ë¥ ì •ë³´ì´ë©°, êµ¬ì²´ ì‚¬ê±´', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1770017858, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_1590f93f9d', usage=CompletionUsage(completion_tokens=3072, prompt_tokens=18858, total_tokens=21930, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=7168)))\n",
      "</completion>\n",
      "</generation>\n",
      "\n",
      "<generation number=\"2\">\n",
      "<exception>\n",
      "    The output is incomplete due to a max_tokens length limit.\n",
      "</exception>\n",
      "<completion>\n",
      "    ChatCompletion(id='chatcmpl-D4injS7iiYhALcvtiQbMtLG6kvd8o', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n    \"statements\": [\\n        {\\n            \"statement\": \"ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­ì„ ìœ„ë°˜í•˜ë©´ ê³„ì•½ í•´ì§€ ë° ë³´ì¦ê¸ˆ ë°˜í™˜ì— ë¶ˆì´ìµì´ ìˆì„ ìˆ˜ ìˆë‹¤.\",\\n            \"reason\": \"The context discusses various legal provisions related to lease agreements, including the implications of violating terms, which can include contract termination and issues with deposit return.\",\\n            \"verdict\": 1\\n        },\\n        {\\n            \"statement\": \"ì„ëŒ€ì°¨ê³„ì•½ì„œì˜ íŠ¹ì•½ì‚¬í•­ì„ ë‹¤ì‹œ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n            \"reason\": \"The context implies the importance of understanding the lease agreement, including any special provisions, but does not explicitly state that it must be rechecked.\",\\n            \"verdict\": 0\\n        },\\n        {\\n            \"statement\": \"ì„ëŒ€ì¸ê³¼ì˜ ëŒ€í™”ì—ì„œ ë°˜ë ¤ë™ë¬¼ ê´€ë ¨ ì¡°í•­ì— ëŒ€í•´ ëª…í™•íˆ í•´ì•¼ í•œë‹¤.\",\\n            \"reason\": \"While the context suggests the importance of clarity in lease terms, it does not specifically mention the need to clarify pet-related clauses in conversations with the landlord.\",\\n            \"verdict\": 0\\n        },\\n        {\\n            \"statement\": \"ë°˜ë ¤ë™ë¬¼ ê´€ë ¨ ë¬¸ì œ ë°œìƒ ì‹œ, ì¦ê±°ë¥¼ ë‚¨ê²¨ì•¼ í•œë‹¤.\",\\n            \"reason\": \"The context does not provide specific guidance on the need to preserve evidence in case of pet-related issues.\",\\n            \"verdict\": 0\\n        },\\n        {\\n            \"statement\": \"í•„ìš”ì‹œ ë²•ë¥  ìƒë‹´ì„ ê³ ë ¤í•´ì•¼ í•œë‹¤.\",\\n            \"reason\": \"The context does not mention the need for legal consultation, although it may be implied in complex situations.\",\\n            \"verdict\": 0\\n        },\\n        {\\n            \"statement\": \"ë³´ì¦ê¸ˆ ë°˜í™˜ ì¡°ê±´ì„ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n            \"reason\": \"The context discusses the conditions under which deposits may be returned, indicating that it is important to verify these conditions.\",\\n            \"verdict\": 1\\n        },\\n        {\\n            \"statement\": \"ë¯¼ë²• ì œ629ì¡°ì— ë”°ë¥´ë©´ ì„ì°¨ì¸ì€ ì„ëŒ€ì¸ì˜ ë™ì˜ ì—†ì´ ê·¸ ê¶Œë¦¬ë¥¼ ì–‘ë„í•˜ê±°ë‚˜ ì„ì°¨ë¬¼ì„ ì „ëŒ€í•˜ì§€ ëª»í•œë‹¤.\",\\n            \"reason\": \"This statement directly references a specific legal provision that is mentioned in the context, making it a verifiable fact.\",\\n            \"verdict\": 1\\n        },\\n        {\\n            \"statement\": \"ë¯¼ë²• ì œ625ì¡°ì— ë”°ë¥´ë©´ ì„ëŒ€ì¸ì´ ì„ì°¨ì¸ì˜ ì˜ì‚¬ì— ë°˜í•˜ì—¬ ë³´ì¡´í–‰ìœ„ë¥¼ í•˜ëŠ” ê²½ìš°ì— ì„ì°¨ì¸ì€ ê³„ì•½ì„ í•´ì§€í•  ìˆ˜ ìˆë‹¤.\",\\n            \"reason\": \"This statement accurately reflects the content of the context, which discusses the rights of tenants in relation to actions taken by landlords.\",\\n            \"verdict\": 1\\n        },\\n        {\\n            \"statement\": \"ê³„ì•½ í•´ì§€ ì‹œ, ì„ëŒ€ì¸ì€ í•´ì§€ ì‚¬ìœ ë¥¼ ì„œë©´ìœ¼ë¡œ í†µì§€í•´ì•¼ í•œë‹¤.\",\\n            \"reason\": \"The context does not explicitly state this requirement, so it cannot be directly inferred.\",\\n            \"verdict\": 0\\n        },\\n        {\\n            \"statement\": \"ë³´ì¦ê¸ˆ ë°˜í™˜ì„ ìœ„í•´ì„œëŠ” ê³„ì•½ì„œì— ëª…ì‹œëœ ì¡°ê±´ì„ ì¶©ì¡±í•´ì•¼ í•˜ë©°, í•´ì§€ ì‚¬ìœ ê°€ ì •ë‹¹í•œ ê²½ìš° ë³´ì¦ê¸ˆ ë°˜í™˜ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆë‹¤.\",\\n            \"reason\": \"The context implies that conditions for deposit return are tied to the terms of the contract and the legitimacy of the termination reasons.\",\\n            \"verdict\": 1\\n        },\\n        {\\n            \"statement\": \"ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­ì„ ìœ„ë°˜í•œ ê²½ìš°, ê³„ì•½ í•´ì§€ ë° ë³´ì¦ê¸ˆ ë°˜í™˜ ê±°ë¶€ ì‚¬ë¡€ê°€ ìˆë‹¤.\",\\n            \"reason\": \"The context discusses the implications of violating lease terms, including potential termination and issues with deposit return, which supports this statement.\",\\n            \"verdict\": 1\\n        },\\n        {\\n            \"statement\": \"ê° ì‚¬ê±´ì˜ êµ¬ì²´ì ì¸ ìƒí™©ì— ë”°ë¼ ê²°ê³¼ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤.\",\\n            \"reason\": \"The context suggests that outcomes can vary based on specific circumstances, making this statement valid.\",\\n            \"verdict\": 1\\n        },\\n        {\\n            \"statement\": \"ê³„ì•½ì„œì— ëª…ì‹œëœ ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­ì˜ êµ¬ì²´ì ì¸ ë‚´ìš©ì€ ë¬´ì—‡ì¸ì§€ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n            \"reason\": \"The context does not provide specific details about the pet prohibition clause, so this statement cannot be directly inferred.\",\\n            \"verdict\": 0\\n        },\\n        {\\n            \"statement\": \"ì„ëŒ€ì¸ì´ ë°˜ë ¤ë™ë¬¼ ê´€ë ¨ ìœ„ë°˜ ì‚¬ì‹¤ì„ ì–´ë–»ê²Œ ì•Œê²Œ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n            \"reason\": \"The context does not address how landlords become aware of violations, making this statement unverifiable.\",\\n            \"verdict\": 0\\n        },\\n        {\\n            \"statement\": \"ê³„ì•½ í•´ì§€ í†µë³´ë¥¼ ë°›ì€ ì ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n            \"reason\": \"The context does not mention the need to verify if a termination notice has been received.\",\\n            \"verdict\": 0\\n        },\\n        {\\n            \"statement\": \"ë³´ì¦ê¸ˆ', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1770017899, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_1590f93f9d', usage=CompletionUsage(completion_tokens=3072, prompt_tokens=18858, total_tokens=21930, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=7168)))\n",
      "</completion>\n",
      "</generation>\n",
      "\n",
      "<generation number=\"3\">\n",
      "<exception>\n",
      "    The output is incomplete due to a max_tokens length limit.\n",
      "</exception>\n",
      "<completion>\n",
      "    ChatCompletion(id='chatcmpl-D4ioTYvr7WmGiBiIlVyTiCG6NUXEv', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"statements\": [\\n    {\\n      \"statement\": \"ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­ì„ ìœ„ë°˜í•˜ë©´ ê³„ì•½ í•´ì§€ ë° ë³´ì¦ê¸ˆ ë°˜í™˜ì— ë¶ˆì´ìµì´ ìˆì„ ìˆ˜ ìˆë‹¤.\",\\n      \"reason\": \"The context discusses various legal aspects of lease agreements, including the implications of violating terms, which can include contract termination and issues with deposit return.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"ì„ëŒ€ì°¨ê³„ì•½ì„œì˜ íŠ¹ì•½ì‚¬í•­ì„ ë‹¤ì‹œ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context implies the importance of understanding the lease agreement, including any special provisions, although it does not explicitly state this.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"ì„ëŒ€ì¸ê³¼ì˜ ëŒ€í™”ì—ì„œ ë°˜ë ¤ë™ë¬¼ ê´€ë ¨ ì¡°í•­ì— ëŒ€í•´ ëª…í™•íˆ í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"While the context emphasizes the importance of understanding lease terms, it does not specifically mention the need for clarity in discussions with the landlord regarding pet clauses.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"ë°˜ë ¤ë™ë¬¼ ê´€ë ¨ ë¬¸ì œ ë°œìƒ ì‹œ, ì¦ê±°ë¥¼ ë‚¨ê²¨ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not provide specific guidance on the need to document issues related to pets, making this statement not directly inferable.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"í•„ìš”ì‹œ ë²•ë¥  ìƒë‹´ì„ ê³ ë ¤í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not mention the need for legal consultation, so this statement cannot be directly inferred.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"ë³´ì¦ê¸ˆ ë°˜í™˜ ì¡°ê±´ì„ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context discusses the conditions under which deposits may be returned, making this statement directly inferable.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"ë¯¼ë²• ì œ629ì¡°ì— ë”°ë¥´ë©´ ì„ì°¨ì¸ì€ ì„ëŒ€ì¸ì˜ ë™ì˜ ì—†ì´ ê·¸ ê¶Œë¦¬ë¥¼ ì–‘ë„í•˜ê±°ë‚˜ ì„ì°¨ë¬¼ì„ ì „ëŒ€í•˜ì§€ ëª»í•œë‹¤.\",\\n      \"reason\": \"The context includes legal references that support this statement, making it directly inferable.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"ë¯¼ë²• ì œ625ì¡°ì— ë”°ë¥´ë©´ ì„ëŒ€ì¸ì´ ì„ì°¨ì¸ì˜ ì˜ì‚¬ì— ë°˜í•˜ì—¬ ë³´ì¡´í–‰ìœ„ë¥¼ í•˜ëŠ” ê²½ìš°ì— ì„ì°¨ì¸ì€ ê³„ì•½ì„ í•´ì§€í•  ìˆ˜ ìˆë‹¤.\",\\n      \"reason\": \"The context provides legal information that supports this statement, making it directly inferable.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"ê³„ì•½ í•´ì§€ ì‹œ, ì„ëŒ€ì¸ì€ í•´ì§€ ì‚¬ìœ ë¥¼ ì„œë©´ìœ¼ë¡œ í†µì§€í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not explicitly mention the requirement for written notice of termination reasons, making this statement not directly inferable.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"ë³´ì¦ê¸ˆ ë°˜í™˜ì„ ìœ„í•´ì„œëŠ” ê³„ì•½ì„œì— ëª…ì‹œëœ ì¡°ê±´ì„ ì¶©ì¡±í•´ì•¼ í•˜ë©°, í•´ì§€ ì‚¬ìœ ê°€ ì •ë‹¹í•œ ê²½ìš° ë³´ì¦ê¸ˆ ë°˜í™˜ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆë‹¤.\",\\n      \"reason\": \"The context discusses conditions for deposit return, making this statement directly inferable.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­ì„ ìœ„ë°˜í•œ ê²½ìš°, ê³„ì•½ í•´ì§€ ë° ë³´ì¦ê¸ˆ ë°˜í™˜ ê±°ë¶€ ì‚¬ë¡€ê°€ ìˆë‹¤.\",\\n      \"reason\": \"The context implies that violations of lease terms can lead to termination and issues with deposit return, making this statement directly inferable.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"ê° ì‚¬ê±´ì˜ êµ¬ì²´ì ì¸ ìƒí™©ì— ë”°ë¼ ê²°ê³¼ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤.\",\\n      \"reason\": \"The context suggests that outcomes can vary based on specific circumstances, making this statement directly inferable.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"ê³„ì•½ì„œì— ëª…ì‹œëœ ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­ì˜ êµ¬ì²´ì ì¸ ë‚´ìš©ì€ ë¬´ì—‡ì¸ì§€ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context emphasizes the importance of understanding the lease terms, including pet clauses, making this statement directly inferable.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"ì„ëŒ€ì¸ì´ ë°˜ë ¤ë™ë¬¼ ê´€ë ¨ ìœ„ë°˜ ì‚¬ì‹¤ì„ ì–´ë–»ê²Œ ì•Œê²Œ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not provide information on how landlords become aware of violations, making this statement not directly inferable.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"ê³„ì•½ í•´ì§€ í†µë³´ë¥¼ ë°›ì€ ì ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not mention the need to verify if a termination notice has been received, making this statement not directly inferable.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"ë³´ì¦ê¸ˆ ë°˜í™˜ì— ëŒ€í•œ ì„ëŒ€', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1770017945, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_1590f93f9d', usage=CompletionUsage(completion_tokens=3072, prompt_tokens=18858, total_tokens=21930, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=7168)))\n",
      "</completion>\n",
      "</generation>\n",
      "\n",
      "</failed_attempts>\n",
      "\n",
      "<last_exception>\n",
      "    The output is incomplete due to a max_tokens length limit.\n",
      "</last_exception>)\n",
      "2026-02-02 16:39:33,570 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:39:34,102 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2026-02-02 16:39:34,623 - ragas.executor - ERROR - Exception raised in Job[11]: AttributeError('OpenAIEmbeddings' object has no attribute 'embed_query')\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [04:39<00:00, 23.33s/it]\n",
      "Evaluating:   0%|                                                                               | 0/12 [00:00<?, ?it/s]2026-02-02 16:39:37,959 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:39:45,693 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:39:53,856 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:40:17,971 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:40:20,086 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:40:20,630 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2026-02-02 16:40:21,150 - ragas.executor - ERROR - Exception raised in Job[3]: AttributeError('OpenAIEmbeddings' object has no attribute 'embed_query')\n",
      "2026-02-02 16:40:24,052 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:40:27,702 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:40:30,751 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:40:34,245 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:40:37,848 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:40:41,324 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:40:49,286 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:41:00,115 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:41:23,765 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:41:46,604 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:42:13,565 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:42:13,575 - ragas.executor - ERROR - Exception raised in Job[6]: InstructorRetryException(<failed_attempts>\n",
      "\n",
      "<generation number=\"1\">\n",
      "<exception>\n",
      "    The output is incomplete due to a max_tokens length limit.\n",
      "</exception>\n",
      "<completion>\n",
      "    ChatCompletion(id='chatcmpl-D4iqLJgFsIIJpfPZnjmaSA5Ni3K4n', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"statements\": [\\n    {\\n      \"statement\": \"í™•ì •ì¼ìë¥¼ ë°›ì€ ê²½ìš° ìš°ì„ ë³€ì œê¶Œì´ ë°œìƒí•œë‹¤.\",\\n      \"reason\": \"The context states that if a tenant receives a confirmed date, they have priority rights, which directly supports this statement.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"ì„ëŒ€ì°¨ê³„ì•½ì„œì™€ í™•ì •ì¼ì ì¦ëª…ì„œ ì‚¬ë³¸ì„ í™•ë³´í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context implies that having the lease contract and proof of the confirmed date is necessary for various legal processes, thus this statement can be inferred.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"ë“±ê¸°ë¶€ë“±ë³¸ì„ í™•ì¸í•˜ì—¬ ì„ëŒ€ì¸ì˜ ì†Œìœ ê¶Œ ë° ë‹¤ë¥¸ ì„ì°¨ì¸ì˜ ê³„ì•½ ì—¬ë¶€ë¥¼ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not explicitly mention checking the registration details, but it is a common legal practice when dealing with leases, making this statement reasonable but not directly supported.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"ì „ì…ì‹ ê³ ê°€ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context mentions that tenants must complete their resident registration, which supports the need to verify this step.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"ì„ëŒ€ì¸ì—ê²Œ ì´ì¤‘ê³„ì•½ ì‚¬ì‹¤ì„ í†µì§€í•˜ê³ , ë³´ì¦ê¸ˆ ë°˜í™˜ì„ ìš”êµ¬í•˜ëŠ” ë‚´ìš©ì¦ëª…ì„ ë³´ë‚´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not provide information about notifying the landlord of double contracts or sending a formal request for deposit return, making this statement unsupported.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"í•„ìš”ì‹œ ë²•ë¥  ìƒë‹´ì„ í†µí•´ ì¶”ê°€ ì¡°ì¹˜ë¥¼ ë…¼ì˜í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not mention legal consultation, so this statement cannot be directly inferred.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì œ3ì¡°ì˜2ì— ë”°ë¥´ë©´, í™•ì •ì¼ìë¥¼ ë°›ì€ ê²½ìš° ì„ì°¨ì¸ì€ ìš°ì„ ë³€ì œê¶Œì„ ê°€ì§„ë‹¤.\",\\n      \"reason\": \"This statement is directly supported by the context, which references the law that grants priority rights upon receiving a confirmed date.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì œ8ì¡°ì— ë”°ë¥´ë©´, ì„ì°¨ì¸ì€ ê²½ë§¤ ì‹œ ë³´ì¦ê¸ˆì˜ ìš°ì„ ë³€ì œë¥¼ ë°›ì„ ìˆ˜ ìˆë‹¤.\",\\n      \"reason\": \"The context supports this statement as it discusses the priority of deposits during auction processes under the law.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"ê²½ë§¤ ì ˆì°¨ê°€ ì‹œì‘ë˜ë©´, ë²•ì›ì— ì„ì°¨ê¶Œì„ ì£¼ì¥í•˜ê¸° ìœ„í•´ ì„ëŒ€ì°¨ê³„ì•½ì„œ, í™•ì •ì¼ì ì¦ëª…ì„œ, ì „ì…ì‹ ê³ ì„œ ë“±ì„ ì œì¶œí•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context implies that these documents are necessary for asserting tenant rights during auction procedures, thus supporting this statement.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"ê²½ë§¤ê°€ ì§„í–‰ë˜ë©´, ì„ì°¨ì¸ì€ ë³´ì¦ê¸ˆì˜ ìš°ì„ ë³€ì œë¥¼ ë°›ì„ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ê²½ë§¤ ê²°ê³¼ì— ë”°ë¼ ê²°ì •ëœë‹¤.\",\\n      \"reason\": \"The context supports this statement as it discusses the priority of deposits during the auction process.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"ëŒ€ë²•ì› íŒë¡€ì— ë”°ë¥´ë©´, í™•ì •ì¼ìë¥¼ ë°›ì€ ì„ì°¨ì¸ì€ ê²½ë§¤ ì ˆì°¨ì—ì„œ ìš°ì„ ë³€ì œê¶Œì„ ì£¼ì¥í•  ìˆ˜ ìˆë‹¤.\",\\n      \"reason\": \"This statement is directly supported by the context, which references court precedents regarding priority rights for tenants with confirmed dates.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"í™•ì •ì¼ìë¥¼ ë°›ì€ ì„ì°¨ì¸ì€ ë‹¤ë¥¸ ì±„ê¶Œìë³´ë‹¤ ìš°ì„ ì ìœ¼ë¡œ ë³´ì¦ê¸ˆì„ íšŒìˆ˜í•  ìˆ˜ ìˆëŠ” ê¶Œë¦¬ë¥¼ ê°€ì§„ë‹¤.\",\\n      \"reason\": \"The context supports this statement as it discusses the rights of tenants with confirmed dates to recover deposits before other creditors.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"í™•ì •ì¼ìëŠ” ì–¸ì œ ë¶€ì—¬ë°›ì•˜ëŠ”ì§€ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not explicitly mention the need to verify when the confirmed date was granted, making this statement unsupported.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"ì´ì¤‘ê³„ì•½ì´ ë°œìƒí•œ ì„ì°¨ì¸ì€ ëª‡ ëª…ì¸ì§€ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not provide information about the number of tenants involved in double contracts, making this statement unsupported.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"í˜„ì¬ ì„ëŒ€ì¸ì˜ ì†Œìœ ê¶Œ ìƒíƒœëŠ” ì–´ë–»ê²Œ ë˜ëŠ”ì§€ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not mention the need to verify the current ownership status', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1770018061, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_1590f93f9d', usage=CompletionUsage(completion_tokens=3072, prompt_tokens=17829, total_tokens=20901, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=13440)))\n",
      "</completion>\n",
      "</generation>\n",
      "\n",
      "<generation number=\"2\">\n",
      "<exception>\n",
      "    The output is incomplete due to a max_tokens length limit.\n",
      "</exception>\n",
      "<completion>\n",
      "    ChatCompletion(id='chatcmpl-D4iqhFZJOuQTDjKf48gXK7QBoVayP', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"statements\": [\\n    {\\n      \"statement\": \"í™•ì •ì¼ìë¥¼ ë°›ì€ ê²½ìš° ìš°ì„ ë³€ì œê¶Œì´ ë°œìƒí•œë‹¤.\",\\n      \"reason\": \"The context states that if a tenant receives a confirmed date, they have priority rights, which directly supports this statement.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"ì„ëŒ€ì°¨ê³„ì•½ì„œì™€ í™•ì •ì¼ì ì¦ëª…ì„œ ì‚¬ë³¸ì„ í™•ë³´í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not explicitly mention the need to secure copies of the lease agreement and confirmation date certificate, making this statement unverifiable.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"ë“±ê¸°ë¶€ë“±ë³¸ì„ í™•ì¸í•˜ì—¬ ì„ëŒ€ì¸ì˜ ì†Œìœ ê¶Œ ë° ë‹¤ë¥¸ ì„ì°¨ì¸ì˜ ê³„ì•½ ì—¬ë¶€ë¥¼ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not provide information about checking the registration details to confirm ownership or other tenants\\' contracts, so this statement cannot be inferred.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"ì „ì…ì‹ ê³ ê°€ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context mentions that tenants must complete their resident registration, but it does not state that they must verify its completion, making this statement unverifiable.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"ì„ëŒ€ì¸ì—ê²Œ ì´ì¤‘ê³„ì•½ ì‚¬ì‹¤ì„ í†µì§€í•˜ê³ , ë³´ì¦ê¸ˆ ë°˜í™˜ì„ ìš”êµ¬í•˜ëŠ” ë‚´ìš©ì¦ëª…ì„ ë³´ë‚´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not mention the need to notify the landlord of double contracts or send a formal request for the return of the deposit, making this statement unverifiable.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"í•„ìš”ì‹œ ë²•ë¥  ìƒë‹´ì„ í†µí•´ ì¶”ê°€ ì¡°ì¹˜ë¥¼ ë…¼ì˜í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not provide any information about the necessity of legal consultation for additional actions, so this statement cannot be inferred.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì œ3ì¡°ì˜2ì— ë”°ë¥´ë©´, í™•ì •ì¼ìë¥¼ ë°›ì€ ê²½ìš° ì„ì°¨ì¸ì€ ìš°ì„ ë³€ì œê¶Œì„ ê°€ì§„ë‹¤.\",\\n      \"reason\": \"The context directly references the law stating that tenants with a confirmed date have priority rights, which supports this statement.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì œ8ì¡°ì— ë”°ë¥´ë©´, ì„ì°¨ì¸ì€ ê²½ë§¤ ì‹œ ë³´ì¦ê¸ˆì˜ ìš°ì„ ë³€ì œë¥¼ ë°›ì„ ìˆ˜ ìˆë‹¤.\",\\n      \"reason\": \"The context mentions that tenants can receive priority repayment of their deposit during an auction according to the law, which supports this statement.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"ê²½ë§¤ ì ˆì°¨ê°€ ì‹œì‘ë˜ë©´, ë²•ì›ì— ì„ì°¨ê¶Œì„ ì£¼ì¥í•˜ê¸° ìœ„í•´ ì„ëŒ€ì°¨ê³„ì•½ì„œ, í™•ì •ì¼ì ì¦ëª…ì„œ, ì „ì…ì‹ ê³ ì„œ ë“±ì„ ì œì¶œí•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not provide specific details about the submission of documents to the court during the auction process, making this statement unverifiable.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"ê²½ë§¤ê°€ ì§„í–‰ë˜ë©´, ì„ì°¨ì¸ì€ ë³´ì¦ê¸ˆì˜ ìš°ì„ ë³€ì œë¥¼ ë°›ì„ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ê²½ë§¤ ê²°ê³¼ì— ë”°ë¼ ê²°ì •ëœë‹¤.\",\\n      \"reason\": \"The context states that tenants can receive priority repayment of their deposit during the auction, which supports this statement.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"ëŒ€ë²•ì› íŒë¡€ì— ë”°ë¥´ë©´, í™•ì •ì¼ìë¥¼ ë°›ì€ ì„ì°¨ì¸ì€ ê²½ë§¤ ì ˆì°¨ì—ì„œ ìš°ì„ ë³€ì œê¶Œì„ ì£¼ì¥í•  ìˆ˜ ìˆë‹¤.\",\\n      \"reason\": \"The context supports this statement by indicating that tenants with a confirmed date can assert priority rights in auction proceedings.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"í™•ì •ì¼ìë¥¼ ë°›ì€ ì„ì°¨ì¸ì€ ë‹¤ë¥¸ ì±„ê¶Œìë³´ë‹¤ ìš°ì„ ì ìœ¼ë¡œ ë³´ì¦ê¸ˆì„ íšŒìˆ˜í•  ìˆ˜ ìˆëŠ” ê¶Œë¦¬ë¥¼ ê°€ì§„ë‹¤.\",\\n      \"reason\": \"The context states that tenants with a confirmed date have the right to recover their deposit before other creditors, which supports this statement.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"í™•ì •ì¼ìëŠ” ì–¸ì œ ë¶€ì—¬ë°›ì•˜ëŠ”ì§€ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not provide information about the necessity of confirming when the confirmed date was granted, making this statement unverifiable.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"ì´ì¤‘ê³„ì•½ì´ ë°œìƒí•œ ì„ì°¨ì¸ì€ ëª‡ ëª…ì¸ì§€ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not mention the need to verify the number of tenants involved in double contracts, making this statement unverifiable.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\":', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1770018083, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_1590f93f9d', usage=CompletionUsage(completion_tokens=3072, prompt_tokens=17829, total_tokens=20901, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=13440)))\n",
      "</completion>\n",
      "</generation>\n",
      "\n",
      "<generation number=\"3\">\n",
      "<exception>\n",
      "    The output is incomplete due to a max_tokens length limit.\n",
      "</exception>\n",
      "<completion>\n",
      "    ChatCompletion(id='chatcmpl-D4ir4G79zE3YPiQV2IKmTa5vMdRJ5', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"statements\": [\\n    {\\n      \"statement\": \"í™•ì •ì¼ìë¥¼ ë°›ì€ ê²½ìš° ìš°ì„ ë³€ì œê¶Œì´ ë°œìƒí•œë‹¤.\",\\n      \"reason\": \"The context states that if a tenant receives a confirmed date, they have priority rights, which directly supports the statement.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"ì„ëŒ€ì°¨ê³„ì•½ì„œì™€ í™•ì •ì¼ì ì¦ëª…ì„œ ì‚¬ë³¸ì„ í™•ë³´í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not explicitly mention the need to secure copies of the lease agreement and confirmation date certificate, making this statement unverifiable.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"ë“±ê¸°ë¶€ë“±ë³¸ì„ í™•ì¸í•˜ì—¬ ì„ëŒ€ì¸ì˜ ì†Œìœ ê¶Œ ë° ë‹¤ë¥¸ ì„ì°¨ì¸ì˜ ê³„ì•½ ì—¬ë¶€ë¥¼ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not provide information about checking the registration details to confirm ownership or other tenants\\' contracts, so this statement cannot be inferred.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"ì „ì…ì‹ ê³ ê°€ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context mentions that tenants must complete their resident registration, but it does not state that they need to verify its completion, making this statement unverifiable.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"ì„ëŒ€ì¸ì—ê²Œ ì´ì¤‘ê³„ì•½ ì‚¬ì‹¤ì„ í†µì§€í•˜ê³ , ë³´ì¦ê¸ˆ ë°˜í™˜ì„ ìš”êµ¬í•˜ëŠ” ë‚´ìš©ì¦ëª…ì„ ë³´ë‚´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not mention the need to notify the landlord about double contracts or send a formal request for the return of the deposit, making this statement unverifiable.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"í•„ìš”ì‹œ ë²•ë¥  ìƒë‹´ì„ í†µí•´ ì¶”ê°€ ì¡°ì¹˜ë¥¼ ë…¼ì˜í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not provide any information about the necessity of legal consultation for further actions, so this statement cannot be inferred.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì œ3ì¡°ì˜2ì— ë”°ë¥´ë©´, í™•ì •ì¼ìë¥¼ ë°›ì€ ê²½ìš° ì„ì°¨ì¸ì€ ìš°ì„ ë³€ì œê¶Œì„ ê°€ì§„ë‹¤.\",\\n      \"reason\": \"The context directly supports this statement as it references the law that grants priority rights to tenants who have received a confirmed date.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì œ8ì¡°ì— ë”°ë¥´ë©´, ì„ì°¨ì¸ì€ ê²½ë§¤ ì‹œ ë³´ì¦ê¸ˆì˜ ìš°ì„ ë³€ì œë¥¼ ë°›ì„ ìˆ˜ ìˆë‹¤.\",\\n      \"reason\": \"The context supports this statement as it mentions that tenants can receive priority repayment of their deposits during auction proceedings according to the law.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"ê²½ë§¤ ì ˆì°¨ê°€ ì‹œì‘ë˜ë©´, ë²•ì›ì— ì„ì°¨ê¶Œì„ ì£¼ì¥í•˜ê¸° ìœ„í•´ ì„ëŒ€ì°¨ê³„ì•½ì„œ, í™•ì •ì¼ì ì¦ëª…ì„œ, ì „ì…ì‹ ê³ ì„œ ë“±ì„ ì œì¶œí•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not provide specific details about the documents required to assert tenant rights in court during auction proceedings, making this statement unverifiable.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"ê²½ë§¤ê°€ ì§„í–‰ë˜ë©´, ì„ì°¨ì¸ì€ ë³´ì¦ê¸ˆì˜ ìš°ì„ ë³€ì œë¥¼ ë°›ì„ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ê²½ë§¤ ê²°ê³¼ì— ë”°ë¼ ê²°ì •ëœë‹¤.\",\\n      \"reason\": \"The context supports this statement as it indicates that tenants can receive priority repayment of their deposits during the auction process, which is contingent on the auction results.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"ëŒ€ë²•ì› íŒë¡€ì— ë”°ë¥´ë©´, í™•ì •ì¼ìë¥¼ ë°›ì€ ì„ì°¨ì¸ì€ ê²½ë§¤ ì ˆì°¨ì—ì„œ ìš°ì„ ë³€ì œê¶Œì„ ì£¼ì¥í•  ìˆ˜ ìˆë‹¤.\",\\n      \"reason\": \"The context supports this statement as it aligns with the legal precedent that allows tenants with a confirmed date to assert priority rights during auction proceedings.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"í™•ì •ì¼ìë¥¼ ë°›ì€ ì„ì°¨ì¸ì€ ë‹¤ë¥¸ ì±„ê¶Œìë³´ë‹¤ ìš°ì„ ì ìœ¼ë¡œ ë³´ì¦ê¸ˆì„ íšŒìˆ˜í•  ìˆ˜ ìˆëŠ” ê¶Œë¦¬ë¥¼ ê°€ì§„ë‹¤.\",\\n      \"reason\": \"The context supports this statement as it states that tenants with a confirmed date have the right to recover their deposits before other creditors.\",\\n      \"verdict\": 1\\n    },\\n    {\\n      \"statement\": \"í™•ì •ì¼ìëŠ” ì–¸ì œ ë¶€ì—¬ë°›ì•˜ëŠ”ì§€ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not provide information about the necessity to verify when the confirmed date was granted, making this statement unverifiable.\",\\n      \"verdict\": 0\\n    },\\n    {\\n      \"statement\": \"ì´ì¤‘ê³„ì•½ì´ ë°œìƒí•œ ì„ì°¨ì¸ì€ ëª‡ ëª…ì¸ì§€ í™•ì¸í•´ì•¼ í•œë‹¤.\",\\n      \"reason\": \"The context does not mention the need to verify the number of tenants involved in double contracts, making this statement', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1770018106, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_1590f93f9d', usage=CompletionUsage(completion_tokens=3072, prompt_tokens=17829, total_tokens=20901, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=13440)))\n",
      "</completion>\n",
      "</generation>\n",
      "\n",
      "</failed_attempts>\n",
      "\n",
      "<last_exception>\n",
      "    The output is incomplete due to a max_tokens length limit.\n",
      "</last_exception>)\n",
      "2026-02-02 16:42:14,730 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:42:15,267 - ragas.prompt.pydantic_prompt - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2026-02-02 16:42:15,799 - ragas.executor - ERROR - Exception raised in Job[7]: AttributeError('OpenAIEmbeddings' object has no attribute 'embed_query')\n",
      "2026-02-02 16:42:17,668 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:42:20,745 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:42:24,547 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:42:28,852 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:42:32,066 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:42:35,302 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:42:38,136 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:42:40,824 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:42:44,756 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:42:48,132 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:42:55,089 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 16:43:03,332 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# One-cell: RAGAS compare + save (+ delta outputs + dataset fingerprint)\n",
    "# Adds:\n",
    "#  - delta_summary.csv\n",
    "#  - delta_detail.csv\n",
    "#  - top_regressions.csv\n",
    "#  - top_improvements.csv\n",
    "#  - meta.json: testset fingerprint (path/lines/sha1) if TESTSET_JSONL exists\n",
    "# ============================================================\n",
    "\n",
    "import time, re, json, hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "from openai import OpenAI\n",
    "from ragas import evaluate\n",
    "from ragas.llms import llm_factory\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# LLM (non-deprecated)\n",
    "# ----------------------------\n",
    "client = OpenAI()\n",
    "llm = llm_factory(\"gpt-4o-mini\", client=client)  # âœ… client ì „ë‹¬\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# METRICS (version-tolerant)\n",
    "# ----------------------------\n",
    "def build_metrics():\n",
    "    from ragas.metrics import context_precision, context_recall, faithfulness\n",
    "    metrics = [context_precision, context_recall, faithfulness]\n",
    "    try:\n",
    "        from ragas.metrics import answer_relevancy\n",
    "        metrics.append(answer_relevancy)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    return metrics\n",
    "\n",
    "METRICS = build_metrics()\n",
    "print(\"âœ… METRICS:\", [getattr(m, \"__name__\", m.__class__.__name__) for m in METRICS])\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# JSON helpers (safe)\n",
    "# ----------------------------\n",
    "def _json_safe(obj):\n",
    "    try:\n",
    "        json.dumps(obj, ensure_ascii=False)\n",
    "        return obj\n",
    "    except TypeError:\n",
    "        if hasattr(obj, \"model_dump\"):\n",
    "            return obj.model_dump()\n",
    "        if hasattr(obj, \"dict\"):\n",
    "            return obj.dict()\n",
    "        if hasattr(obj, \"__dict__\"):\n",
    "            return obj.__dict__\n",
    "        return str(obj)\n",
    "\n",
    "def _write_json(path: Path, data):\n",
    "    path.write_text(json.dumps(_json_safe(data), ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def _write_jsonl(path: Path, rows):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(_json_safe(r), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def _df_to_jsonl_rows(df, prefer_cols):\n",
    "    cols = [c for c in prefer_cols if c in df.columns]\n",
    "    if cols:\n",
    "        df = df[cols].copy()\n",
    "    return df.to_dict(orient=\"records\"), cols\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# dataset fingerprint helpers\n",
    "# ----------------------------\n",
    "def _file_sha1(path: Path, chunk_size=1024 * 1024) -> str:\n",
    "    h = hashlib.sha1()\n",
    "    with path.open(\"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def _count_jsonl_lines(path: Path) -> int:\n",
    "    n = 0\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                n += 1\n",
    "    return n\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# core: eval + detail/summary frames\n",
    "# ----------------------------\n",
    "def eval_ragas_with_details(samples, run_tag: str):\n",
    "    ds = Dataset.from_list(samples)\n",
    "\n",
    "    t0 = time.time()\n",
    "    res = evaluate(dataset=ds, metrics=METRICS, llm=llm)  # âœ… llmì€ ì—¬ê¸°ë¡œ\n",
    "    eval_sec = time.time() - t0\n",
    "\n",
    "    t1 = time.time()\n",
    "    detail_df = res.to_pandas() if hasattr(res, \"to_pandas\") else pd.DataFrame()\n",
    "    to_pandas_sec = time.time() - t1\n",
    "\n",
    "    samples_df = pd.DataFrame(samples)\n",
    "\n",
    "    # merge\n",
    "    if len(detail_df) == len(samples_df) and len(detail_df) > 0:\n",
    "        out_detail = pd.concat(\n",
    "            [samples_df.reset_index(drop=True), detail_df.reset_index(drop=True)],\n",
    "            axis=1\n",
    "        )\n",
    "    else:\n",
    "        out_detail = samples_df.copy()\n",
    "\n",
    "    out_detail[\"run_tag\"] = run_tag\n",
    "    out_detail[\"eval_seconds\"] = round(eval_sec, 3)\n",
    "    out_detail[\"to_pandas_seconds\"] = round(to_pandas_sec, 3)\n",
    "\n",
    "    # summary\n",
    "    summary = {}\n",
    "    if len(detail_df) > 0:\n",
    "        summary = detail_df.mean(numeric_only=True).to_dict()\n",
    "    elif isinstance(res, dict):\n",
    "        summary = {k: float(v) for k, v in res.items() if isinstance(v, (int, float))}\n",
    "\n",
    "    summary[\"run_tag\"] = run_tag\n",
    "    summary[\"eval_seconds\"] = round(eval_sec, 3)\n",
    "    summary[\"to_pandas_seconds\"] = round(to_pandas_sec, 3)\n",
    "\n",
    "    return res, out_detail, pd.DataFrame([summary])\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# run dir allocator\n",
    "# ----------------------------\n",
    "def _next_run_dir(project_root: Path, prefix: str):\n",
    "    runs_root = Path(project_root) / \"results\" / \"ragas_runs\"\n",
    "    runs_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pat = re.compile(rf\"^{re.escape(prefix)}_(\\d{{4}})_\")\n",
    "    nums = []\n",
    "    for p in runs_root.iterdir():\n",
    "        if p.is_dir():\n",
    "            m = pat.match(p.name)\n",
    "            if m:\n",
    "                nums.append(int(m.group(1)))\n",
    "    next_id = (max(nums) + 1) if nums else 1\n",
    "\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = runs_root / f\"{prefix}_{next_id:04d}_{ts}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=False)\n",
    "    return run_dir, next_id, ts\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# delta builders\n",
    "# ----------------------------\n",
    "def _pick_question_col(df: pd.DataFrame) -> str:\n",
    "    for c in [\"question\", \"normalized_question\", \"normalized_query\", \"query\"]:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return \"\"\n",
    "\n",
    "def _ensure_question_id(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # prefer an existing stable id\n",
    "    for c in [\"question_id\", \"id\", \"sample_id\"]:\n",
    "        if c in df.columns:\n",
    "            df = df.copy()\n",
    "            df[\"__qid__\"] = df[c].astype(str)\n",
    "            return df\n",
    "    # fallback: hash question text\n",
    "    qcol = _pick_question_col(df)\n",
    "    df = df.copy()\n",
    "    if qcol:\n",
    "        def _h(x: str) -> str:\n",
    "            s = (x or \"\").strip().encode(\"utf-8\")\n",
    "            return hashlib.sha1(s).hexdigest()[:12]\n",
    "        df[\"__qid__\"] = df[qcol].astype(str).map(_h)\n",
    "    else:\n",
    "        df[\"__qid__\"] = [f\"row{i:04d}\" for i in range(len(df))]\n",
    "    return df\n",
    "\n",
    "def _metric_cols(df: pd.DataFrame) -> list:\n",
    "    # heuristic: numeric columns from ragas result + common metric names\n",
    "    prefer = [\n",
    "        \"context_precision\", \"context_recall\", \"faithfulness\", \"answer_relevancy\",\n",
    "        \"ContextPrecision\", \"ContextRecall\", \"Faithfulness\", \"AnswerRelevancy\",\n",
    "    ]\n",
    "    cols = [c for c in prefer if c in df.columns]\n",
    "    if cols:\n",
    "        return cols\n",
    "\n",
    "    # fallback: any numeric columns that are not obvious non-metrics\n",
    "    exclude = set([\"eval_seconds\", \"to_pandas_seconds\"])\n",
    "    num_cols = []\n",
    "    for c in df.columns:\n",
    "        if c in exclude:\n",
    "            continue\n",
    "        if pd.api.types.is_numeric_dtype(df[c]):\n",
    "            num_cols.append(c)\n",
    "    return num_cols\n",
    "\n",
    "\n",
    "def _make_delta_summary(summary_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # summary_df has rows: baseline/experiment\n",
    "    metric_cols = [c for c in summary_df.columns if c not in [\"run_tag\"]]\n",
    "    base = summary_df[summary_df[\"run_tag\"] == \"baseline\"].iloc[0].to_dict()\n",
    "    exp  = summary_df[summary_df[\"run_tag\"] == \"experiment\"].iloc[0].to_dict()\n",
    "\n",
    "    rows = []\n",
    "    for c in metric_cols:\n",
    "        if c == \"run_tag\":\n",
    "            continue\n",
    "        b = base.get(c)\n",
    "        e = exp.get(c)\n",
    "        if isinstance(b, (int, float)) and isinstance(e, (int, float)):\n",
    "            rows.append({\"metric\": c, \"baseline\": float(b), \"experiment\": float(e), \"delta\": float(e - b)})\n",
    "        else:\n",
    "            # keep non-numeric too\n",
    "            rows.append({\"metric\": c, \"baseline\": b, \"experiment\": e, \"delta\": None})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def _make_delta_detail(detail_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = _ensure_question_id(detail_df)\n",
    "\n",
    "    base = df[df[\"run_tag\"] == \"baseline\"].copy()\n",
    "    exp  = df[df[\"run_tag\"] == \"experiment\"].copy()\n",
    "\n",
    "    # --- choose question column ---\n",
    "    qcol = _pick_question_col(df)\n",
    "\n",
    "    # --- metric columns ---\n",
    "    mcols = _metric_cols(df)\n",
    "\n",
    "    # --- info columns to keep (these overlap across base/exp) ---\n",
    "    info_cols = [\"__qid__\"]\n",
    "    if qcol:\n",
    "        info_cols.append(qcol)\n",
    "    for c in [\"ground_truth\", \"reference\", \"answer\", \"contexts\"]:\n",
    "        if c in df.columns:\n",
    "            info_cols.append(c)\n",
    "\n",
    "    # --- select + rename (so no overlap) ---\n",
    "    base_small = base[info_cols + mcols].copy()\n",
    "    exp_small  = exp[info_cols + mcols].copy()\n",
    "\n",
    "    rename_base = {c: f\"{c}_base\" for c in info_cols if c != \"__qid__\"}\n",
    "    rename_exp  = {c: f\"{c}_exp\"  for c in info_cols if c != \"__qid__\"}\n",
    "    rename_base.update({c: f\"{c}_base\" for c in mcols})\n",
    "    rename_exp.update({c: f\"{c}_exp\"  for c in mcols})\n",
    "\n",
    "    base_small = base_small.rename(columns=rename_base)\n",
    "    exp_small  = exp_small.rename(columns=rename_exp)\n",
    "\n",
    "    # --- merge safely ---\n",
    "    merged = exp_small.merge(base_small, on=\"__qid__\", how=\"outer\")\n",
    "\n",
    "    # --- compute deltas ---\n",
    "    for c in mcols:\n",
    "        cb = f\"{c}_base\"\n",
    "        ce = f\"{c}_exp\"\n",
    "        if cb in merged.columns and ce in merged.columns:\n",
    "            merged[f\"{c}_delta\"] = merged[ce] - merged[cb]\n",
    "\n",
    "    # --- convenience: make a unified question column (prefer exp, fallback base) ---\n",
    "    if qcol:\n",
    "        qe = f\"{qcol}_exp\"\n",
    "        qb = f\"{qcol}_base\"\n",
    "        if qe in merged.columns or qb in merged.columns:\n",
    "            merged[qcol] = None\n",
    "            if qe in merged.columns:\n",
    "                merged[qcol] = merged[qe]\n",
    "            if qb in merged.columns:\n",
    "                merged[qcol] = merged[qcol].fillna(merged[qb])\n",
    "\n",
    "    # --- order columns nicely ---\n",
    "    ordered = [\"__qid__\"]\n",
    "    if qcol and qcol in merged.columns:\n",
    "        ordered.append(qcol)\n",
    "\n",
    "    # keep references (unified view is optional; we keep exp/base separately)\n",
    "    for c in [\"ground_truth\", \"reference\"]:\n",
    "        # add unified if you want; here we keep exp/base columns only\n",
    "        pass\n",
    "\n",
    "    # metrics grouped\n",
    "    for c in mcols:\n",
    "        for suf in [\"_base\", \"_exp\", \"_delta\"]:\n",
    "            col = f\"{c}{suf}\"\n",
    "            if col in merged.columns:\n",
    "                ordered.append(col)\n",
    "\n",
    "    # then common info columns (exp/base)\n",
    "    tail_info = []\n",
    "    for c in [\"ground_truth\", \"reference\", \"answer\", \"contexts\"]:\n",
    "        ce, cb = f\"{c}_exp\", f\"{c}_base\"\n",
    "        if ce in merged.columns:\n",
    "            tail_info.append(ce)\n",
    "        if cb in merged.columns:\n",
    "            tail_info.append(cb)\n",
    "\n",
    "    remaining = [c for c in merged.columns if c not in ordered and c not in tail_info]\n",
    "    return merged[ordered + remaining + tail_info].copy()\n",
    "\n",
    "\n",
    "\n",
    "def _make_top_changes(delta_detail_df: pd.DataFrame, top_k=10) -> tuple[pd.DataFrame, pd.DataFrame, str]:\n",
    "    # choose primary metric for sorting\n",
    "    candidates = [\n",
    "        \"answer_relevancy_delta\", \"AnswerRelevancy_delta\",\n",
    "        \"faithfulness_delta\", \"Faithfulness_delta\",\n",
    "        \"context_precision_delta\", \"ContextPrecision_delta\",\n",
    "        \"context_recall_delta\", \"ContextRecall_delta\",\n",
    "    ]\n",
    "    sort_col = next((c for c in candidates if c in delta_detail_df.columns), None)\n",
    "    if sort_col is None:\n",
    "        # fallback: first *_delta numeric column\n",
    "        delta_cols = [c for c in delta_detail_df.columns if c.endswith(\"_delta\") and pd.api.types.is_numeric_dtype(delta_detail_df[c])]\n",
    "        sort_col = delta_cols[0] if delta_cols else \"\"\n",
    "\n",
    "    if not sort_col:\n",
    "        # nothing to rank\n",
    "        return pd.DataFrame(), pd.DataFrame(), \"\"\n",
    "\n",
    "    # pick minimal view columns\n",
    "    qcol = _pick_question_col(delta_detail_df)\n",
    "    view_cols = [c for c in [\"__qid__\", qcol, sort_col] if c and c in delta_detail_df.columns]\n",
    "    # add also base/exp of that metric if present\n",
    "    base_col = sort_col.replace(\"_delta\", \"_base\")\n",
    "    exp_col  = sort_col.replace(\"_delta\", \"_exp\")\n",
    "    for c in [base_col, exp_col]:\n",
    "        if c in delta_detail_df.columns and c not in view_cols:\n",
    "            view_cols.append(c)\n",
    "\n",
    "    regress = delta_detail_df.sort_values(sort_col, ascending=True).head(top_k)[view_cols].copy()\n",
    "    improve = delta_detail_df.sort_values(sort_col, ascending=False).head(top_k)[view_cols].copy()\n",
    "    return regress, improve, sort_col\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# main: compare + save (csv/json + optional snapshots)\n",
    "# ----------------------------\n",
    "def run_compare_and_save_all(\n",
    "    base_samples,\n",
    "    exp_samples,\n",
    "    project_root: Path,\n",
    "    prefix=\"ragas_compare\",\n",
    "    save_snapshots=True,\n",
    "    save_config=True,\n",
    "    save_samples_jsonl=True,\n",
    "    save_trace_jsonl=True,\n",
    "    trace_cols_priority=None,\n",
    "    save_delta_outputs=True,\n",
    "    top_k=10,\n",
    "):\n",
    "    # 1) evaluate\n",
    "    base_res, base_detail_df, base_summary_df = eval_ragas_with_details(base_samples, \"baseline\")\n",
    "    exp_res,  exp_detail_df,  exp_summary_df  = eval_ragas_with_details(exp_samples,  \"experiment\")\n",
    "\n",
    "    summary_df = pd.concat([base_summary_df, exp_summary_df], ignore_index=True)\n",
    "    detail_df  = pd.concat([base_detail_df,  exp_detail_df],  ignore_index=True)\n",
    "\n",
    "    # 2) run dir\n",
    "    run_dir, run_id, ts = _next_run_dir(project_root, prefix)\n",
    "\n",
    "    # 3) basic outputs\n",
    "    out_summary = run_dir / \"summary.csv\"\n",
    "    out_detail  = run_dir / \"detail.csv\"\n",
    "    out_meta    = run_dir / \"meta.json\"\n",
    "\n",
    "    summary_df.to_csv(out_summary, index=False, encoding=\"utf-8-sig\")\n",
    "    detail_df.to_csv(out_detail, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # --- dataset fingerprint (optional) ---\n",
    "    testset_info = None\n",
    "    if \"TESTSET_JSONL\" in globals():\n",
    "        p = Path(globals()[\"TESTSET_JSONL\"])\n",
    "        if p.exists():\n",
    "            testset_info = {\n",
    "                \"testset_path\": str(p),\n",
    "                \"testset_lines\": _count_jsonl_lines(p),\n",
    "                \"testset_sha1\": _file_sha1(p),\n",
    "            }\n",
    "\n",
    "    meta = {\n",
    "        \"created_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"run_id\": run_id,\n",
    "        \"timestamp\": ts,\n",
    "        \"run_dir\": str(run_dir),\n",
    "        \"prefix\": prefix,\n",
    "        \"n_base_samples\": len(base_samples),\n",
    "        \"n_exp_samples\": len(exp_samples),\n",
    "        \"metrics\": [getattr(m, \"__name__\", m.__class__.__name__) for m in METRICS],\n",
    "        \"llm\": {\"model\": \"gpt-4o-mini\"},\n",
    "        \"testset\": testset_info,\n",
    "    }\n",
    "    _write_json(out_meta, meta)\n",
    "\n",
    "    # 3-1) DELTA outputs (â­ï¸ NEW)\n",
    "    extra = {}\n",
    "    if save_delta_outputs:\n",
    "        # delta_summary\n",
    "        delta_summary_df = _make_delta_summary(summary_df)\n",
    "        out_delta_summary = run_dir / \"delta_summary.csv\"\n",
    "        delta_summary_df.to_csv(out_delta_summary, index=False, encoding=\"utf-8-sig\")\n",
    "        extra[\"out_delta_summary\"] = str(out_delta_summary)\n",
    "\n",
    "        # delta_detail\n",
    "        delta_detail_df = _make_delta_detail(detail_df)\n",
    "        out_delta_detail = run_dir / \"delta_detail.csv\"\n",
    "        delta_detail_df.to_csv(out_delta_detail, index=False, encoding=\"utf-8-sig\")\n",
    "        extra[\"out_delta_detail\"] = str(out_delta_detail)\n",
    "\n",
    "        # top changes (regressions/improvements)\n",
    "        top_regress, top_improve, sort_col = _make_top_changes(delta_detail_df, top_k=top_k)\n",
    "        out_top_regress = run_dir / \"top_regressions.csv\"\n",
    "        out_top_improve = run_dir / \"top_improvements.csv\"\n",
    "        top_regress.to_csv(out_top_regress, index=False, encoding=\"utf-8-sig\")\n",
    "        top_improve.to_csv(out_top_improve, index=False, encoding=\"utf-8-sig\")\n",
    "        extra[\"out_top_regressions\"] = str(out_top_regress)\n",
    "        extra[\"out_top_improvements\"] = str(out_top_improve)\n",
    "        extra[\"top_rank_metric\"] = sort_col\n",
    "\n",
    "    # 4) optional snapshots\n",
    "    if save_snapshots:\n",
    "        # 4-1) config.json\n",
    "        if save_config:\n",
    "            cfg_payload = {\n",
    "                \"created_at\": datetime.now().isoformat(),\n",
    "                \"llm\": {\"model\": \"gpt-4o-mini\"},\n",
    "                \"metrics\": [getattr(m, \"__name__\", m.__class__.__name__) for m in METRICS],\n",
    "                \"base_cfg\": _json_safe(globals().get(\"base_cfg\")) if \"base_cfg\" in globals() else None,\n",
    "                \"exp_cfg\":  _json_safe(globals().get(\"exp_cfg\"))  if \"exp_cfg\"  in globals() else None,\n",
    "            }\n",
    "            out_config = run_dir / \"config.json\"\n",
    "            _write_json(out_config, cfg_payload)\n",
    "            extra[\"out_config\"] = str(out_config)\n",
    "\n",
    "        # 4-2) input samples jsonl\n",
    "        if save_samples_jsonl:\n",
    "            out_samples_base = run_dir / \"samples_base.jsonl\"\n",
    "            out_samples_exp  = run_dir / \"samples_exp.jsonl\"\n",
    "            _write_jsonl(out_samples_base, base_samples)\n",
    "            _write_jsonl(out_samples_exp,  exp_samples)\n",
    "            extra[\"out_samples_base\"] = str(out_samples_base)\n",
    "            extra[\"out_samples_exp\"]  = str(out_samples_exp)\n",
    "\n",
    "        # 4-3) trace jsonl (from detail_df)\n",
    "        if save_trace_jsonl:\n",
    "            if trace_cols_priority is None:\n",
    "                trace_cols_priority = [\n",
    "                    \"id\", \"sample_id\", \"__qid__\",\n",
    "                    \"question\", \"normalized_question\", \"normalized_query\", \"query\",\n",
    "                    \"answer\", \"ground_truth\", \"reference\",\n",
    "                    \"contexts\",\n",
    "                    \"_trace\",\n",
    "                    \"retrieved_doc_ids\", \"retrieved_docs\", \"retrieval_scores\",\n",
    "                    \"rerank_selected_ids\", \"rerank_scores\",\n",
    "                    \"final_context_ids\", \"final_contexts\",\n",
    "                    \"latency_ms\", \"latency_sec\",\n",
    "                    \"run_tag\",\n",
    "                ]\n",
    "            trace_rows, used_cols = _df_to_jsonl_rows(detail_df, trace_cols_priority)\n",
    "            out_trace = run_dir / \"trace.jsonl\"\n",
    "            _write_jsonl(out_trace, trace_rows)\n",
    "            extra[\"out_trace\"] = str(out_trace)\n",
    "            extra[\"trace_cols_used\"] = used_cols\n",
    "\n",
    "    return {\n",
    "        \"base_res\": base_res,\n",
    "        \"exp_res\": exp_res,\n",
    "        \"summary_df\": summary_df,\n",
    "        \"detail_df\": detail_df,\n",
    "        \"run_dir\": str(run_dir),\n",
    "        \"out_summary\": str(out_summary),\n",
    "        \"out_detail\": str(out_detail),\n",
    "        \"out_meta\": str(out_meta),\n",
    "        **extra,\n",
    "    }\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# RUN\n",
    "# ----------------------------\n",
    "result = run_compare_and_save_all(\n",
    "    base_samples=BASE_SAMPLES,\n",
    "    exp_samples=EXP_SAMPLES,\n",
    "    project_root=PROJECT_ROOT,\n",
    "    prefix=\"ragas_compare\",\n",
    "    save_snapshots=True,\n",
    "    save_config=True,\n",
    "    save_samples_jsonl=True,\n",
    "    save_trace_jsonl=True,\n",
    "    save_delta_outputs=True,  # âœ… NEW\n",
    "    top_k=10,                 # âœ… NEW\n",
    ")\n",
    "\n",
    "print(\"âœ… run_dir:\", result[\"run_dir\"])\n",
    "print(\"âœ… saved:\", result[\"out_summary\"], result[\"out_detail\"], result[\"out_meta\"])\n",
    "\n",
    "# NEW delta outputs\n",
    "if \"out_delta_summary\" in result:\n",
    "    print(\"âœ… delta saved:\", result[\"out_delta_summary\"], result.get(\"out_delta_detail\"))\n",
    "    print(\"âœ… top changes metric:\", result.get(\"top_rank_metric\"))\n",
    "    print(\"âœ… top regressions:\", result.get(\"out_top_regressions\"))\n",
    "    print(\"âœ… top improvements:\", result.get(\"out_top_improvements\"))\n",
    "\n",
    "# snapshots\n",
    "if \"out_config\" in result:\n",
    "    print(\"âœ… extra saved config :\", result[\"out_config\"])\n",
    "if \"out_samples_base\" in result:\n",
    "    print(\"âœ… extra saved samples:\", result[\"out_samples_base\"], \"and\", result.get(\"out_samples_exp\"))\n",
    "if \"out_trace\" in result:\n",
    "    print(\"âœ… extra saved trace  :\", result[\"out_trace\"])\n",
    "    print(\"âœ… trace columns used :\", result.get(\"trace_cols_used\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6845ece-5d8b-47af-8aaf-0a1dc5ef87c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa7f586-c190-4912-a1c7-b08b1f1703db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0e570-c6a8-44d7-8628-8f2fb44c5755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9df58dc-5dbf-4542-945f-0cf156a581dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83ccdd3-e88b-4212-b6a2-fe100c7f1cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860dd3a7-6a46-4477-84a4-ef2861fe6829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c068515-0f85-4ef4-84dd-c22ad87302dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e96bfa9-487a-49b6-b4b1-df555cf06ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3be758-526b-4c43-a796-c5036f476881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5335ed-f34b-4677-950b-bd10cb6a0cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c55f40-ddfa-4a3a-a7dc-abd143d6eda4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97946b48-b0d8-4dd8-9a89-f6a451d7e36a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d03785-23ae-4846-9368-4a45eaf6d3be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ab5d29-e922-4a2c-a0c0-fcb337ae36ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv chatbot_app)",
   "language": "python",
   "name": "chatbot-app-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
