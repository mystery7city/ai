{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4decb793",
   "metadata": {},
   "source": [
    "# RAGAS Evaluation Notebook (Clean)\n",
    "\n",
    "This notebook evaluates **baseline vs experiment** RAG pipelines using RAGAS and saves **summary/detail** outputs per run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf76203-b1ff-4ee3-b17e-06004982adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdaa3541-ee50-497b-81cd-91c2274ba4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1b5f49e-4fde-42f4-a42a-40dd814c571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys, subprocess, textwrap\n",
    "\n",
    "# def sh(cmd):\n",
    "#     print(\">\", cmd)\n",
    "#     r = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "#     print(r.stdout)\n",
    "#     if r.stderr.strip():\n",
    "#         print(\"[stderr]\")\n",
    "#         print(r.stderr)\n",
    "\n",
    "# print(\"python:\", sys.executable)\n",
    "# print(\"version:\", sys.version)\n",
    "\n",
    "# # í˜„ì¬ íŒ¨í‚¤ì§€ ìƒíƒœ í™•ì¸\n",
    "# sh(\"python -c \\\"import numpy; print('numpy', numpy.__version__)\\\"\")\n",
    "# sh(\"python -c \\\"import pyarrow; print('pyarrow', pyarrow.__version__)\\\"\")\n",
    "# sh(\"python -c \\\"import datasets; print('datasets', datasets.__version__)\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b2f1ee4-078f-4f37-9e75-6da6b135d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys, subprocess\n",
    "\n",
    "# def pip(cmd):\n",
    "#     print(\">\", cmd)\n",
    "#     r = subprocess.run([sys.executable, \"-m\", \"pip\"] + cmd.split(), capture_output=True, text=True)\n",
    "#     print(r.stdout)\n",
    "#     if r.stderr.strip():\n",
    "#         print(\"[stderr]\")\n",
    "#         print(r.stderr)\n",
    "\n",
    "# # 1) ì œê±°\n",
    "# pip(\"uninstall -y pyarrow datasets numpy\")\n",
    "\n",
    "# # 2) ì¬ì„¤ì¹˜: numpy<2 + ìµœì‹  pyarrow + datasets(ë„ˆê°€ ì“°ë˜ ë²„ì „)\n",
    "# pip(\"install numpy<2 pyarrow>=14 datasets==2.19.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f6d4dab-9416-4c6e-b037-2526c17623ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darks\\anaconda3\\envs\\tfenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… METRICS: ['ContextPrecision', 'ContextRecall', 'Faithfulness', 'AnswerRelevancy']\n"
     ]
    }
   ],
   "source": [
    "# ---- RAGAS metrics: version-tolerant loader ----\n",
    "def build_metrics():\n",
    "    # 1) ê°€ì¥ í˜¸í™˜ ì˜ ë˜ëŠ” \"í•¨ìˆ˜/ê°ì²´\" ìŠ¤íƒ€ì¼(ë§ì€ ë²„ì „ì—ì„œ ì¡´ì¬)\n",
    "    try:\n",
    "        from ragas.metrics import context_precision, context_recall, faithfulness, answer_relevancy\n",
    "        return [context_precision, context_recall, faithfulness, answer_relevancy]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) ì–´ë–¤ ë²„ì „ì€ ragas.metrics ì— í´ë˜ìŠ¤ê°€ ì§ì ‘ ìˆìŒ\n",
    "    try:\n",
    "        from ragas.metrics import ContextPrecision, ContextRecall, Faithfulness, AnswerRelevancy\n",
    "        return [ContextPrecision(), ContextRecall(), Faithfulness(), AnswerRelevancy()]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) ì–´ë–¤ ë²„ì „ì€ ragas.metrics._somewhere / ragas.metrics.critic ë“±ìœ¼ë¡œ í©ì–´ì ¸ ìˆìŒ\n",
    "    #    -> ë§ˆì§€ë§‰ ìˆ˜ë‹¨: ragas.metrics ì•ˆì—ì„œ ì´ë¦„ì„ ì°¾ì•„ ìƒì„±\n",
    "    import ragas.metrics as m\n",
    "\n",
    "    wanted = [\"ContextPrecision\", \"ContextRecall\", \"Faithfulness\", \"AnswerRelevancy\"]\n",
    "    found = []\n",
    "    for name in wanted:\n",
    "        if hasattr(m, name):\n",
    "            found.append(getattr(m, name)())\n",
    "    if found:\n",
    "        return found\n",
    "\n",
    "    raise ImportError(\n",
    "        \"RAGAS metrics import failed. \"\n",
    "        \"Try upgrading ragas, or paste `pip show ragas` + `python -c \\\"import ragas; print(ragas.__version__)\\\"`.\"\n",
    "    )\n",
    "\n",
    "METRICS = build_metrics()\n",
    "print(\"âœ… METRICS:\", [getattr(x, '__name__', x.__class__.__name__) for x in METRICS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24085385-5708-4be6-8d18-1c17f11f456d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ragas version: 0.3.2\n"
     ]
    }
   ],
   "source": [
    "import ragas\n",
    "print(\"ragas version:\", getattr(ragas, \"__version__\", \"unknown\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85cba8ec-b4a6-4dcf-8fda-13b78e62e686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… METRICS: ['ContextPrecision', 'ContextRecall', 'Faithfulness', 'AnswerRelevancy']\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import context_precision, context_recall, faithfulness, answer_relevancy\n",
    "\n",
    "METRICS = [context_precision, context_recall, faithfulness, answer_relevancy]\n",
    "print(\"âœ… METRICS:\", [getattr(m, \"__name__\", m.__class__.__name__) for m in METRICS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ac8615-1f58-462f-8f2e-811d673d8d56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22cee99-4aaa-40fb-bbbb-451aa89c8c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53db9df-4693-41e1-b255-bebb90c4be53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "663e3c80",
   "metadata": {},
   "source": [
    "## 0) Environment & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3772574e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: C:\\Users\\darks\\Desktop\\ìë£Œë“¤\\AIìë£Œ\\ai\\source\\chatbot_app\n",
      "TESTSET_PATH: C:\\Users\\darks\\Desktop\\ìë£Œë“¤\\AIìë£Œ\\ai\\source\\chatbot_app\\ragas_testset_mini_0001.jsonl\n",
      "exists: True\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PATH CONFIG (only this cell is modified)\n",
    "# ============================================================\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys, importlib\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# âœ… í”„ë¡œì íŠ¸ ë£¨íŠ¸ (ìƒˆ ê²½ë¡œ)\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\darks\\Desktop\\ìë£Œë“¤\\AIìë£Œ\\ai\\source\\chatbot_app\")\n",
    "\n",
    "# âœ… ëª¨ë“ˆ ê²½ë¡œ (ì›ë˜ ì“°ë˜ êµ¬ì¡° ê·¸ëŒ€ë¡œ)\n",
    "MODULE_DIR = PROJECT_ROOT / \"modules\"\n",
    "\n",
    "# âœ… í™˜ê²½ë³€ìˆ˜\n",
    "ENV_PATH = PROJECT_ROOT / \".env\"\n",
    "\n",
    "# âœ… ê²°ê³¼ ì €ì¥ ë£¨íŠ¸\n",
    "RUNS_DIR = PROJECT_ROOT / \"results\" / \"ragas_runs\"\n",
    "\n",
    "# â­•ï¸ ì—¬ê¸°ì„œ ì–´ë–¤ í…ŒìŠ¤íŠ¸ì…‹ ì“¸ì§€ ë„¤ê°€ ì§ì ‘ ì„ íƒ\n",
    "# TESTSET_PATH = PROJECT_ROOT / \"ragas_testset_single.jsonl\"\n",
    "# TESTSET_PATH = PROJECT_ROOT / \"ragas_testset_v1_from_docx.jsonl\"\n",
    "TESTSET_PATH = PROJECT_ROOT / \"ragas_testset_mini_0001.jsonl\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# setup\n",
    "# ------------------------------------------------------------\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "sys.path.insert(0, str(MODULE_DIR))\n",
    "importlib.invalidate_caches()\n",
    "\n",
    "if ENV_PATH.exists():\n",
    "    load_dotenv(ENV_PATH)\n",
    "\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"TESTSET_PATH:\", TESTSET_PATH)\n",
    "print(\"exists:\", TESTSET_PATH.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc0093e",
   "metadata": {},
   "source": [
    "## 1) Load testset (JSONL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8108580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… rows: 3\n",
      "âœ… keys example: dict_keys(['question_id', 'question', 'ground_truth', 'reference'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mini-001</td>\n",
       "      <td>ì§‘ì£¼ì¸ì´ ì•„ë¬´ ë§ë„ ì•ˆ í–ˆëŠ”ë° ê³„ì•½ê¸°ê°„ì´ ëë‚¬ì–´ìš”. ì œê°€ ê³„ì† ì‚´ë©´ì„œ ì›”ì„¸ë„ ëƒˆë‹¤ë©´...</td>\n",
       "      <td>ì„ëŒ€ì°¨ ê¸°ê°„ ë§Œë£Œ í›„ ì„ì°¨ì¸ì´ ê³„ì† ê±°ì£¼í•˜ë©° ì°¨ì„ì„ ì§€ê¸‰í•˜ê³ , ì„ëŒ€ì¸ì´ ìƒë‹¹ ê¸°ê°„ ...</td>\n",
       "      <td>ì£¼íƒì„ëŒ€ì°¨ì—ì„œ ë¬µì‹œì  ê°±ì‹ ì˜ ìš”ê±´ê³¼ íš¨ê³¼, ê°±ì‹ ê±°ì ˆÂ·í•´ì§€ í†µì§€ ê¸°ê°„ ê·œì •</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mini-002</td>\n",
       "      <td>ì „ì…ì‹ ê³ ì™€ í™•ì •ì¼ìëŠ” ë°›ì•„ë‘ì—ˆëŠ”ë°, ì§‘ì£¼ì¸ì´ ê°™ì€ ì§‘ìœ¼ë¡œ ë‹¤ë¥¸ ì„ì°¨ì¸ê³¼ ì´ì¤‘ê³„ì•½ì„ ...</td>\n",
       "      <td>ìš°ì„ ë³€ì œê¶Œì€ ì£¼íƒì„ëŒ€ì°¨ì—ì„œ ëŒ€í•­ë ¥ê³¼ í™•ì •ì¼ìê°€ ê²°í•©ë˜ì–´ ì¸ì •ëœë‹¤. ëŒ€í•­ë ¥ì€ ì£¼íƒì˜ ...</td>\n",
       "      <td>ëŒ€í•­ë ¥Â·í™•ì •ì¼ìÂ·ìš°ì„ ë³€ì œê¶Œì˜ ë°œìƒ ìš”ê±´ê³¼ ê²½ë§¤ ë°°ë‹¹ ìˆœìœ„ì— ê´€í•œ ì£¼íƒì„ëŒ€ì°¨ ê·œì •</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mini-003</td>\n",
       "      <td>ì„ëŒ€ì°¨ ê³„ì•½ì„œì— íŠ¹ì•½ìœ¼ë¡œ ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­ì´ ìˆëŠ”ë°, ì´ë¥¼ ì–´ê¸°ê³  ë°˜ë ¤ë™ë¬¼ì„ í‚¤ìš°...</td>\n",
       "      <td>ê³„ì•½ì„œì˜ íŠ¹ì•½ì„ ìœ„ë°˜í–ˆë‹¤ê³  í•´ì„œ ê³§ë°”ë¡œ ê³„ì•½í•´ì§€ê°€ ì¸ì •ë˜ëŠ” ê²ƒì€ ì•„ë‹ˆë©°, ìœ„ë°˜ì˜ ì •...</td>\n",
       "      <td>ì„ëŒ€ì°¨ íŠ¹ì•½ ìœ„ë°˜ ì‹œ í•´ì§€ íŒë‹¨ ê¸°ì¤€(ì‹ ë¢°ê´€ê³„ íŒŒê´´)ê³¼ ë³´ì¦ê¸ˆ ë°˜í™˜Â·ì†í•´ë°°ìƒ ì›ì¹™</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question_id                                           question  \\\n",
       "0    mini-001  ì§‘ì£¼ì¸ì´ ì•„ë¬´ ë§ë„ ì•ˆ í–ˆëŠ”ë° ê³„ì•½ê¸°ê°„ì´ ëë‚¬ì–´ìš”. ì œê°€ ê³„ì† ì‚´ë©´ì„œ ì›”ì„¸ë„ ëƒˆë‹¤ë©´...   \n",
       "1    mini-002  ì „ì…ì‹ ê³ ì™€ í™•ì •ì¼ìëŠ” ë°›ì•„ë‘ì—ˆëŠ”ë°, ì§‘ì£¼ì¸ì´ ê°™ì€ ì§‘ìœ¼ë¡œ ë‹¤ë¥¸ ì„ì°¨ì¸ê³¼ ì´ì¤‘ê³„ì•½ì„ ...   \n",
       "2    mini-003  ì„ëŒ€ì°¨ ê³„ì•½ì„œì— íŠ¹ì•½ìœ¼ë¡œ ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­ì´ ìˆëŠ”ë°, ì´ë¥¼ ì–´ê¸°ê³  ë°˜ë ¤ë™ë¬¼ì„ í‚¤ìš°...   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0  ì„ëŒ€ì°¨ ê¸°ê°„ ë§Œë£Œ í›„ ì„ì°¨ì¸ì´ ê³„ì† ê±°ì£¼í•˜ë©° ì°¨ì„ì„ ì§€ê¸‰í•˜ê³ , ì„ëŒ€ì¸ì´ ìƒë‹¹ ê¸°ê°„ ...   \n",
       "1  ìš°ì„ ë³€ì œê¶Œì€ ì£¼íƒì„ëŒ€ì°¨ì—ì„œ ëŒ€í•­ë ¥ê³¼ í™•ì •ì¼ìê°€ ê²°í•©ë˜ì–´ ì¸ì •ëœë‹¤. ëŒ€í•­ë ¥ì€ ì£¼íƒì˜ ...   \n",
       "2  ê³„ì•½ì„œì˜ íŠ¹ì•½ì„ ìœ„ë°˜í–ˆë‹¤ê³  í•´ì„œ ê³§ë°”ë¡œ ê³„ì•½í•´ì§€ê°€ ì¸ì •ë˜ëŠ” ê²ƒì€ ì•„ë‹ˆë©°, ìœ„ë°˜ì˜ ì •...   \n",
       "\n",
       "                                       reference  \n",
       "0       ì£¼íƒì„ëŒ€ì°¨ì—ì„œ ë¬µì‹œì  ê°±ì‹ ì˜ ìš”ê±´ê³¼ íš¨ê³¼, ê°±ì‹ ê±°ì ˆÂ·í•´ì§€ í†µì§€ ê¸°ê°„ ê·œì •  \n",
       "1   ëŒ€í•­ë ¥Â·í™•ì •ì¼ìÂ·ìš°ì„ ë³€ì œê¶Œì˜ ë°œìƒ ìš”ê±´ê³¼ ê²½ë§¤ ë°°ë‹¹ ìˆœìœ„ì— ê´€í•œ ì£¼íƒì„ëŒ€ì°¨ ê·œì •  \n",
       "2  ì„ëŒ€ì°¨ íŠ¹ì•½ ìœ„ë°˜ ì‹œ í•´ì§€ íŒë‹¨ ê¸°ì¤€(ì‹ ë¢°ê´€ê³„ íŒŒê´´)ê³¼ ë³´ì¦ê¸ˆ ë°˜í™˜Â·ì†í•´ë°°ìƒ ì›ì¹™  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TESTSET_JSONL = PROJECT_ROOT / \"ragas_testset_mini_0001.jsonl\"  # change if needed\n",
    "assert TESTSET_JSONL.exists(), f\"âŒ JSONL not found: {TESTSET_JSONL}\"\n",
    "\n",
    "rows = []\n",
    "with open(TESTSET_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        rows.append(json.loads(line))\n",
    "\n",
    "print(\"âœ… rows:\", len(rows))\n",
    "print(\"âœ… keys example:\", rows[0].keys())\n",
    "pd.DataFrame(rows[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347b7dac-4985-472c-a539-581bb0105eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd5607-f5ef-4c8c-a324-21e45c1831e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec1529b4",
   "metadata": {},
   "source": [
    "## 2) Define baseline & experiment configs\n",
    "\n",
    "- Keep **base_cfg** stable.\n",
    "- Only put **changed knobs** in `exp_cfg = replace(base_cfg, ...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69c6ca41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RAGConfig(normalize_model='solar-pro2', generation_model='gpt-4o-mini', temperature=0.1, normalize_temperature=0.0, embedding_backend='upstage', embedding_model='solar-embedding-1-large-passage', k_law=5, k_rule=5, k_case=3, search_multiplier=2, enable_bm25=True, sparse_mode='auto', sparse_k_law=None, sparse_k_rule=None, sparse_k_case=None, bm25_algorithm='okapi', bm25_k1=1.5, bm25_b=0.75, bm25_use_kiwi=True, bm25_max_doc_chars=2400, enable_bm25_title=True, bm25_title_field='title', bm25_title_max_chars=512, hybrid_sparse_title_ratio=0.35, hybrid_fusion='rrf', hybrid_dense_weight=0.6, hybrid_sparse_weight=0.4, rrf_k=60, enable_rerank=True, rerank_threshold=0.2, rerank_model='rerank-multilingual-v3.0', rerank_max_documents=20, rerank_doc_max_chars=2400, case_candidate_k=40, case_expand_top_n=None, case_context_top_k=50, dedupe_key_fields=('chunk_id', 'id')),\n",
       " RAGConfig(normalize_model='solar-pro2', generation_model='gpt-4o-mini', temperature=0.1, normalize_temperature=0.0, embedding_backend='upstage', embedding_model='solar-embedding-1-large-passage', k_law=5, k_rule=5, k_case=3, search_multiplier=2, enable_bm25=True, sparse_mode='auto', sparse_k_law=None, sparse_k_rule=None, sparse_k_case=None, bm25_algorithm='okapi', bm25_k1=1.5, bm25_b=0.75, bm25_use_kiwi=True, bm25_max_doc_chars=2400, enable_bm25_title=True, bm25_title_field='title', bm25_title_max_chars=512, hybrid_sparse_title_ratio=0.35, hybrid_fusion='rrf', hybrid_dense_weight=0.6, hybrid_sparse_weight=0.4, rrf_k=60, enable_rerank=True, rerank_threshold=0.25, rerank_model='rerank-multilingual-v3.0', rerank_max_documents=16, rerank_doc_max_chars=2400, case_candidate_k=40, case_expand_top_n=None, case_context_top_k=50, dedupe_key_fields=('chunk_id', 'id')))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import replace\n",
    "from rag_module import RAGConfig\n",
    "\n",
    "# =========================\n",
    "# Base config (edit as needed)\n",
    "# =========================\n",
    "base_cfg = RAGConfig(\n",
    "    # ---- LLM ----\n",
    "    normalize_model=\"solar-pro2\",\n",
    "    generation_model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    normalize_temperature=0.0,\n",
    "\n",
    "    # ---- Embedding ----\n",
    "    embedding_backend=\"upstage\",\n",
    "    embedding_model=\"solar-embedding-1-large-passage\",\n",
    "\n",
    "    # ---- Dense Retrieval ----\n",
    "    k_law=5,\n",
    "    k_rule=5,\n",
    "    k_case=3,\n",
    "    search_multiplier=2,\n",
    "\n",
    "    # ---- BM25 / Sparse ----\n",
    "    enable_bm25=True,\n",
    "\n",
    "    # ---- Rerank ----\n",
    "    enable_rerank=True,\n",
    "    rerank_threshold=0.2,\n",
    "    rerank_max_documents=20,\n",
    "\n",
    "    # ---- Output trimming ----\n",
    "    bm25_max_doc_chars=2400,\n",
    "    rerank_doc_max_chars=2400,\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Experiment config (only diffs here)\n",
    "# =========================\n",
    "exp_cfg = replace(\n",
    "    base_cfg,\n",
    "    rerank_threshold=0.25,    # 0.28 â†’ 0.25 (ì™„í™”)\n",
    "    rerank_max_documents=16,  # ìœ ì§€\n",
    ")\n",
    "base_cfg, exp_cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162fbbb5",
   "metadata": {},
   "source": [
    "## 3) Build pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba0f69f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 19:23:16,327 - rag_module - INFO - ğŸ”— Pinecone 3ì¤‘ ì¸ë±ìŠ¤ ì—°ê²° ì¤‘...\n",
      "2026-02-01 19:23:19,624 - rag_module - INFO - âœ… [Law / Rule / Case] 3ê°œ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ!\n",
      "2026-02-01 19:23:20,726 - rag_module - INFO - âœ… Kiwi í† í¬ë‚˜ì´ì € ì‚¬ìš© (BM25)\n",
      "2026-02-01 19:23:22,440 - rag_module - INFO - ğŸ”— Pinecone 3ì¤‘ ì¸ë±ìŠ¤ ì—°ê²° ì¤‘...\n",
      "2026-02-01 19:23:22,446 - rag_module - INFO - âœ… [Law / Rule / Case] 3ê°œ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ!\n",
      "2026-02-01 19:23:23,000 - rag_module - INFO - âœ… Kiwi í† í¬ë‚˜ì´ì € ì‚¬ìš© (BM25)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… pipelines ready\n"
     ]
    }
   ],
   "source": [
    "from rag_module import create_pipeline\n",
    "\n",
    "base_pipe = create_pipeline(config=base_cfg)\n",
    "exp_pipe  = create_pipeline(config=exp_cfg)\n",
    "\n",
    "print(\"âœ… pipelines ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6959059d-0ad1-4770-a338-6be9623871f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8fd6fa-516a-44f7-aa82-3095b2320555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e6dfb91",
   "metadata": {},
   "source": [
    "## 4) (Optional) Quick trace sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85371f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ Skip or customize depending on your pipeline API.\n"
     ]
    }
   ],
   "source": [
    "# If your rag_module exposes a trace / debug method, call it here.\n",
    "# Otherwise you can skip this cell.\n",
    "\n",
    "# Example (adjust to your actual API):\n",
    "# ans, trace = base_pipe.answer_with_trace(\"í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ...\")\n",
    "# display(trace)\n",
    "\n",
    "print(\"â„¹ï¸ Skip or customize depending on your pipeline API.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16739f05",
   "metadata": {},
   "source": [
    "## 5) Build RAGAS samples from your pipeline outputs\n",
    "\n",
    "This converts each testset row into the RAGAS format:\n",
    "- `question`\n",
    "- `answer`\n",
    "- `contexts` (list[str])\n",
    "- `ground_truth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a8d6173",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 19:23:25,146 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:23:25,161 - rag_module - INFO - ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: ì„ëŒ€ì¸(ì§‘ì£¼ì¸)ì´ ì•„ë¬´ ë§ë„ ì•ˆ í–ˆëŠ”ë° ê³„ì•½ê¸°ê°„ì´ ëë‚¬ì–´ìš”. ì œê°€ ê³„ì† ì‚´ë©´ì„œ ì°¨ì„(ì›”ì„¸)ë„ ëƒˆë‹¤ë©´ ë¬µì‹œì ê°±ì‹ (ìë™ì—°ì¥)ì´ ëœ ê±´ê°€ìš”? ë§Œì•½ ì„ëŒ€ì¸(ì§‘ì£¼ì¸)ì´ ëª…ë„(ë‚˜ê°€ë¼ê³  í•¨)í•˜ë¼ê³  í•˜ë©´ ì–¸ì œê¹Œì§€ ê³„ì•½í•´ì§€(í†µë³´)ë¥¼ í•´ì•¼ í•˜ë‚˜ìš”?\n",
      "2026-02-01 19:23:25,162 - rag_module - INFO - ğŸ” [Hybrid Retrieval] query='ì„ëŒ€ì¸(ì§‘ì£¼ì¸)ì´ ì•„ë¬´ ë§ë„ ì•ˆ í–ˆëŠ”ë° ê³„ì•½ê¸°ê°„ì´ ëë‚¬ì–´ìš”. ì œê°€ ê³„ì† ì‚´ë©´ì„œ ì°¨ì„(ì›”ì„¸)ë„ ëƒˆë‹¤ë©´ ë¬µì‹œì ê°±ì‹ (ìë™ì—°ì¥)ì´ ëœ ê±´ê°€ìš”? ë§Œì•½ ì„ëŒ€ì¸(ì§‘ì£¼ì¸)ì´ ëª…ë„(ë‚˜ê°€ë¼ê³  í•¨)í•˜ë¼ê³  í•˜ë©´ ì–¸ì œê¹Œì§€ ê³„ì•½í•´ì§€(í†µë³´)ë¥¼ í•´ì•¼ í•˜ë‚˜ìš”?'\n",
      "2026-02-01 19:23:26,017 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:23:28,118 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:23:30,522 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:23:35,934 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:23:35,941 - rag_module - INFO - ğŸ“Œ Rerank selected=1 (threshold=0.2)\n",
      "2026-02-01 19:23:35,942 - rag_module - INFO - ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n",
      "2026-02-01 19:23:42,759 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:23:43,921 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:23:43,924 - rag_module - INFO - ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: ì£¼ë¯¼ë“±ë¡ê³¼ í™•ì •ì¼ì(í™•ì •ì¼ì)ë¥¼ ë°›ì•„ë‘ì—ˆëŠ”ë°, ì„ëŒ€ì¸(ì„ëŒ€ì¸)ì´ ê°™ì€ ì„ì°¨ì£¼íƒ(ì„ì°¨ì£¼íƒ)ìœ¼ë¡œ ë‹¤ë¥¸ ì„ì°¨ì¸(ì„ì°¨ì¸)ê³¼ ì´ì¤‘ê³„ì•½ì„ í–ˆë‹¤ê³  í•©ë‹ˆë‹¤. ì´ ê²½ìš° ì €ëŠ” ìš°ì„ ë³€ì œê¶Œ(ìš°ì„ ë³€ì œê¶Œ)ì´ ìƒê¸´ ê±´ê°€ìš”? íš¨ë ¥ì€ ì–¸ì œë¶€í„° ë°œìƒí•˜ê³ , ê²½ë§¤ì ˆì°¨(ê²½ë§¤ì ˆì°¨)ì— ë„˜ì–´ê°€ë©´ ì–´ë–¤ ìˆœì„œë¡œ ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ(ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ)ì„ ëŒë ¤ë°›ê²Œ ë˜ë‚˜ìš”?\n",
      "2026-02-01 19:23:43,924 - rag_module - INFO - ğŸ” [Hybrid Retrieval] query='ì£¼ë¯¼ë“±ë¡ê³¼ í™•ì •ì¼ì(í™•ì •ì¼ì)ë¥¼ ë°›ì•„ë‘ì—ˆëŠ”ë°, ì„ëŒ€ì¸(ì„ëŒ€ì¸)ì´ ê°™ì€ ì„ì°¨ì£¼íƒ(ì„ì°¨ì£¼íƒ)ìœ¼ë¡œ ë‹¤ë¥¸ ì„ì°¨ì¸(ì„ì°¨ì¸)ê³¼ ì´ì¤‘ê³„ì•½ì„ í–ˆë‹¤ê³  í•©ë‹ˆë‹¤. ì´ ê²½ìš° ì €ëŠ” ìš°ì„ ë³€ì œê¶Œ(ìš°ì„ ë³€ì œê¶Œ)ì´ ìƒê¸´ ê±´ê°€ìš”? íš¨ë ¥ì€ ì–¸ì œë¶€í„° ë°œìƒí•˜ê³ , ê²½ë§¤ì ˆì°¨(ê²½ë§¤ì ˆì°¨)ì— ë„˜ì–´ê°€ë©´ ì–´ë–¤ ìˆœì„œë¡œ ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ(ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ)ì„ ëŒë ¤ë°›ê²Œ ë˜ë‚˜ìš”?'\n",
      "2026-02-01 19:23:44,554 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:23:45,467 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:23:46,316 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:23:48,244 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:23:48,248 - rag_module - INFO - ğŸ“Œ Rerank selected=6 (threshold=0.2)\n",
      "2026-02-01 19:23:48,249 - rag_module - INFO - ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n",
      "2026-02-01 19:23:56,900 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:23:58,348 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:23:58,350 - rag_module - INFO - ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œì— íŠ¹ì•½ì‚¬í•­ìœ¼ë¡œ ë°˜ë ¤ë™ë¬¼íŠ¹ì•½(ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­)ì´ ìˆëŠ” ê²½ìš°, ì´ë¥¼ ìœ„ë°˜í•˜ì—¬ ë°˜ë ¤ë™ë¬¼ì„ í‚¤ìš°ë‹¤ ì ë°œë˜ë©´ ê³„ì•½í•´ì§€(ê³„ì•½í•´ì§€)ë¥¼ ë‹¹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš° ë³´ì¦ê¸ˆë¯¸ë°˜í™˜(ë³´ì¦ê¸ˆë„ ëŒë ¤ë°›ì§€ ëª»í•  ìˆ˜ ìˆë‚˜ìš”)ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
      "\n",
      "ë³€ê²½ëœ ì§ˆë¬¸:  \n",
      "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ(ê³„ì•½ì„œ)ì— íŠ¹ì•½ì‚¬í•­(íŠ¹ì•½)ìœ¼ë¡œ ë°˜ë ¤ë™ë¬¼íŠ¹ì•½(ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€) ì¡°í•­ì´ ìˆëŠ”ë°, ì´ë¥¼ ì–´ê¸°ê³  ë°˜ë ¤ë™ë¬¼íŠ¹ì•½(ë°˜ë ¤ë™ë¬¼)ì„ í‚¤ìš°ë‹¤ ì ë°œë˜ë©´ ë°”ë¡œ ê³„ì•½í•´ì§€(ê³„ì•½í•´ì§€)ë¥¼ ë‹¹í•˜ë‚˜ìš”? ì´ ê²½ìš° ë³´ì¦ê¸ˆë¯¸ë°˜í™˜(ë³´ì¦ê¸ˆë„ ëŒë ¤ë°›ì§€ ëª»í•  ìˆ˜ ìˆë‚˜ìš”)?\n",
      "2026-02-01 19:23:58,351 - rag_module - INFO - ğŸ” [Hybrid Retrieval] query='ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œì— íŠ¹ì•½ì‚¬í•­ìœ¼ë¡œ ë°˜ë ¤ë™ë¬¼íŠ¹ì•½(ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­)ì´ ìˆëŠ” ê²½ìš°, ì´ë¥¼ ìœ„ë°˜í•˜ì—¬ ë°˜ë ¤ë™ë¬¼ì„ í‚¤ìš°ë‹¤ ì ë°œë˜ë©´ ê³„ì•½í•´ì§€(ê³„ì•½í•´ì§€)ë¥¼ ë‹¹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš° ë³´ì¦ê¸ˆë¯¸ë°˜í™˜(ë³´ì¦ê¸ˆë„ ëŒë ¤ë°›ì§€ ëª»í•  ìˆ˜ ìˆë‚˜ìš”)ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
      "\n",
      "ë³€ê²½ëœ ì§ˆë¬¸:  \n",
      "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ(ê³„ì•½ì„œ)ì— íŠ¹ì•½ì‚¬í•­(íŠ¹ì•½)ìœ¼ë¡œ ë°˜ë ¤ë™ë¬¼íŠ¹ì•½(ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€) ì¡°í•­ì´ ìˆëŠ”ë°, ì´ë¥¼ ì–´ê¸°ê³  ë°˜ë ¤ë™ë¬¼íŠ¹ì•½(ë°˜ë ¤ë™ë¬¼)ì„ í‚¤ìš°ë‹¤ ì ë°œë˜ë©´ ë°”ë¡œ ê³„ì•½í•´ì§€(ê³„ì•½í•´ì§€)ë¥¼ ë‹¹í•˜ë‚˜ìš”? ì´ ê²½ìš° ë³´ì¦ê¸ˆë¯¸ë°˜í™˜(ë³´ì¦ê¸ˆë„ ëŒë ¤ë°›ì§€ ëª»í•  ìˆ˜ ìˆë‚˜ìš”)?'\n",
      "2026-02-01 19:23:58,837 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:23:59,518 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:00,230 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:02,197 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:02,200 - rag_module - INFO - ğŸ“Œ Rerank selected=20 (threshold=0.2)\n",
      "2026-02-01 19:24:02,201 - rag_module - INFO - ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n",
      "2026-02-01 19:24:10,775 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:11,937 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:11,939 - rag_module - INFO - ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: ì„ëŒ€ì¸(ì§‘ì£¼ì¸)ì´ ì•„ë¬´ ë§ë„ ì•ˆ í–ˆëŠ”ë° ê³„ì•½ê¸°ê°„ì´ ëë‚¬ì–´ìš”. ì œê°€ ê³„ì† ì‚´ë©´ì„œ ì°¨ì„(ì›”ì„¸)ë„ ëƒˆë‹¤ë©´ ë¬µì‹œì ê°±ì‹ (ìë™ì—°ì¥)ì´ ëœ ê±´ê°€ìš”? ë§Œì•½ ì„ëŒ€ì¸(ì§‘ì£¼ì¸)ì´ ëª…ë„(ë‚˜ê°€ë¼ê³  í•¨)í•˜ë¼ê³  í•˜ë©´ ì–¸ì œê¹Œì§€ ê³„ì•½í•´ì§€(í†µë³´)ë¥¼ í•´ì•¼ í•˜ë‚˜ìš”?\n",
      "2026-02-01 19:24:11,940 - rag_module - INFO - ğŸ” [Hybrid Retrieval] query='ì„ëŒ€ì¸(ì§‘ì£¼ì¸)ì´ ì•„ë¬´ ë§ë„ ì•ˆ í–ˆëŠ”ë° ê³„ì•½ê¸°ê°„ì´ ëë‚¬ì–´ìš”. ì œê°€ ê³„ì† ì‚´ë©´ì„œ ì°¨ì„(ì›”ì„¸)ë„ ëƒˆë‹¤ë©´ ë¬µì‹œì ê°±ì‹ (ìë™ì—°ì¥)ì´ ëœ ê±´ê°€ìš”? ë§Œì•½ ì„ëŒ€ì¸(ì§‘ì£¼ì¸)ì´ ëª…ë„(ë‚˜ê°€ë¼ê³  í•¨)í•˜ë¼ê³  í•˜ë©´ ì–¸ì œê¹Œì§€ ê³„ì•½í•´ì§€(í†µë³´)ë¥¼ í•´ì•¼ í•˜ë‚˜ìš”?'\n",
      "2026-02-01 19:24:12,565 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:14,505 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:16,792 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:21,440 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:21,444 - rag_module - INFO - ğŸ“Œ Rerank selected=1 (threshold=0.25)\n",
      "2026-02-01 19:24:21,445 - rag_module - INFO - ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n",
      "2026-02-01 19:24:28,551 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:29,436 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:29,438 - rag_module - INFO - ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: ì£¼ë¯¼ë“±ë¡ê³¼ í™•ì •ì¼ì(í™•ì •ì¼ì)ë¥¼ ë°›ì•„ë‘ì—ˆëŠ”ë°, ì„ëŒ€ì¸(ì„ëŒ€ì¸)ì´ ê°™ì€ ì„ì°¨ì£¼íƒ(ì„ì°¨ì£¼íƒ)ìœ¼ë¡œ ë‹¤ë¥¸ ì„ì°¨ì¸(ì„ì°¨ì¸)ê³¼ ì´ì¤‘ê³„ì•½ì„ í–ˆë‹¤ê³  í•©ë‹ˆë‹¤. ì´ ê²½ìš° ì €ëŠ” ìš°ì„ ë³€ì œê¶Œ(ìš°ì„ ë³€ì œê¶Œ)ì´ ìƒê¸´ ê±´ê°€ìš”? íš¨ë ¥ì€ ì–¸ì œë¶€í„° ë°œìƒí•˜ê³ , ê²½ë§¤ì ˆì°¨(ê²½ë§¤ì ˆì°¨)ì— ë„˜ì–´ê°€ë©´ ì–´ë–¤ ìˆœì„œë¡œ ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ(ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ)ì„ ëŒë ¤ë°›ê²Œ ë˜ë‚˜ìš”?\n",
      "2026-02-01 19:24:29,439 - rag_module - INFO - ğŸ” [Hybrid Retrieval] query='ì£¼ë¯¼ë“±ë¡ê³¼ í™•ì •ì¼ì(í™•ì •ì¼ì)ë¥¼ ë°›ì•„ë‘ì—ˆëŠ”ë°, ì„ëŒ€ì¸(ì„ëŒ€ì¸)ì´ ê°™ì€ ì„ì°¨ì£¼íƒ(ì„ì°¨ì£¼íƒ)ìœ¼ë¡œ ë‹¤ë¥¸ ì„ì°¨ì¸(ì„ì°¨ì¸)ê³¼ ì´ì¤‘ê³„ì•½ì„ í–ˆë‹¤ê³  í•©ë‹ˆë‹¤. ì´ ê²½ìš° ì €ëŠ” ìš°ì„ ë³€ì œê¶Œ(ìš°ì„ ë³€ì œê¶Œ)ì´ ìƒê¸´ ê±´ê°€ìš”? íš¨ë ¥ì€ ì–¸ì œë¶€í„° ë°œìƒí•˜ê³ , ê²½ë§¤ì ˆì°¨(ê²½ë§¤ì ˆì°¨)ì— ë„˜ì–´ê°€ë©´ ì–´ë–¤ ìˆœì„œë¡œ ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ(ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ)ì„ ëŒë ¤ë°›ê²Œ ë˜ë‚˜ìš”?'\n",
      "2026-02-01 19:24:30,060 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:30,907 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:31,810 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:33,723 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:33,726 - rag_module - INFO - ğŸ“Œ Rerank selected=3 (threshold=0.25)\n",
      "2026-02-01 19:24:33,727 - rag_module - INFO - ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n",
      "2026-02-01 19:24:42,201 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:43,761 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:43,763 - rag_module - INFO - ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œì— íŠ¹ì•½ì‚¬í•­ìœ¼ë¡œ ë°˜ë ¤ë™ë¬¼íŠ¹ì•½(ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­)ì´ ìˆëŠ” ê²½ìš°, ì´ë¥¼ ìœ„ë°˜í•˜ì—¬ ë°˜ë ¤ë™ë¬¼ì„ í‚¤ìš°ë‹¤ ì ë°œë˜ë©´ ê³„ì•½í•´ì§€(ê³„ì•½í•´ì§€)ë¥¼ ë‹¹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš° ë³´ì¦ê¸ˆë¯¸ë°˜í™˜(ë³´ì¦ê¸ˆë„ ëŒë ¤ë°›ì§€ ëª»í•  ìˆ˜ ìˆë‚˜ìš”)ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
      "\n",
      "ë³€ê²½ëœ ì§ˆë¬¸:  \n",
      "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ(ê³„ì•½ì„œ)ì— íŠ¹ì•½ì‚¬í•­(íŠ¹ì•½)ìœ¼ë¡œ ë°˜ë ¤ë™ë¬¼íŠ¹ì•½(ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€) ì¡°í•­ì´ ìˆëŠ”ë°, ì´ë¥¼ ì–´ê¸°ê³  ë°˜ë ¤ë™ë¬¼íŠ¹ì•½(ë°˜ë ¤ë™ë¬¼)ì„ í‚¤ìš°ë‹¤ ì ë°œë˜ë©´ ë°”ë¡œ ê³„ì•½í•´ì§€(ê³„ì•½í•´ì§€)ë¥¼ ë‹¹í•˜ë‚˜ìš”? ì´ ê²½ìš° ë³´ì¦ê¸ˆë¯¸ë°˜í™˜(ë³´ì¦ê¸ˆë„ ëŒë ¤ë°›ì§€ ëª»í•  ìˆ˜ ìˆë‚˜ìš”)?\n",
      "2026-02-01 19:24:43,764 - rag_module - INFO - ğŸ” [Hybrid Retrieval] query='ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œì— íŠ¹ì•½ì‚¬í•­ìœ¼ë¡œ ë°˜ë ¤ë™ë¬¼íŠ¹ì•½(ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€ ì¡°í•­)ì´ ìˆëŠ” ê²½ìš°, ì´ë¥¼ ìœ„ë°˜í•˜ì—¬ ë°˜ë ¤ë™ë¬¼ì„ í‚¤ìš°ë‹¤ ì ë°œë˜ë©´ ê³„ì•½í•´ì§€(ê³„ì•½í•´ì§€)ë¥¼ ë‹¹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš° ë³´ì¦ê¸ˆë¯¸ë°˜í™˜(ë³´ì¦ê¸ˆë„ ëŒë ¤ë°›ì§€ ëª»í•  ìˆ˜ ìˆë‚˜ìš”)ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
      "\n",
      "ë³€ê²½ëœ ì§ˆë¬¸:  \n",
      "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ(ê³„ì•½ì„œ)ì— íŠ¹ì•½ì‚¬í•­(íŠ¹ì•½)ìœ¼ë¡œ ë°˜ë ¤ë™ë¬¼íŠ¹ì•½(ë°˜ë ¤ë™ë¬¼ ê¸ˆì§€) ì¡°í•­ì´ ìˆëŠ”ë°, ì´ë¥¼ ì–´ê¸°ê³  ë°˜ë ¤ë™ë¬¼íŠ¹ì•½(ë°˜ë ¤ë™ë¬¼)ì„ í‚¤ìš°ë‹¤ ì ë°œë˜ë©´ ë°”ë¡œ ê³„ì•½í•´ì§€(ê³„ì•½í•´ì§€)ë¥¼ ë‹¹í•˜ë‚˜ìš”? ì´ ê²½ìš° ë³´ì¦ê¸ˆë¯¸ë°˜í™˜(ë³´ì¦ê¸ˆë„ ëŒë ¤ë°›ì§€ ëª»í•  ìˆ˜ ìˆë‚˜ìš”)?'\n",
      "2026-02-01 19:24:44,276 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:44,975 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:45,843 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:47,623 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:47,626 - rag_module - INFO - ğŸ“Œ Rerank selected=16 (threshold=0.25)\n",
      "2026-02-01 19:24:47,627 - rag_module - INFO - ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n",
      "2026-02-01 19:24:54,798 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BASE_SAMPLES: 3\n",
      "âœ… EXP_SAMPLES : 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ì§‘ì£¼ì¸ì´ ì•„ë¬´ ë§ë„ ì•ˆ í–ˆëŠ”ë° ê³„ì•½ê¸°ê°„ì´ ëë‚¬ì–´ìš”. ì œê°€ ê³„ì† ì‚´ë©´ì„œ ì›”ì„¸ë„ ëƒˆë‹¤ë©´...</td>\n",
       "      <td>A. ë„¤, ì„ëŒ€ì¸ì´ ì•„ë¬´ëŸ° í†µì§€ë¥¼ í•˜ì§€ ì•Šì•˜ë‹¤ë©´ ë¬µì‹œì  ê°±ì‹ ì´ ëœ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤....</td>\n",
       "      <td>[page_content='â‘  ì„ëŒ€ì¸ì´ ì„ëŒ€ì°¨ê¸°ê°„ì´ ëë‚˜ê¸° 6ê°œì›” ì „ë¶€í„° 2ê°œì›” ì „...</td>\n",
       "      <td>ì„ëŒ€ì°¨ ê¸°ê°„ ë§Œë£Œ í›„ ì„ì°¨ì¸ì´ ê³„ì† ê±°ì£¼í•˜ë©° ì°¨ì„ì„ ì§€ê¸‰í•˜ê³ , ì„ëŒ€ì¸ì´ ìƒë‹¹ ê¸°ê°„ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  ì§‘ì£¼ì¸ì´ ì•„ë¬´ ë§ë„ ì•ˆ í–ˆëŠ”ë° ê³„ì•½ê¸°ê°„ì´ ëë‚¬ì–´ìš”. ì œê°€ ê³„ì† ì‚´ë©´ì„œ ì›”ì„¸ë„ ëƒˆë‹¤ë©´...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  A. ë„¤, ì„ëŒ€ì¸ì´ ì•„ë¬´ëŸ° í†µì§€ë¥¼ í•˜ì§€ ì•Šì•˜ë‹¤ë©´ ë¬µì‹œì  ê°±ì‹ ì´ ëœ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤....   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [page_content='â‘  ì„ëŒ€ì¸ì´ ì„ëŒ€ì°¨ê¸°ê°„ì´ ëë‚˜ê¸° 6ê°œì›” ì „ë¶€í„° 2ê°œì›” ì „...   \n",
       "\n",
       "                                        ground_truth  \n",
       "0  ì„ëŒ€ì°¨ ê¸°ê°„ ë§Œë£Œ í›„ ì„ì°¨ì¸ì´ ê³„ì† ê±°ì£¼í•˜ë©° ì°¨ì„ì„ ì§€ê¸‰í•˜ê³ , ì„ëŒ€ì¸ì´ ìƒë‹¹ ê¸°ê°„ ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def shrink_contexts(ctxs, max_chars=2400, max_contexts=30):\n",
    "    out = []\n",
    "    for c in (ctxs or []):\n",
    "        if c is None:\n",
    "            continue\n",
    "        s = str(c).strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        out.append(s[:max_chars])\n",
    "        if len(out) >= max_contexts:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "def row_get_ground_truth(r: dict):\n",
    "    return r.get(\"ground_truth\") or r.get(\"reference\") or r.get(\"gt\") or r.get(\"answer\")\n",
    "\n",
    "def run_pipe_to_samples(pipe, rows, max_chars=2400, max_contexts=30, limit=None):\n",
    "    samples = []\n",
    "    n = len(rows) if limit is None else min(limit, len(rows))\n",
    "\n",
    "    for i in range(n):\n",
    "        r = rows[i]\n",
    "        q = r.get(\"question\") or r.get(\"query\")\n",
    "        if not q:\n",
    "            continue\n",
    "\n",
    "        # âœ… ë„¤ íŒŒì´í”„ë¼ì¸ì€ ì´ê±¸ë¡œ í˜¸ì¶œí•´ì•¼ í•¨\n",
    "        out = pipe.answer_with_trace(q)\n",
    "\n",
    "        # outì´ dictì¼ ìˆ˜ë„ ìˆê³ , (answer, ctxs, trace) íŠœí”Œì¼ ìˆ˜ë„ ìˆì–´ì„œ ì•ˆì „ ì²˜ë¦¬\n",
    "        ans, ctxs, trace = \"\", [], None\n",
    "\n",
    "        if isinstance(out, dict):\n",
    "            ans = out.get(\"answer\") or out.get(\"result\") or out.get(\"output\") or out.get(\"text\") or \"\"\n",
    "            ctxs = out.get(\"contexts\") or out.get(\"context\") or out.get(\"docs\") or []\n",
    "            trace = out.get(\"trace\") or out.get(\"debug\") or out.get(\"meta\")\n",
    "        elif isinstance(out, tuple):\n",
    "            # í”í•œ íŒ¨í„´ë“¤ ëŒ€ì‘\n",
    "            if len(out) == 3:\n",
    "                ans, ctxs, trace = out\n",
    "            elif len(out) == 2:\n",
    "                ans, ctxs = out\n",
    "            elif len(out) == 1:\n",
    "                ans = out[0]\n",
    "        else:\n",
    "            ans = str(out)\n",
    "\n",
    "        samples.append({\n",
    "            \"question\": q,\n",
    "            \"answer\": ans or \"\",\n",
    "            \"contexts\": shrink_contexts(ctxs, max_chars=max_chars, max_contexts=max_contexts),\n",
    "            \"ground_truth\": row_get_ground_truth(r) or \"\",\n",
    "            \"_trace\": trace,  # âœ… traceë„ ê°™ì´ ë³´ê´€(ì›í•˜ë©´ ì €ì¥ ê°€ëŠ¥)\n",
    "        })\n",
    "\n",
    "    return samples\n",
    "\n",
    "# âœ… ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸: 1ë¬¸ì œë§Œ\n",
    "BASE_SAMPLES = run_pipe_to_samples(base_pipe, rows, limit=None)\n",
    "EXP_SAMPLES = run_pipe_to_samples(exp_pipe, rows, limit=None)\n",
    "\n",
    "print(\"âœ… BASE_SAMPLES:\", len(BASE_SAMPLES))\n",
    "print(\"âœ… EXP_SAMPLES :\", len(EXP_SAMPLES))\n",
    "pd.DataFrame([{k:v for k,v in BASE_SAMPLES[0].items() if k != \"_trace\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286a3c26",
   "metadata": {},
   "source": [
    "## 6) RAGAS evaluation (prepared cell)\n",
    "\n",
    "- Creates per-sample detail dataframe (when supported by your RAGAS version)\n",
    "- Creates summary dataframe (mean over samples)\n",
    "- Keeps timing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10a15582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… METRICS: ['ContextPrecision', 'ContextRecall', 'Faithfulness', 'AnswerRelevancy']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# RAGAS compare + clean saving (ragas==0.3.2 compatible)\n",
    "# FIXES:\n",
    "#  1) detail.csvì—ì„œ _trace ì œê±° (traceëŠ” trace.jsonlë¡œë§Œ)\n",
    "#  2) samplesì— run_tagë¥¼ ë¯¸ë¦¬ ì£¼ì…í•´ì„œ trace.jsonlì— íƒœê·¸ê°€ ë‚¨ë„ë¡\n",
    "#  3) ground_truths=[...] ì•ˆì „ì¥ì¹˜ ì¶”ê°€ (ë²„ì „/í™˜ê²½ í˜¸í™˜ì„±â†‘)\n",
    "#  4) samples/detail ì»¬ëŸ¼ ì¶©ëŒ ë°©ì§€(ê°€ëŠ¥í•œ í•œ ì•ˆì „í•˜ê²Œ merge)\n",
    "# ============================================================\n",
    "\n",
    "import time, json, re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.llms import llm_factory\n",
    "\n",
    "# ----------------------------\n",
    "# LLM + METRICS (ragas 0.3.2)\n",
    "# ----------------------------\n",
    "llm = llm_factory(\"gpt-4o-mini\")  # âœ… 0.3.2: client ì¸ì ì—†ìŒ\n",
    "\n",
    "def build_metrics_032():\n",
    "    from ragas.metrics import context_precision, context_recall, faithfulness\n",
    "    metrics = [context_precision, context_recall, faithfulness]\n",
    "    # answer_relevancyëŠ” í™˜ê²½ì— ë”°ë¼ ì—†ì„ ìˆ˜ ìˆì–´ optional\n",
    "    try:\n",
    "        from ragas.metrics import answer_relevancy\n",
    "        metrics.append(answer_relevancy)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    return metrics\n",
    "\n",
    "METRICS = build_metrics_032()\n",
    "print(\"âœ… METRICS:\", [getattr(m, \"__name__\", m.__class__.__name__) for m in METRICS])\n",
    "\n",
    "# ----------------------------\n",
    "# utils\n",
    "# ----------------------------\n",
    "def _json_safe(obj):\n",
    "    \"\"\"Make config/meta safe to dump to json.\"\"\"\n",
    "    try:\n",
    "        json.dumps(obj, ensure_ascii=False)\n",
    "        return obj\n",
    "    except TypeError:\n",
    "        if hasattr(obj, \"model_dump\"):\n",
    "            return obj.model_dump()\n",
    "        if hasattr(obj, \"dict\"):\n",
    "            return obj.dict()\n",
    "        if hasattr(obj, \"__dict__\"):\n",
    "            return obj.__dict__\n",
    "        return str(obj)\n",
    "\n",
    "def _write_json(path: Path, data):\n",
    "    path.write_text(json.dumps(_json_safe(data), ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def _write_jsonl(path: Path, rows):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(_json_safe(r), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def _next_run_dir(project_root: Path, prefix: str):\n",
    "    runs_root = Path(project_root) / \"results\" / \"ragas_runs\"\n",
    "    runs_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pat = re.compile(rf\"^{re.escape(prefix)}_(\\d{{4}})_\")\n",
    "    nums = []\n",
    "    for p in runs_root.iterdir():\n",
    "        if p.is_dir():\n",
    "            m = pat.match(p.name)\n",
    "            if m:\n",
    "                nums.append(int(m.group(1)))\n",
    "    next_idx = (max(nums) + 1) if nums else 1\n",
    "\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = runs_root / f\"{prefix}_{next_idx:04d}_{ts}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=False)\n",
    "    return run_dir, next_idx, ts\n",
    "\n",
    "def _strip_trace(samples):\n",
    "    \"\"\"detail.csvì—ëŠ” _traceë¥¼ ë„£ì§€ ì•Šê¸°(íŒŒì¼ í­ë°œ ë°©ì§€).\"\"\"\n",
    "    out = []\n",
    "    for s in samples:\n",
    "        if isinstance(s, dict):\n",
    "            out.append({k: v for k, v in s.items() if k != \"_trace\"})\n",
    "        else:\n",
    "            out.append(s)\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# core eval\n",
    "# ----------------------------\n",
    "def eval_ragas_with_details(samples, run_tag: str):\n",
    "    # âœ… samplesì— run_tagë¥¼ ë¯¸ë¦¬ ì£¼ì… (trace.jsonlì—ì„œ íƒœê·¸ ìœ ì§€)\n",
    "    for s in samples:\n",
    "        if isinstance(s, dict):\n",
    "            s[\"run_tag\"] = run_tag\n",
    "            # âœ… ì•ˆì „ì¥ì¹˜: ground_truthsë„ í•¨ê»˜\n",
    "            if \"ground_truths\" not in s:\n",
    "                gt = s.get(\"ground_truth\") or \"\"\n",
    "                s[\"ground_truths\"] = [gt] if isinstance(gt, str) else (gt or [])\n",
    "\n",
    "    ds = Dataset.from_list(samples)\n",
    "\n",
    "    t0 = time.time()\n",
    "    res = evaluate(dataset=ds, metrics=METRICS, llm=llm)  # âœ… 0.3.2 ì•ˆì „ íŒ¨í„´\n",
    "    eval_sec = time.time() - t0\n",
    "\n",
    "    t1 = time.time()\n",
    "    detail_df = res.to_pandas() if hasattr(res, \"to_pandas\") else pd.DataFrame()\n",
    "    to_pandas_sec = time.time() - t1\n",
    "\n",
    "    # âœ… detailì—ëŠ” _trace ì œì™¸\n",
    "    samples_df = pd.DataFrame(_strip_trace(samples))\n",
    "\n",
    "    # merge per-sample metrics back onto samples (ê¸¸ì´ ë™ì¼í•  ë•Œë§Œ)\n",
    "    if len(detail_df) == len(samples_df) and len(detail_df) > 0:\n",
    "        # ì¶©ëŒ ì»¬ëŸ¼ ë°©ì§€: detail_dfì˜ ì»¬ëŸ¼ì´ samples_dfì— ì´ë¯¸ ìˆìœ¼ë©´ prefix\n",
    "        overlap = set(samples_df.columns) & set(detail_df.columns)\n",
    "        if overlap:\n",
    "            detail_df = detail_df.rename(columns={c: f\"metric__{c}\" for c in overlap})\n",
    "\n",
    "        out_detail = pd.concat(\n",
    "            [samples_df.reset_index(drop=True), detail_df.reset_index(drop=True)],\n",
    "            axis=1\n",
    "        )\n",
    "    else:\n",
    "        out_detail = samples_df.copy()\n",
    "\n",
    "    out_detail[\"eval_seconds\"] = round(eval_sec, 3)\n",
    "    out_detail[\"to_pandas_seconds\"] = round(to_pandas_sec, 3)\n",
    "\n",
    "    # summary (mean of numeric metric columns if available)\n",
    "    summary = {}\n",
    "    if len(detail_df) > 0:\n",
    "        summary = detail_df.mean(numeric_only=True).to_dict()\n",
    "    elif isinstance(res, dict):\n",
    "        summary = {k: float(v) for k, v in res.items() if isinstance(v, (int, float))}\n",
    "\n",
    "    summary[\"run_tag\"] = run_tag\n",
    "    summary[\"eval_seconds\"] = round(eval_sec, 3)\n",
    "    summary[\"to_pandas_seconds\"] = round(to_pandas_sec, 3)\n",
    "\n",
    "    return res, out_detail, pd.DataFrame([summary])\n",
    "\n",
    "# ----------------------------\n",
    "# compare + save (clean)\n",
    "# ----------------------------\n",
    "def run_compare_and_save(\n",
    "    base_samples,\n",
    "    exp_samples,\n",
    "    project_root: Path,\n",
    "    prefix=\"ragas_compare\",\n",
    "    base_cfg=None,\n",
    "    exp_cfg=None,\n",
    "):\n",
    "    # --- sanity ---\n",
    "    print(f\"âœ… base_samples: {len(base_samples)} | exp_samples: {len(exp_samples)}\")\n",
    "\n",
    "    base_res, base_detail_df, base_summary_df = eval_ragas_with_details(base_samples, \"baseline\")\n",
    "    exp_res,  exp_detail_df,  exp_summary_df  = eval_ragas_with_details(exp_samples,  \"experiment\")\n",
    "\n",
    "    summary_df = pd.concat([base_summary_df, exp_summary_df], ignore_index=True)\n",
    "    detail_df  = pd.concat([base_detail_df,  exp_detail_df],  ignore_index=True)\n",
    "\n",
    "    run_dir, run_id, ts = _next_run_dir(project_root, prefix)\n",
    "\n",
    "    out_summary = run_dir / \"summary.csv\"\n",
    "    out_detail  = run_dir / \"detail.csv\"\n",
    "    out_meta    = run_dir / \"meta.json\"\n",
    "    out_config  = run_dir / \"config.json\"\n",
    "    out_base_in = run_dir / \"samples_base.jsonl\"\n",
    "    out_exp_in  = run_dir / \"samples_exp.jsonl\"\n",
    "    out_trace   = run_dir / \"trace.jsonl\"\n",
    "\n",
    "    summary_df.to_csv(out_summary, index=False, encoding=\"utf-8-sig\")\n",
    "    detail_df.to_csv(out_detail, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # config snapshot (best-effort)\n",
    "    cfg_payload = {\n",
    "        \"base_cfg\": _json_safe(base_cfg) if base_cfg is not None else None,\n",
    "        \"exp_cfg\":  _json_safe(exp_cfg)  if exp_cfg  is not None else None,\n",
    "        \"llm\": {\"model\": \"gpt-4o-mini\"},\n",
    "        \"metrics\": [getattr(m, \"__name__\", m.__class__.__name__) for m in METRICS],\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "    }\n",
    "    _write_json(out_config, cfg_payload)\n",
    "\n",
    "    # input snapshots (ì›ë³¸ ìœ ì§€: _trace í¬í•¨)\n",
    "    _write_jsonl(out_base_in, base_samples)\n",
    "    _write_jsonl(out_exp_in,  exp_samples)\n",
    "\n",
    "    # trace snapshot (best-effort) - samplesì˜ _traceë§Œ ëª¨ì•„ì„œ ì €ì¥\n",
    "    trace_rows = []\n",
    "    for s in list(base_samples) + list(exp_samples):\n",
    "        if isinstance(s, dict) and (\"_trace\" in s) and (s.get(\"_trace\") is not None):\n",
    "            trace_rows.append({\n",
    "                \"run_tag\": s.get(\"run_tag\"),\n",
    "                \"question\": s.get(\"question\"),\n",
    "                \"_trace\": s.get(\"_trace\"),\n",
    "            })\n",
    "\n",
    "    # traceê°€ ì•„ì˜ˆ ì—†ìœ¼ë©´ ìµœì†Œ ì •ë³´ë¼ë„ ë‚¨ê¹€\n",
    "    if not trace_rows:\n",
    "        cols = [c for c in [\"run_tag\", \"question\", \"eval_seconds\"] if c in detail_df.columns]\n",
    "        trace_rows = detail_df[cols].to_dict(orient=\"records\") if cols else []\n",
    "\n",
    "    _write_jsonl(out_trace, trace_rows)\n",
    "\n",
    "    meta = {\n",
    "        \"created_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"ragas_version\": \"0.3.2\",\n",
    "        \"run_id\": run_id,\n",
    "        \"timestamp\": ts,\n",
    "        \"run_dir\": str(run_dir),\n",
    "        \"prefix\": prefix,\n",
    "        \"n_base_samples\": len(base_samples),\n",
    "        \"n_exp_samples\": len(exp_samples),\n",
    "        \"saved\": {\n",
    "            \"summary\": str(out_summary),\n",
    "            \"detail\": str(out_detail),\n",
    "            \"meta\": str(out_meta),\n",
    "            \"config\": str(out_config),\n",
    "            \"samples_base\": str(out_base_in),\n",
    "            \"samples_exp\": str(out_exp_in),\n",
    "            \"trace\": str(out_trace),\n",
    "        },\n",
    "    }\n",
    "    _write_json(out_meta, meta)\n",
    "\n",
    "    print(f\"âœ… Saved to: {run_dir}\")\n",
    "    print(f\"   - summary: {out_summary.name}\")\n",
    "    print(f\"   - detail : {out_detail.name}\")\n",
    "    print(f\"   - meta   : {out_meta.name}\")\n",
    "    print(f\"   - config : {out_config.name}\")\n",
    "    print(f\"   - inputs : {out_base_in.name}, {out_exp_in.name}\")\n",
    "    print(f\"   - trace  : {out_trace.name}\")\n",
    "\n",
    "    return {\n",
    "        \"base_res\": base_res,\n",
    "        \"exp_res\": exp_res,\n",
    "        \"summary_df\": summary_df,\n",
    "        \"detail_df\": detail_df,\n",
    "        \"run_dir\": run_dir,\n",
    "        \"out_summary\": out_summary,\n",
    "        \"out_detail\": out_detail,\n",
    "        \"out_meta\": out_meta,\n",
    "        \"out_config\": out_config,\n",
    "        \"out_samples_base\": out_base_in,\n",
    "        \"out_samples_exp\": out_exp_in,\n",
    "        \"out_trace\": out_trace,\n",
    "        \"run_id\": run_id,\n",
    "    }\n",
    "\n",
    "# ============================\n",
    "# USAGE (ì˜ˆì‹œ)\n",
    "# ============================\n",
    "# result = run_compare_and_save(\n",
    "#     base_samples=BASE_SAMPLES,\n",
    "#     exp_samples=EXP_SAMPLES,\n",
    "#     project_root=PROJECT_ROOT,\n",
    "#     prefix=\"ragas_compare\",\n",
    "#     base_cfg=base_cfg,\n",
    "#     exp_cfg=exp_cfg,\n",
    "# )\n",
    "# display(result[\"summary_df\"])\n",
    "# display(result[\"detail_df\"].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee50b62",
   "metadata": {},
   "source": [
    "## 7) Run + compare + save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29525fd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… METRICS: ['ContextPrecision', 'ContextRecall', 'Faithfulness', 'AnswerRelevancy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 19:24:57,583 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:57,586 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:57,602 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:57,604 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:57,610 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:58,090 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:58,398 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:58,410 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:58,413 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:58,414 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:58,416 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:58,417 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:58,696 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"s/it]\n",
      "2026-02-01 19:24:59,005 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:59,275 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:59,500 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:59,504 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:24:59,756 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:01,746 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:02,009 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:03,311 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:03,699 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:04,023 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:04,029 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:04,068 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:04,626 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:05,482 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:05,618 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:05,798 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:07,479 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:07,708 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:09,370 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:09,986 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:11,139 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:11,710 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:13,422 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:16,031 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:18,337 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:19,486 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:24,253 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:26,358 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:29<00:00,  2.48s/it]\n",
      "2026-02-01 19:25:29,861 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:29,879 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:29,900 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:29,919 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:30,140 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:30,619 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:30,622 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:30,624 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:30,625 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:30,627 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:30,628 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:30,629 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:30,926 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"s/it]\n",
      "2026-02-01 19:25:31,330 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:31,596 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:31,801 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:31,805 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:31,807 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:33,479 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:33,868 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:35,146 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:35,958 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:36,810 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:36,893 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:37,074 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:37,200 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:37,709 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:37,835 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:38,025 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:40,254 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:42,238 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:44,325 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:46,307 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:48,313 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:50,029 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:55,065 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:25:55,640 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 19:26:00,441 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:31<00:00,  2.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… run_dir: C:\\Users\\darks\\Desktop\\ìë£Œë“¤\\AIìë£Œ\\ai\\source\\chatbot_app\\results\\ragas_runs\\ragas_compare_0007_20260201_192601\n",
      "âœ… saved: C:\\Users\\darks\\Desktop\\ìë£Œë“¤\\AIìë£Œ\\ai\\source\\chatbot_app\\results\\ragas_runs\\ragas_compare_0007_20260201_192601\\summary.csv C:\\Users\\darks\\Desktop\\ìë£Œë“¤\\AIìë£Œ\\ai\\source\\chatbot_app\\results\\ragas_runs\\ragas_compare_0007_20260201_192601\\detail.csv C:\\Users\\darks\\Desktop\\ìë£Œë“¤\\AIìë£Œ\\ai\\source\\chatbot_app\\results\\ragas_runs\\ragas_compare_0007_20260201_192601\\meta.json\n",
      "âœ… delta saved: C:\\Users\\darks\\Desktop\\ìë£Œë“¤\\AIìë£Œ\\ai\\source\\chatbot_app\\results\\ragas_runs\\ragas_compare_0007_20260201_192601\\delta_summary.csv C:\\Users\\darks\\Desktop\\ìë£Œë“¤\\AIìë£Œ\\ai\\source\\chatbot_app\\results\\ragas_runs\\ragas_compare_0007_20260201_192601\\delta_detail.csv\n",
      "âœ… top changes metric: answer_relevancy_delta\n",
      "âœ… top regressions: C:\\Users\\darks\\Desktop\\ìë£Œë“¤\\AIìë£Œ\\ai\\source\\chatbot_app\\results\\ragas_runs\\ragas_compare_0007_20260201_192601\\top_regressions.csv\n",
      "âœ… top improvements: C:\\Users\\darks\\Desktop\\ìë£Œë“¤\\AIìë£Œ\\ai\\source\\chatbot_app\\results\\ragas_runs\\ragas_compare_0007_20260201_192601\\top_improvements.csv\n",
      "âœ… extra saved config : C:\\Users\\darks\\Desktop\\ìë£Œë“¤\\AIìë£Œ\\ai\\source\\chatbot_app\\results\\ragas_runs\\ragas_compare_0007_20260201_192601\\config.json\n",
      "âœ… extra saved samples: C:\\Users\\darks\\Desktop\\ìë£Œë“¤\\AIìë£Œ\\ai\\source\\chatbot_app\\results\\ragas_runs\\ragas_compare_0007_20260201_192601\\samples_base.jsonl and C:\\Users\\darks\\Desktop\\ìë£Œë“¤\\AIìë£Œ\\ai\\source\\chatbot_app\\results\\ragas_runs\\ragas_compare_0007_20260201_192601\\samples_exp.jsonl\n",
      "âœ… extra saved trace  : C:\\Users\\darks\\Desktop\\ìë£Œë“¤\\AIìë£Œ\\ai\\source\\chatbot_app\\results\\ragas_runs\\ragas_compare_0007_20260201_192601\\trace.jsonl\n",
      "âœ… trace columns used : ['question', 'answer', 'ground_truth', 'reference', 'contexts', '_trace', 'run_tag']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# One-cell: RAGAS compare + save (+ delta outputs + dataset fingerprint)\n",
    "# Adds:\n",
    "#  - delta_summary.csv\n",
    "#  - delta_detail.csv\n",
    "#  - top_regressions.csv\n",
    "#  - top_improvements.csv\n",
    "#  - meta.json: testset fingerprint (path/lines/sha1) if TESTSET_JSONL exists\n",
    "# ============================================================\n",
    "\n",
    "import time, re, json, hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "from openai import OpenAI\n",
    "from ragas import evaluate\n",
    "from ragas.llms import llm_factory\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# LLM (non-deprecated)\n",
    "# ----------------------------\n",
    "client = OpenAI()\n",
    "llm = llm_factory(\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# METRICS (version-tolerant)\n",
    "# ----------------------------\n",
    "def build_metrics():\n",
    "    from ragas.metrics import context_precision, context_recall, faithfulness\n",
    "    metrics = [context_precision, context_recall, faithfulness]\n",
    "    try:\n",
    "        from ragas.metrics import answer_relevancy\n",
    "        metrics.append(answer_relevancy)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    return metrics\n",
    "\n",
    "METRICS = build_metrics()\n",
    "print(\"âœ… METRICS:\", [getattr(m, \"__name__\", m.__class__.__name__) for m in METRICS])\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# JSON helpers (safe)\n",
    "# ----------------------------\n",
    "def _json_safe(obj):\n",
    "    try:\n",
    "        json.dumps(obj, ensure_ascii=False)\n",
    "        return obj\n",
    "    except TypeError:\n",
    "        if hasattr(obj, \"model_dump\"):\n",
    "            return obj.model_dump()\n",
    "        if hasattr(obj, \"dict\"):\n",
    "            return obj.dict()\n",
    "        if hasattr(obj, \"__dict__\"):\n",
    "            return obj.__dict__\n",
    "        return str(obj)\n",
    "\n",
    "def _write_json(path: Path, data):\n",
    "    path.write_text(json.dumps(_json_safe(data), ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def _write_jsonl(path: Path, rows):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(_json_safe(r), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def _df_to_jsonl_rows(df, prefer_cols):\n",
    "    cols = [c for c in prefer_cols if c in df.columns]\n",
    "    if cols:\n",
    "        df = df[cols].copy()\n",
    "    return df.to_dict(orient=\"records\"), cols\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# dataset fingerprint helpers\n",
    "# ----------------------------\n",
    "def _file_sha1(path: Path, chunk_size=1024 * 1024) -> str:\n",
    "    h = hashlib.sha1()\n",
    "    with path.open(\"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def _count_jsonl_lines(path: Path) -> int:\n",
    "    n = 0\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                n += 1\n",
    "    return n\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# core: eval + detail/summary frames\n",
    "# ----------------------------\n",
    "def eval_ragas_with_details(samples, run_tag: str):\n",
    "    ds = Dataset.from_list(samples)\n",
    "\n",
    "    t0 = time.time()\n",
    "    res = evaluate(dataset=ds, metrics=METRICS, llm=llm)  # âœ… llmì€ ì—¬ê¸°ë¡œ\n",
    "    eval_sec = time.time() - t0\n",
    "\n",
    "    t1 = time.time()\n",
    "    detail_df = res.to_pandas() if hasattr(res, \"to_pandas\") else pd.DataFrame()\n",
    "    to_pandas_sec = time.time() - t1\n",
    "\n",
    "    samples_df = pd.DataFrame(samples)\n",
    "\n",
    "    # merge\n",
    "    if len(detail_df) == len(samples_df) and len(detail_df) > 0:\n",
    "        out_detail = pd.concat(\n",
    "            [samples_df.reset_index(drop=True), detail_df.reset_index(drop=True)],\n",
    "            axis=1\n",
    "        )\n",
    "    else:\n",
    "        out_detail = samples_df.copy()\n",
    "\n",
    "    out_detail[\"run_tag\"] = run_tag\n",
    "    out_detail[\"eval_seconds\"] = round(eval_sec, 3)\n",
    "    out_detail[\"to_pandas_seconds\"] = round(to_pandas_sec, 3)\n",
    "\n",
    "    # summary\n",
    "    summary = {}\n",
    "    if len(detail_df) > 0:\n",
    "        summary = detail_df.mean(numeric_only=True).to_dict()\n",
    "    elif isinstance(res, dict):\n",
    "        summary = {k: float(v) for k, v in res.items() if isinstance(v, (int, float))}\n",
    "\n",
    "    summary[\"run_tag\"] = run_tag\n",
    "    summary[\"eval_seconds\"] = round(eval_sec, 3)\n",
    "    summary[\"to_pandas_seconds\"] = round(to_pandas_sec, 3)\n",
    "\n",
    "    return res, out_detail, pd.DataFrame([summary])\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# run dir allocator\n",
    "# ----------------------------\n",
    "def _next_run_dir(project_root: Path, prefix: str):\n",
    "    runs_root = Path(project_root) / \"results\" / \"ragas_runs\"\n",
    "    runs_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pat = re.compile(rf\"^{re.escape(prefix)}_(\\d{{4}})_\")\n",
    "    nums = []\n",
    "    for p in runs_root.iterdir():\n",
    "        if p.is_dir():\n",
    "            m = pat.match(p.name)\n",
    "            if m:\n",
    "                nums.append(int(m.group(1)))\n",
    "    next_id = (max(nums) + 1) if nums else 1\n",
    "\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = runs_root / f\"{prefix}_{next_id:04d}_{ts}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=False)\n",
    "    return run_dir, next_id, ts\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# delta builders\n",
    "# ----------------------------\n",
    "def _pick_question_col(df: pd.DataFrame) -> str:\n",
    "    for c in [\"question\", \"normalized_question\", \"normalized_query\", \"query\"]:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return \"\"\n",
    "\n",
    "def _ensure_question_id(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # prefer an existing stable id\n",
    "    for c in [\"question_id\", \"id\", \"sample_id\"]:\n",
    "        if c in df.columns:\n",
    "            df = df.copy()\n",
    "            df[\"__qid__\"] = df[c].astype(str)\n",
    "            return df\n",
    "    # fallback: hash question text\n",
    "    qcol = _pick_question_col(df)\n",
    "    df = df.copy()\n",
    "    if qcol:\n",
    "        def _h(x: str) -> str:\n",
    "            s = (x or \"\").strip().encode(\"utf-8\")\n",
    "            return hashlib.sha1(s).hexdigest()[:12]\n",
    "        df[\"__qid__\"] = df[qcol].astype(str).map(_h)\n",
    "    else:\n",
    "        df[\"__qid__\"] = [f\"row{i:04d}\" for i in range(len(df))]\n",
    "    return df\n",
    "\n",
    "def _metric_cols(df: pd.DataFrame) -> list:\n",
    "    # heuristic: numeric columns from ragas result + common metric names\n",
    "    prefer = [\n",
    "        \"context_precision\", \"context_recall\", \"faithfulness\", \"answer_relevancy\",\n",
    "        \"ContextPrecision\", \"ContextRecall\", \"Faithfulness\", \"AnswerRelevancy\",\n",
    "    ]\n",
    "    cols = [c for c in prefer if c in df.columns]\n",
    "    if cols:\n",
    "        return cols\n",
    "\n",
    "    # fallback: any numeric columns that are not obvious non-metrics\n",
    "    exclude = set([\"eval_seconds\", \"to_pandas_seconds\"])\n",
    "    num_cols = []\n",
    "    for c in df.columns:\n",
    "        if c in exclude:\n",
    "            continue\n",
    "        if pd.api.types.is_numeric_dtype(df[c]):\n",
    "            num_cols.append(c)\n",
    "    return num_cols\n",
    "\n",
    "\n",
    "def _make_delta_summary(summary_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # summary_df has rows: baseline/experiment\n",
    "    metric_cols = [c for c in summary_df.columns if c not in [\"run_tag\"]]\n",
    "    base = summary_df[summary_df[\"run_tag\"] == \"baseline\"].iloc[0].to_dict()\n",
    "    exp  = summary_df[summary_df[\"run_tag\"] == \"experiment\"].iloc[0].to_dict()\n",
    "\n",
    "    rows = []\n",
    "    for c in metric_cols:\n",
    "        if c == \"run_tag\":\n",
    "            continue\n",
    "        b = base.get(c)\n",
    "        e = exp.get(c)\n",
    "        if isinstance(b, (int, float)) and isinstance(e, (int, float)):\n",
    "            rows.append({\"metric\": c, \"baseline\": float(b), \"experiment\": float(e), \"delta\": float(e - b)})\n",
    "        else:\n",
    "            # keep non-numeric too\n",
    "            rows.append({\"metric\": c, \"baseline\": b, \"experiment\": e, \"delta\": None})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def _make_delta_detail(detail_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = _ensure_question_id(detail_df)\n",
    "\n",
    "    base = df[df[\"run_tag\"] == \"baseline\"].copy()\n",
    "    exp  = df[df[\"run_tag\"] == \"experiment\"].copy()\n",
    "\n",
    "    # --- choose question column ---\n",
    "    qcol = _pick_question_col(df)\n",
    "\n",
    "    # --- metric columns ---\n",
    "    mcols = _metric_cols(df)\n",
    "\n",
    "    # --- info columns to keep (these overlap across base/exp) ---\n",
    "    info_cols = [\"__qid__\"]\n",
    "    if qcol:\n",
    "        info_cols.append(qcol)\n",
    "    for c in [\"ground_truth\", \"reference\", \"answer\", \"contexts\"]:\n",
    "        if c in df.columns:\n",
    "            info_cols.append(c)\n",
    "\n",
    "    # --- select + rename (so no overlap) ---\n",
    "    base_small = base[info_cols + mcols].copy()\n",
    "    exp_small  = exp[info_cols + mcols].copy()\n",
    "\n",
    "    rename_base = {c: f\"{c}_base\" for c in info_cols if c != \"__qid__\"}\n",
    "    rename_exp  = {c: f\"{c}_exp\"  for c in info_cols if c != \"__qid__\"}\n",
    "    rename_base.update({c: f\"{c}_base\" for c in mcols})\n",
    "    rename_exp.update({c: f\"{c}_exp\"  for c in mcols})\n",
    "\n",
    "    base_small = base_small.rename(columns=rename_base)\n",
    "    exp_small  = exp_small.rename(columns=rename_exp)\n",
    "\n",
    "    # --- merge safely ---\n",
    "    merged = exp_small.merge(base_small, on=\"__qid__\", how=\"outer\")\n",
    "\n",
    "    # --- compute deltas ---\n",
    "    for c in mcols:\n",
    "        cb = f\"{c}_base\"\n",
    "        ce = f\"{c}_exp\"\n",
    "        if cb in merged.columns and ce in merged.columns:\n",
    "            merged[f\"{c}_delta\"] = merged[ce] - merged[cb]\n",
    "\n",
    "    # --- convenience: make a unified question column (prefer exp, fallback base) ---\n",
    "    if qcol:\n",
    "        qe = f\"{qcol}_exp\"\n",
    "        qb = f\"{qcol}_base\"\n",
    "        if qe in merged.columns or qb in merged.columns:\n",
    "            merged[qcol] = None\n",
    "            if qe in merged.columns:\n",
    "                merged[qcol] = merged[qe]\n",
    "            if qb in merged.columns:\n",
    "                merged[qcol] = merged[qcol].fillna(merged[qb])\n",
    "\n",
    "    # --- order columns nicely ---\n",
    "    ordered = [\"__qid__\"]\n",
    "    if qcol and qcol in merged.columns:\n",
    "        ordered.append(qcol)\n",
    "\n",
    "    # keep references (unified view is optional; we keep exp/base separately)\n",
    "    for c in [\"ground_truth\", \"reference\"]:\n",
    "        # add unified if you want; here we keep exp/base columns only\n",
    "        pass\n",
    "\n",
    "    # metrics grouped\n",
    "    for c in mcols:\n",
    "        for suf in [\"_base\", \"_exp\", \"_delta\"]:\n",
    "            col = f\"{c}{suf}\"\n",
    "            if col in merged.columns:\n",
    "                ordered.append(col)\n",
    "\n",
    "    # then common info columns (exp/base)\n",
    "    tail_info = []\n",
    "    for c in [\"ground_truth\", \"reference\", \"answer\", \"contexts\"]:\n",
    "        ce, cb = f\"{c}_exp\", f\"{c}_base\"\n",
    "        if ce in merged.columns:\n",
    "            tail_info.append(ce)\n",
    "        if cb in merged.columns:\n",
    "            tail_info.append(cb)\n",
    "\n",
    "    remaining = [c for c in merged.columns if c not in ordered and c not in tail_info]\n",
    "    return merged[ordered + remaining + tail_info].copy()\n",
    "\n",
    "\n",
    "\n",
    "def _make_top_changes(delta_detail_df: pd.DataFrame, top_k=10) -> tuple[pd.DataFrame, pd.DataFrame, str]:\n",
    "    # choose primary metric for sorting\n",
    "    candidates = [\n",
    "        \"answer_relevancy_delta\", \"AnswerRelevancy_delta\",\n",
    "        \"faithfulness_delta\", \"Faithfulness_delta\",\n",
    "        \"context_precision_delta\", \"ContextPrecision_delta\",\n",
    "        \"context_recall_delta\", \"ContextRecall_delta\",\n",
    "    ]\n",
    "    sort_col = next((c for c in candidates if c in delta_detail_df.columns), None)\n",
    "    if sort_col is None:\n",
    "        # fallback: first *_delta numeric column\n",
    "        delta_cols = [c for c in delta_detail_df.columns if c.endswith(\"_delta\") and pd.api.types.is_numeric_dtype(delta_detail_df[c])]\n",
    "        sort_col = delta_cols[0] if delta_cols else \"\"\n",
    "\n",
    "    if not sort_col:\n",
    "        # nothing to rank\n",
    "        return pd.DataFrame(), pd.DataFrame(), \"\"\n",
    "\n",
    "    # pick minimal view columns\n",
    "    qcol = _pick_question_col(delta_detail_df)\n",
    "    view_cols = [c for c in [\"__qid__\", qcol, sort_col] if c and c in delta_detail_df.columns]\n",
    "    # add also base/exp of that metric if present\n",
    "    base_col = sort_col.replace(\"_delta\", \"_base\")\n",
    "    exp_col  = sort_col.replace(\"_delta\", \"_exp\")\n",
    "    for c in [base_col, exp_col]:\n",
    "        if c in delta_detail_df.columns and c not in view_cols:\n",
    "            view_cols.append(c)\n",
    "\n",
    "    regress = delta_detail_df.sort_values(sort_col, ascending=True).head(top_k)[view_cols].copy()\n",
    "    improve = delta_detail_df.sort_values(sort_col, ascending=False).head(top_k)[view_cols].copy()\n",
    "    return regress, improve, sort_col\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# main: compare + save (csv/json + optional snapshots)\n",
    "# ----------------------------\n",
    "def run_compare_and_save_all(\n",
    "    base_samples,\n",
    "    exp_samples,\n",
    "    project_root: Path,\n",
    "    prefix=\"ragas_compare\",\n",
    "    save_snapshots=True,\n",
    "    save_config=True,\n",
    "    save_samples_jsonl=True,\n",
    "    save_trace_jsonl=True,\n",
    "    trace_cols_priority=None,\n",
    "    save_delta_outputs=True,\n",
    "    top_k=10,\n",
    "):\n",
    "    # 1) evaluate\n",
    "    base_res, base_detail_df, base_summary_df = eval_ragas_with_details(base_samples, \"baseline\")\n",
    "    exp_res,  exp_detail_df,  exp_summary_df  = eval_ragas_with_details(exp_samples,  \"experiment\")\n",
    "\n",
    "    summary_df = pd.concat([base_summary_df, exp_summary_df], ignore_index=True)\n",
    "    detail_df  = pd.concat([base_detail_df,  exp_detail_df],  ignore_index=True)\n",
    "\n",
    "    # 2) run dir\n",
    "    run_dir, run_id, ts = _next_run_dir(project_root, prefix)\n",
    "\n",
    "    # 3) basic outputs\n",
    "    out_summary = run_dir / \"summary.csv\"\n",
    "    out_detail  = run_dir / \"detail.csv\"\n",
    "    out_meta    = run_dir / \"meta.json\"\n",
    "\n",
    "    summary_df.to_csv(out_summary, index=False, encoding=\"utf-8-sig\")\n",
    "    detail_df.to_csv(out_detail, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # --- dataset fingerprint (optional) ---\n",
    "    testset_info = None\n",
    "    if \"TESTSET_JSONL\" in globals():\n",
    "        p = Path(globals()[\"TESTSET_JSONL\"])\n",
    "        if p.exists():\n",
    "            testset_info = {\n",
    "                \"testset_path\": str(p),\n",
    "                \"testset_lines\": _count_jsonl_lines(p),\n",
    "                \"testset_sha1\": _file_sha1(p),\n",
    "            }\n",
    "\n",
    "    meta = {\n",
    "        \"created_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"run_id\": run_id,\n",
    "        \"timestamp\": ts,\n",
    "        \"run_dir\": str(run_dir),\n",
    "        \"prefix\": prefix,\n",
    "        \"n_base_samples\": len(base_samples),\n",
    "        \"n_exp_samples\": len(exp_samples),\n",
    "        \"metrics\": [getattr(m, \"__name__\", m.__class__.__name__) for m in METRICS],\n",
    "        \"llm\": {\"model\": \"gpt-4o-mini\"},\n",
    "        \"testset\": testset_info,\n",
    "    }\n",
    "    _write_json(out_meta, meta)\n",
    "\n",
    "    # 3-1) DELTA outputs (â­ï¸ NEW)\n",
    "    extra = {}\n",
    "    if save_delta_outputs:\n",
    "        # delta_summary\n",
    "        delta_summary_df = _make_delta_summary(summary_df)\n",
    "        out_delta_summary = run_dir / \"delta_summary.csv\"\n",
    "        delta_summary_df.to_csv(out_delta_summary, index=False, encoding=\"utf-8-sig\")\n",
    "        extra[\"out_delta_summary\"] = str(out_delta_summary)\n",
    "\n",
    "        # delta_detail\n",
    "        delta_detail_df = _make_delta_detail(detail_df)\n",
    "        out_delta_detail = run_dir / \"delta_detail.csv\"\n",
    "        delta_detail_df.to_csv(out_delta_detail, index=False, encoding=\"utf-8-sig\")\n",
    "        extra[\"out_delta_detail\"] = str(out_delta_detail)\n",
    "\n",
    "        # top changes (regressions/improvements)\n",
    "        top_regress, top_improve, sort_col = _make_top_changes(delta_detail_df, top_k=top_k)\n",
    "        out_top_regress = run_dir / \"top_regressions.csv\"\n",
    "        out_top_improve = run_dir / \"top_improvements.csv\"\n",
    "        top_regress.to_csv(out_top_regress, index=False, encoding=\"utf-8-sig\")\n",
    "        top_improve.to_csv(out_top_improve, index=False, encoding=\"utf-8-sig\")\n",
    "        extra[\"out_top_regressions\"] = str(out_top_regress)\n",
    "        extra[\"out_top_improvements\"] = str(out_top_improve)\n",
    "        extra[\"top_rank_metric\"] = sort_col\n",
    "\n",
    "    # 4) optional snapshots\n",
    "    if save_snapshots:\n",
    "        # 4-1) config.json\n",
    "        if save_config:\n",
    "            cfg_payload = {\n",
    "                \"created_at\": datetime.now().isoformat(),\n",
    "                \"llm\": {\"model\": \"gpt-4o-mini\"},\n",
    "                \"metrics\": [getattr(m, \"__name__\", m.__class__.__name__) for m in METRICS],\n",
    "                \"base_cfg\": _json_safe(globals().get(\"base_cfg\")) if \"base_cfg\" in globals() else None,\n",
    "                \"exp_cfg\":  _json_safe(globals().get(\"exp_cfg\"))  if \"exp_cfg\"  in globals() else None,\n",
    "            }\n",
    "            out_config = run_dir / \"config.json\"\n",
    "            _write_json(out_config, cfg_payload)\n",
    "            extra[\"out_config\"] = str(out_config)\n",
    "\n",
    "        # 4-2) input samples jsonl\n",
    "        if save_samples_jsonl:\n",
    "            out_samples_base = run_dir / \"samples_base.jsonl\"\n",
    "            out_samples_exp  = run_dir / \"samples_exp.jsonl\"\n",
    "            _write_jsonl(out_samples_base, base_samples)\n",
    "            _write_jsonl(out_samples_exp,  exp_samples)\n",
    "            extra[\"out_samples_base\"] = str(out_samples_base)\n",
    "            extra[\"out_samples_exp\"]  = str(out_samples_exp)\n",
    "\n",
    "        # 4-3) trace jsonl (from detail_df)\n",
    "        if save_trace_jsonl:\n",
    "            if trace_cols_priority is None:\n",
    "                trace_cols_priority = [\n",
    "                    \"id\", \"sample_id\", \"__qid__\",\n",
    "                    \"question\", \"normalized_question\", \"normalized_query\", \"query\",\n",
    "                    \"answer\", \"ground_truth\", \"reference\",\n",
    "                    \"contexts\",\n",
    "                    \"_trace\",\n",
    "                    \"retrieved_doc_ids\", \"retrieved_docs\", \"retrieval_scores\",\n",
    "                    \"rerank_selected_ids\", \"rerank_scores\",\n",
    "                    \"final_context_ids\", \"final_contexts\",\n",
    "                    \"latency_ms\", \"latency_sec\",\n",
    "                    \"run_tag\",\n",
    "                ]\n",
    "            trace_rows, used_cols = _df_to_jsonl_rows(detail_df, trace_cols_priority)\n",
    "            out_trace = run_dir / \"trace.jsonl\"\n",
    "            _write_jsonl(out_trace, trace_rows)\n",
    "            extra[\"out_trace\"] = str(out_trace)\n",
    "            extra[\"trace_cols_used\"] = used_cols\n",
    "\n",
    "    return {\n",
    "        \"base_res\": base_res,\n",
    "        \"exp_res\": exp_res,\n",
    "        \"summary_df\": summary_df,\n",
    "        \"detail_df\": detail_df,\n",
    "        \"run_dir\": str(run_dir),\n",
    "        \"out_summary\": str(out_summary),\n",
    "        \"out_detail\": str(out_detail),\n",
    "        \"out_meta\": str(out_meta),\n",
    "        **extra,\n",
    "    }\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# RUN\n",
    "# ----------------------------\n",
    "result = run_compare_and_save_all(\n",
    "    base_samples=BASE_SAMPLES,\n",
    "    exp_samples=EXP_SAMPLES,\n",
    "    project_root=PROJECT_ROOT,\n",
    "    prefix=\"ragas_compare\",\n",
    "    save_snapshots=True,\n",
    "    save_config=True,\n",
    "    save_samples_jsonl=True,\n",
    "    save_trace_jsonl=True,\n",
    "    save_delta_outputs=True,  # âœ… NEW\n",
    "    top_k=10,                 # âœ… NEW\n",
    ")\n",
    "\n",
    "print(\"âœ… run_dir:\", result[\"run_dir\"])\n",
    "print(\"âœ… saved:\", result[\"out_summary\"], result[\"out_detail\"], result[\"out_meta\"])\n",
    "\n",
    "# NEW delta outputs\n",
    "if \"out_delta_summary\" in result:\n",
    "    print(\"âœ… delta saved:\", result[\"out_delta_summary\"], result.get(\"out_delta_detail\"))\n",
    "    print(\"âœ… top changes metric:\", result.get(\"top_rank_metric\"))\n",
    "    print(\"âœ… top regressions:\", result.get(\"out_top_regressions\"))\n",
    "    print(\"âœ… top improvements:\", result.get(\"out_top_improvements\"))\n",
    "\n",
    "# snapshots\n",
    "if \"out_config\" in result:\n",
    "    print(\"âœ… extra saved config :\", result[\"out_config\"])\n",
    "if \"out_samples_base\" in result:\n",
    "    print(\"âœ… extra saved samples:\", result[\"out_samples_base\"], \"and\", result.get(\"out_samples_exp\"))\n",
    "if \"out_trace\" in result:\n",
    "    print(\"âœ… extra saved trace  :\", result[\"out_trace\"])\n",
    "    print(\"âœ… trace columns used :\", result.get(\"trace_cols_used\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6845ece-5d8b-47af-8aaf-0a1dc5ef87c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa7f586-c190-4912-a1c7-b08b1f1703db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0e570-c6a8-44d7-8628-8f2fb44c5755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9df58dc-5dbf-4542-945f-0cf156a581dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83ccdd3-e88b-4212-b6a2-fe100c7f1cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860dd3a7-6a46-4477-84a4-ef2861fe6829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c068515-0f85-4ef4-84dd-c22ad87302dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e96bfa9-487a-49b6-b4b1-df555cf06ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3be758-526b-4c43-a796-c5036f476881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5335ed-f34b-4677-950b-bd10cb6a0cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c55f40-ddfa-4a3a-a7dc-abd143d6eda4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97946b48-b0d8-4dd8-9a89-f6a451d7e36a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d03785-23ae-4846-9368-4a45eaf6d3be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ab5d29-e922-4a2c-a0c0-fcb337ae36ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tfenv)",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
