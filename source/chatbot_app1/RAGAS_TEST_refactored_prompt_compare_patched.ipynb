{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4decb793",
   "metadata": {},
   "source": [
    "# RAGAS Evaluation Notebook (Clean)\n",
    "\n",
    "This notebook evaluates **baseline vs experiment** RAG pipelines using RAGAS and saves **summary/detail** outputs per run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf76203-b1ff-4ee3-b17e-06004982adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdaa3541-ee50-497b-81cd-91c2274ba4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e96fb77-1e91-45b0-a0c8-0513c54bcea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1b5f49e-4fde-42f4-a42a-40dd814c571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys, subprocess, textwrap\n",
    "\n",
    "# def sh(cmd):\n",
    "#     print(\">\", cmd)\n",
    "#     r = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "#     print(r.stdout)\n",
    "#     if r.stderr.strip():\n",
    "#         print(\"[stderr]\")\n",
    "#         print(r.stderr)\n",
    "\n",
    "# print(\"python:\", sys.executable)\n",
    "# print(\"version:\", sys.version)\n",
    "\n",
    "# # í˜„ì¬ íŒ¨í‚¤ì§€ ìƒíƒœ í™•ì¸\n",
    "# sh(\"python -c \\\"import numpy; print('numpy', numpy.__version__)\\\"\")\n",
    "# sh(\"python -c \\\"import pyarrow; print('pyarrow', pyarrow.__version__)\\\"\")\n",
    "# sh(\"python -c \\\"import datasets; print('datasets', datasets.__version__)\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b2f1ee4-078f-4f37-9e75-6da6b135d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys, subprocess\n",
    "\n",
    "# def pip(cmd):\n",
    "#     print(\">\", cmd)\n",
    "#     r = subprocess.run([sys.executable, \"-m\", \"pip\"] + cmd.split(), capture_output=True, text=True)\n",
    "#     print(r.stdout)\n",
    "#     if r.stderr.strip():\n",
    "#         print(\"[stderr]\")\n",
    "#         print(r.stderr)\n",
    "\n",
    "# # 1) ì œê±°\n",
    "# pip(\"uninstall -y pyarrow datasets numpy\")\n",
    "\n",
    "# # 2) ì¬ì„¤ì¹˜: numpy<2 + ìµœì‹  pyarrow + datasets(ë„ˆê°€ ì“°ë˜ ë²„ì „)\n",
    "# pip(\"install numpy<2 pyarrow>=14 datasets==2.19.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f6d4dab-9416-4c6e-b037-2526c17623ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ai\\source\\chatbot_app\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… METRICS: ['ContextPrecision', 'ContextRecall', 'Faithfulness']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18412\\1708373533.py:6: DeprecationWarning: Importing context_precision from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_precision\n",
      "  from ragas.metrics import context_precision, context_recall, faithfulness\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18412\\1708373533.py:6: DeprecationWarning: Importing context_recall from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_recall\n",
      "  from ragas.metrics import context_precision, context_recall, faithfulness\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18412\\1708373533.py:6: DeprecationWarning: Importing faithfulness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import faithfulness\n",
      "  from ragas.metrics import context_precision, context_recall, faithfulness\n"
     ]
    }
   ],
   "source": [
    "# ---- RAGAS metrics: version-tolerant loader ----\n",
    "def build_metrics():\n",
    "    # Aì•ˆ: embeddings ì˜ì¡´ ê°€ëŠ¥ì„±ì´ í° AnswerRelevancyëŠ” ë¹¼ê³  \"ì™„ì£¼\"ë¶€í„°\n",
    "    # 1) í•¨ìˆ˜í˜• metric\n",
    "    try:\n",
    "        from ragas.metrics import context_precision, context_recall, faithfulness\n",
    "        return [context_precision, context_recall, faithfulness]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) í´ë˜ìŠ¤í˜• metric\n",
    "    try:\n",
    "        from ragas.metrics import ContextPrecision, ContextRecall, Faithfulness\n",
    "        return [ContextPrecision(), ContextRecall(), Faithfulness()]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) fallback íƒìƒ‰\n",
    "    import ragas.metrics as m\n",
    "    wanted = [\"ContextPrecision\", \"ContextRecall\", \"Faithfulness\"]\n",
    "    found = []\n",
    "    for name in wanted:\n",
    "        if hasattr(m, name):\n",
    "            found.append(getattr(m, name)())\n",
    "    if found:\n",
    "        return found\n",
    "\n",
    "    raise ImportError(\n",
    "        \"RAGAS metrics import failed for A-plan (without AnswerRelevancy). \"\n",
    "        \"Paste `pip show ragas` and `python -c \\\"import ragas; print(ragas.__version__)\\\"`.\"\n",
    "    )\n",
    "\n",
    "METRICS = build_metrics()\n",
    "print(\"âœ… METRICS:\", [getattr(x, '__name__', x.__class__.__name__) for x in METRICS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24085385-5708-4be6-8d18-1c17f11f456d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ragas version: 0.4.3\n"
     ]
    }
   ],
   "source": [
    "import ragas\n",
    "print(\"ragas version:\", getattr(ragas, \"__version__\", \"unknown\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85cba8ec-b4a6-4dcf-8fda-13b78e62e686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… METRICS: ['ContextPrecision', 'ContextRecall', 'Faithfulness', 'AnswerRelevancy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18412\\3044559028.py:1: DeprecationWarning: Importing context_precision from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_precision\n",
      "  from ragas.metrics import context_precision, context_recall, faithfulness, answer_relevancy\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18412\\3044559028.py:1: DeprecationWarning: Importing context_recall from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_recall\n",
      "  from ragas.metrics import context_precision, context_recall, faithfulness, answer_relevancy\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18412\\3044559028.py:1: DeprecationWarning: Importing faithfulness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import faithfulness\n",
      "  from ragas.metrics import context_precision, context_recall, faithfulness, answer_relevancy\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18412\\3044559028.py:1: DeprecationWarning: Importing answer_relevancy from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import answer_relevancy\n",
      "  from ragas.metrics import context_precision, context_recall, faithfulness, answer_relevancy\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import context_precision, context_recall, faithfulness, answer_relevancy\n",
    "\n",
    "METRICS = [context_precision, context_recall, faithfulness, answer_relevancy]\n",
    "print(\"âœ… METRICS:\", [getattr(m, \"__name__\", m.__class__.__name__) for m in METRICS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ac8615-1f58-462f-8f2e-811d673d8d56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22cee99-4aaa-40fb-bbbb-451aa89c8c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53db9df-4693-41e1-b255-bebb90c4be53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "663e3c80",
   "metadata": {},
   "source": [
    "## 0) Environment & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3772574e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: C:\\ai\\source\\chatbot_app\n",
      "TESTSET_PATH: C:\\ai\\source\\chatbot_app\\ragas_testset_full.jsonl\n",
      "exists: True\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PATH CONFIG (only this cell is modified)\n",
    "# ============================================================\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys, importlib\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# âœ… í”„ë¡œì íŠ¸ ë£¨íŠ¸ (ìƒˆ ê²½ë¡œ)\n",
    "PROJECT_ROOT = Path(r\"C:\\ai\\source\\chatbot_app\")\n",
    "\n",
    "# âœ… ëª¨ë“ˆ ê²½ë¡œ (ì›ë˜ ì“°ë˜ êµ¬ì¡° ê·¸ëŒ€ë¡œ)\n",
    "MODULE_DIR = PROJECT_ROOT / \"modules\"\n",
    "\n",
    "# âœ… í™˜ê²½ë³€ìˆ˜\n",
    "ENV_PATH = PROJECT_ROOT / \".env\"\n",
    "\n",
    "# âœ… ê²°ê³¼ ì €ì¥ ë£¨íŠ¸\n",
    "RUNS_DIR = PROJECT_ROOT / \"results\" / \"ragas_runs\"\n",
    "\n",
    "# â­•ï¸ ì—¬ê¸°ì„œ ì–´ë–¤ í…ŒìŠ¤íŠ¸ì…‹ ì“¸ì§€ ë„¤ê°€ ì§ì ‘ ì„ íƒ\n",
    "# TESTSET_PATH = PROJECT_ROOT / \"ragas_testset_single.jsonl\"\n",
    "# TESTSET_PATH = PROJECT_ROOT / \"ragas_testset_v1_from_docx.jsonl\"\n",
    "TESTSET_PATH = PROJECT_ROOT / \"ragas_testset_full.jsonl\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# setup\n",
    "# ------------------------------------------------------------\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "sys.path.insert(0, str(MODULE_DIR))\n",
    "importlib.invalidate_caches()\n",
    "\n",
    "if ENV_PATH.exists():\n",
    "    load_dotenv(ENV_PATH)\n",
    "\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"TESTSET_PATH:\", TESTSET_PATH)\n",
    "print(\"exists:\", TESTSET_PATH.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc0093e",
   "metadata": {},
   "source": [
    "## 1) Load testset (JSONL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8108580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… rows: 25\n",
      "âœ… keys example: dict_keys(['question_id', 'question', 'ground_truth', 'contexts', 'meta'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>contexts</th>\n",
       "      <th>meta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q001</td>\n",
       "      <td>ì „ì…ì‹ ê³ Â·í™•ì •ì¼ì í–ˆëŠ”ë°, í™•ì •ì¼ìë¶€ ë‚´ìš©ê¹Œì§€ ì¤‘ìš”í•œê°€ìš”?</td>\n",
       "      <td>ë„¤, ì¤‘ìš”í•©ë‹ˆë‹¤. í™•ì •ì¼ìë¥¼ ë°›ì•˜ë‹¤ëŠ” ì‚¬ì‹¤ë§Œìœ¼ë¡œ ëë‚˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, í™•ì •ì¼ìë¶€ì— ...</td>\n",
       "      <td>[ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì œ3ì¡°ì˜2 ì œ2í•­, ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì‹œí–‰ë ¹ ì œ4ì¡°, ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸...</td>\n",
       "      <td>{'source': 'RAGAS í•™ìŠµìš© ì§ˆë¬¸.docx', 'version': 'v1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q002</td>\n",
       "      <td>ê³„ì•½ì„œì— 1ë…„ì´ë¼ê³  ì¨ ìˆìœ¼ë©´, 1ë…„ ì§€ë‚˜ë©´ ë¬´ì¡°ê±´ ë‚˜ê°€ì•¼ í•˜ë‚˜ìš”?</td>\n",
       "      <td>ì•„ë‹ˆìš”. ê³„ì•½ì„œì— 1ë…„ì´ë¼ê³  ì í˜€ ìˆì–´ë„, 1ë…„ì´ ì§€ë‚˜ë©´ ë¬´ì¡°ê±´ ë‚˜ê°€ì•¼ í•˜ëŠ” ê²ƒì€ ...</td>\n",
       "      <td>[ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì œ4ì¡° ì œ1í•­]</td>\n",
       "      <td>{'source': 'RAGAS í•™ìŠµìš© ì§ˆë¬¸.docx', 'version': 'v1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q003</td>\n",
       "      <td>ì§‘ì£¼ì¸ì´ ì•„ë¬´ ë§ ì•ˆ í–ˆëŠ”ë°, ê³„ì•½ì´ ìë™ìœ¼ë¡œ ì—°ì¥ëœ ê±´ê°€ìš”?</td>\n",
       "      <td>ë„¤, ì¼ì •í•œ ìš”ê±´ì„ ì¶©ì¡±í•˜ë©´ ê³„ì•½ì€ ìë™ìœ¼ë¡œ ì—°ì¥ëœ ê²ƒìœ¼ë¡œ ë´…ë‹ˆë‹¤.\\nì£¼íƒì„ëŒ€ì°¨ë³´í˜¸...</td>\n",
       "      <td>[ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì œ6ì¡° ì œ1í•­]</td>\n",
       "      <td>{'source': 'RAGAS í•™ìŠµìš© ì§ˆë¬¸.docx', 'version': 'v1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question_id                               question  \\\n",
       "0        q001       ì „ì…ì‹ ê³ Â·í™•ì •ì¼ì í–ˆëŠ”ë°, í™•ì •ì¼ìë¶€ ë‚´ìš©ê¹Œì§€ ì¤‘ìš”í•œê°€ìš”?   \n",
       "1        q002  ê³„ì•½ì„œì— 1ë…„ì´ë¼ê³  ì¨ ìˆìœ¼ë©´, 1ë…„ ì§€ë‚˜ë©´ ë¬´ì¡°ê±´ ë‚˜ê°€ì•¼ í•˜ë‚˜ìš”?   \n",
       "2        q003     ì§‘ì£¼ì¸ì´ ì•„ë¬´ ë§ ì•ˆ í–ˆëŠ”ë°, ê³„ì•½ì´ ìë™ìœ¼ë¡œ ì—°ì¥ëœ ê±´ê°€ìš”?   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0  ë„¤, ì¤‘ìš”í•©ë‹ˆë‹¤. í™•ì •ì¼ìë¥¼ ë°›ì•˜ë‹¤ëŠ” ì‚¬ì‹¤ë§Œìœ¼ë¡œ ëë‚˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, í™•ì •ì¼ìë¶€ì— ...   \n",
       "1  ì•„ë‹ˆìš”. ê³„ì•½ì„œì— 1ë…„ì´ë¼ê³  ì í˜€ ìˆì–´ë„, 1ë…„ì´ ì§€ë‚˜ë©´ ë¬´ì¡°ê±´ ë‚˜ê°€ì•¼ í•˜ëŠ” ê²ƒì€ ...   \n",
       "2  ë„¤, ì¼ì •í•œ ìš”ê±´ì„ ì¶©ì¡±í•˜ë©´ ê³„ì•½ì€ ìë™ìœ¼ë¡œ ì—°ì¥ëœ ê²ƒìœ¼ë¡œ ë´…ë‹ˆë‹¤.\\nì£¼íƒì„ëŒ€ì°¨ë³´í˜¸...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì œ3ì¡°ì˜2 ì œ2í•­, ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì‹œí–‰ë ¹ ì œ4ì¡°, ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸...   \n",
       "1                                 [ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì œ4ì¡° ì œ1í•­]   \n",
       "2                                 [ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì œ6ì¡° ì œ1í•­]   \n",
       "\n",
       "                                                meta  \n",
       "0  {'source': 'RAGAS í•™ìŠµìš© ì§ˆë¬¸.docx', 'version': 'v1...  \n",
       "1  {'source': 'RAGAS í•™ìŠµìš© ì§ˆë¬¸.docx', 'version': 'v1...  \n",
       "2  {'source': 'RAGAS í•™ìŠµìš© ì§ˆë¬¸.docx', 'version': 'v1...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TESTSET_JSONL = PROJECT_ROOT / \"ragas_testset_full.jsonl\"  # change if needed\n",
    "assert TESTSET_JSONL.exists(), f\"âŒ JSONL not found: {TESTSET_JSONL}\"\n",
    "\n",
    "rows = []\n",
    "with open(TESTSET_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        rows.append(json.loads(line))\n",
    "\n",
    "print(\"âœ… rows:\", len(rows))\n",
    "print(\"âœ… keys example:\", rows[0].keys())\n",
    "pd.DataFrame(rows[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347b7dac-4985-472c-a539-581bb0105eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd5607-f5ef-4c8c-a324-21e45c1831e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec1529b4",
   "metadata": {},
   "source": [
    "## 2) Define baseline & experiment configs\n",
    "\n",
    "- Keep **base_cfg** stable.\n",
    "- Only put **changed knobs** in `exp_cfg = replace(base_cfg, ...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69c6ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import replace\n",
    "from rag_module import RAGConfig\n",
    "\n",
    "# =========================\n",
    "# Base config (edit as needed)\n",
    "# =========================\n",
    "# =========================\n",
    "# Base config (Champion = ì´ì „ ì‹¤í—˜ì—ì„œ ì´ê¸´ ì„¤ì •ìœ¼ë¡œ ìŠ¹ê²©)\n",
    "# =========================\n",
    "base_cfg = RAGConfig(\n",
    "    # -------- LLMs --------\n",
    "    normalize_model=\"solar-pro2\",\n",
    "    generation_model=\"solar-pro2\",\n",
    "    temperature=0.1,\n",
    "    normalize_temperature=0.0,\n",
    "\n",
    "    # -------- Embeddings --------\n",
    "    embedding_backend=\"upstage\",\n",
    "    embedding_model=\"solar-embedding-1-large-passage\",\n",
    "\n",
    "    # -------- Retrieval sizes (A íŒŒíŠ¸: ê³ ì •) --------\n",
    "    k_law=7,\n",
    "    k_rule=7,\n",
    "    k_case=3,\n",
    "    search_multiplier=4,\n",
    "\n",
    "    # -------- Candidate-level BM25 --------\n",
    "    enable_bm25=True,\n",
    "    sparse_mode=\"auto\",\n",
    "    sparse_k_law=None,\n",
    "    sparse_k_rule=None,\n",
    "    sparse_k_case=None,\n",
    "\n",
    "    bm25_algorithm=\"okapi\",\n",
    "    bm25_k1=1.5,\n",
    "    bm25_b=0.85,\n",
    "    bm25_use_kiwi=True,\n",
    "    bm25_max_doc_chars=2600,   # â¬†ï¸ ì„¤ëª…ìš© ë¬¸ë§¥ ì—¬ì§€\n",
    "\n",
    "    # -------- Sparse: BM25-title --------\n",
    "    enable_bm25_title=True,\n",
    "    bm25_title_field=\"title\",\n",
    "    bm25_title_max_chars=512,\n",
    "    hybrid_sparse_title_ratio=0.35,\n",
    "\n",
    "    # -------- Fusion --------\n",
    "    hybrid_fusion=\"rrf\",\n",
    "    hybrid_dense_weight=0.5,\n",
    "    hybrid_sparse_weight=0.5,\n",
    "    rrf_k=60,\n",
    "\n",
    "    # -------- Rerank (LLM ë¹„êµìš© ì™„í™” ì„¸íŒ…) --------\n",
    "    enable_rerank=True,\n",
    "    rerank_threshold=0.20,     # â¬‡ï¸ ì™„í™” (LLMì´ ì„ íƒí•  ì—¬ì§€)\n",
    "    rerank_model=\"rerank-multilingual-v3.0\",\n",
    "    rerank_max_documents=22,   # â¬†ï¸ ì§ˆë¬¸ ì˜ë„ ë³´ê°• ë¬¸ë§¥ í¬í•¨\n",
    "    rerank_doc_max_chars=2600, # â¬†ï¸ faithfulness / relevancy ì•ˆì •í™”\n",
    "\n",
    "    # -------- 2-stage case expansion --------\n",
    "    case_candidate_k=40,\n",
    "    case_expand_top_n=None,\n",
    "    case_context_top_k=50,\n",
    ")\n",
    "\n",
    "# ======================================\n",
    "# EXP config (ëª¨ë¸ë§Œ ë°”ê¾¼ ì‹¤í—˜êµ°)\n",
    "# ======================================\n",
    "cfg = base_cfg \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162fbbb5",
   "metadata": {},
   "source": [
    "## 3) Build pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba0f69f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 16:27:29,141 - rag_module - INFO - ğŸ”— Pinecone 3ì¤‘ ì¸ë±ìŠ¤ ì—°ê²° ì¤‘...\n",
      "2026-02-05 16:27:33,372 - rag_module - INFO - âœ… [Law / Rule / Case] 3ê°œ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ!\n",
      "2026-02-05 16:27:34,667 - rag_module - INFO - âœ… Kiwi í† í¬ë‚˜ì´ì € ì‚¬ìš© (BM25)\n",
      "2026-02-05 16:27:37,108 - rag_module - INFO - ğŸ”— Pinecone 3ì¤‘ ì¸ë±ìŠ¤ ì—°ê²° ì¤‘...\n",
      "2026-02-05 16:27:37,110 - rag_module - INFO - âœ… [Law / Rule / Case] 3ê°œ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ!\n",
      "2026-02-05 16:27:37,627 - rag_module - INFO - âœ… Kiwi í† í¬ë‚˜ì´ì € ì‚¬ìš© (BM25)\n",
      "2026-02-05 16:27:39,775 - rag_module - INFO - ğŸ”— Pinecone 3ì¤‘ ì¸ë±ìŠ¤ ì—°ê²° ì¤‘...\n",
      "2026-02-05 16:27:39,779 - rag_module - INFO - âœ… [Law / Rule / Case] 3ê°œ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ!\n",
      "2026-02-05 16:27:40,515 - rag_module - INFO - âœ… Kiwi í† í¬ë‚˜ì´ì € ì‚¬ìš© (BM25)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… pipelines ready\n"
     ]
    }
   ],
   "source": [
    "from rag_module import create_pipeline\n",
    "\n",
    "pipe_p0 = create_pipeline(config=cfg)\n",
    "pipe_p1 = create_pipeline(config=cfg)\n",
    "pipe_p2 = create_pipeline(config=cfg)\n",
    "\n",
    "print(\"âœ… pipelines ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4223f9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… prompt variants ready: P1/P2\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Prompt variants (for prompt-only A/B test)\n",
    "# ----------------------------\n",
    "SYSTEM_PROMPT_P1 = \"\"\"\n",
    "ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ 'ì£¼íƒ ì„ëŒ€ì°¨(ì „ì›”ì„¸)Â·ì „ì„¸ì‚¬ê¸° ì˜ˆë°©' ë¶„ì•¼ì˜ ë²•ë¥ ì •ë³´ ìƒë‹´ AIì…ë‹ˆë‹¤.\n",
    "ëª©í‘œëŠ” ì„ì°¨ì¸(ì„¸ì…ì)ì˜ ê¶Œë¦¬ ë³´í˜¸ì™€ í”¼í•´ ì˜ˆë°©ì„ ìµœìš°ì„ ìœ¼ë¡œ, ì‚¬ìš©ìê°€ ë°”ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ì•ˆì „í•œ ë‹¤ìŒ í–‰ë™ì„ ì œì‹œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ [ì°¸ê³  ë¬¸ì„œ]ì—ëŠ” ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ì •ë³´ê°€ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "[SECTION 0: ì‚¬ìš©ì ê³„ì•½ì„œ OCR (ìˆì„ ê²½ìš°)]\n",
    "- ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ ê³„ì•½ì„œ ë˜ëŠ” ì•ˆë‚´ë¬¸ì„ OCRë¡œ ì¶”ì¶œí•œ ì›ë¬¸ì…ë‹ˆë‹¤.\n",
    "- ì´ëŠ” ë³¸ ì‚¬ê±´ì˜ 'ì‚¬ì‹¤ê´€ê³„'ì— í•´ë‹¹í•˜ë©°, ì¡´ì¬í•˜ëŠ” ê²½ìš° ë°˜ë“œì‹œ ìµœìš°ì„ ìœ¼ë¡œ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "- ë¬¸ì„œì— ëª…ì‹œëœ ì œëª©, ë¬¸êµ¬, ì¡°í•­, ìˆ«ì, ì‹œì  ë“±ì„ ê·¸ëŒ€ë¡œ ì¸ìš©í•˜ê±°ë‚˜ ìš”ì•½í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "- SECTION 0ì— ì—†ëŠ” ë‚´ìš©ì„ ì„ì˜ë¡œ ì¶”ì •í•˜ê±°ë‚˜ ì¼ë°˜ì ì¸ ê³„ì•½ì„œ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.\n",
    "\n",
    "[SECTION 1 ì´í›„]\n",
    "- ê´€ë ¨ ë²•ë ¹, íŒë¡€, ê·œì • ë“± ì¼ë°˜ì ì¸ ë²•ë¥  ì •ë³´ì…ë‹ˆë‹¤.\n",
    "- ì´ëŠ” ê³„ì•½ì„œ ë‚´ìš©ì„ í•´ì„Â·í‰ê°€í•˜ê¸° ìœ„í•œ ë³´ì¡° ìë£Œì…ë‹ˆë‹¤.\n",
    "\n",
    "[ëŒ€í™” í†¤/íƒœë„]\n",
    "- ì§§ê³  ëª…í™•í•œ ë¬¸ì¥ìœ¼ë¡œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
    "- ì‚¬ìš©ìì˜ ë¶ˆì•ˆ/ê¸´ê¸‰ì„±ì„ ê³ ë ¤í•´ 'ì§€ê¸ˆ ë‹¹ì¥ í•  ì¼'ì„ ë¨¼ì € ì œì‹œí•©ë‹ˆë‹¤.\n",
    "- ë‹¨ì •ì´ ì–´ë ¤ìš°ë©´ \"ê°€ëŠ¥ì„±ì´ í¼/ì¶”ê°€ í™•ì¸ í•„ìš”\"ë¡œ ë§í•˜ê³ , í™•ì¸ ì§ˆë¬¸ 2~4ê°œë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.\n",
    "\n",
    "[ì¶œì²˜Â·ì¡°í•­ í‘œê¸° ê·œì¹™(ë§¤ìš° ì¤‘ìš”)]\n",
    "- [ì°¸ê³  ë¬¸ì„œ]ëŠ” ë²•ì  ìœ„ê³„(ë²•ë ¹ > í•˜ìœ„ê·œì • > íŒë¡€) ë° ì¤‘ìš”ë„(priority) ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ë˜ì–´ ì œê³µë©ë‹ˆë‹¤.\n",
    "- ê° í•­ëª©ì€ \"{{src_title}} {{article}} - {{text}}\" í˜•ì‹ì…ë‹ˆë‹¤.\n",
    "- ì¸ìš© ì‹œ \"src_title article\"ì„ ë°˜ë“œì‹œ í•¨ê»˜ ê·¸ëŒ€ë¡œ ë§í•˜ì„¸ìš”.\n",
    "- ì°¸ê³  ë¬¸ì„œì— ì—†ëŠ” ë²•ë ¹ëª…Â·ì¡°ë¬¸ ë²ˆí˜¸ë¥¼ ë§Œë“¤ì–´ë‚´ê±°ë‚˜, ë‹¤ë¥¸ ë²•ë ¹ëª…ìœ¼ë¡œ ë°”ê¿” ê²°í•©í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "- ê·¼ê±°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¡°ë¬¸ ë²ˆí˜¸ë¥¼ ì–¸ê¸‰í•˜ê³ , ê·¼ê±°ê°€ ë¶ˆì¶©ë¶„í•˜ë©´ \"ì œê³µëœ ìë£Œ ë²”ìœ„ì—ì„œ í™•ì¸ë˜ëŠ” ë‚´ìš©\"ì´ë¼ê³  ì œí•œí•˜ì„¸ìš”.\n",
    "\n",
    "[ë‹µë³€ ìƒì„± ì›ì¹™]\n",
    "1) ë²•ì  ìœ„ê³„ ì¤€ìˆ˜\n",
    "- SECTION 0ì´ ì œê³µëœ ê²½ìš°, ë°˜ë“œì‹œ ê·¸ ë‚´ìš©ì„ ë¨¼ì € ì½ê³  ë‹µë³€í•˜ì„¸ìš”.\n",
    "- [SECTION 1: í•µì‹¬ ë²•ë ¹]ì„ ìµœìš°ì„  íŒë‹¨ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ìŠµë‹ˆë‹¤.\n",
    "- [SECTION 1]ì´ ëª¨í˜¸í•  ë•Œë§Œ [SECTION 2: ê´€ë ¨ ê·œì •], [SECTION 3: íŒë¡€/ì‚¬ë¡€]ë¥¼ ë³´ì¶© ê·¼ê±°ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "2) ë‹µë³€ êµ¬ì¡°(ê¶Œë¦¬ë³´í˜¸/í”¼í•´ì˜ˆë°© ì¤‘ì‹¬) â€” ë°˜ë“œì‹œ ì´ ìˆœì„œë¡œ\n",
    "A. í•œ ì¤„ ê²°ë¡ (ë‘ê´„ì‹)\n",
    "B. ì§€ê¸ˆ ë‹¹ì¥ í•  ì¼(ì²´í¬ë¦¬ìŠ¤íŠ¸ 3~7ê°œ)\n",
    "C. ë²•ì  ê·¼ê±°(ê·¼ê±° ë¬¸ì„œì—ì„œ 1~3ê°œ ì¸ìš©. ì¸ìš©ì€ \"src_title article\"ë¡œ)\n",
    "D. ì ˆì°¨/ì¦ë¹™(ê¸°ê´€Â·ì„œë¥˜Â·ê¸°í•œÂ·ì£¼ì˜ì )\n",
    "E. ìœ ì‚¬ íŒë¡€/ì‚¬ë¡€(ìˆë‹¤ë©´ 1~2ê°œ + ì ìš© í•œê³„)\n",
    "F. ì¶”ê°€ í™•ì¸ ì§ˆë¬¸(ì‚¬ì‹¤ê´€ê³„ 2~4ê°œ)\n",
    "\n",
    "3) ì•ˆì „ì¥ì¹˜/ë©´ì±…\n",
    "- ë‹µë³€ì€ ì¼ë°˜ì ì¸ ë²•ë¥ ì •ë³´ì´ë©°, êµ¬ì²´ ì‚¬ê±´ì˜ ë²•ë¥  ìë¬¸ì´ ì•„ë‹˜ì„ ë§ˆì§€ë§‰ì— í•œ ë¬¸ì¥ìœ¼ë¡œ ê³ ì§€í•˜ì„¸ìš”.\n",
    "- ì‚¬ìš©ìì—ê²Œ ë¶ˆë¦¬í•œ ê²°ê³¼ ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë©´, ì‚¬ì‹¤í™•ì¸Â·ì¦ê±°ë³´ì „Â·ê¸°í•œ ì¤€ìˆ˜ë¥¼ ìš°ì„  ì•ˆë‚´í•˜ì„¸ìš”.\n",
    "\n",
    "[ì¤‘ìš” ì œí•œ]\n",
    "- ì°¸ê³  ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì •/ì¼ë°˜í™”í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "- ë¬¸ì„œì— ì§ì ‘ ê·¼ê±°ê°€ ì—†ìœ¼ë©´, ê²°ë¡ ì„ ê°•í•˜ê²Œ ë‹¨ì •í•˜ì§€ ë§ê³  \"ì¶”ê°€ í™•ì¸ í•„ìš”\"ë¡œ ì²˜ë¦¬í•˜ì„¸ìš”.\n",
    "\n",
    "[ì°¸ê³  ë¬¸ì„œ]\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_P2 = \"\"\"\n",
    "ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ 'ì£¼íƒ ì„ëŒ€ì°¨(ì „ì›”ì„¸)Â·ì „ì„¸ì‚¬ê¸° ì˜ˆë°©' ë¶„ì•¼ì˜ ë²•ë¥ ì •ë³´ ìƒë‹´ AIì…ë‹ˆë‹¤.\n",
    "ëª©í‘œëŠ” ì„ì°¨ì¸(ì„¸ì…ì)ì˜ ê¶Œë¦¬ ë³´í˜¸ì™€ í”¼í•´ ì˜ˆë°©ì„ ìµœìš°ì„ ìœ¼ë¡œ, ì‚¬ìš©ìê°€ ë°”ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ì•ˆì „í•œ ë‹¤ìŒ í–‰ë™ì„ ì œì‹œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "[ìš°ì„ ìˆœìœ„]\n",
    "- ì‚¬ìš©ìê°€ ë‹¹ì¥ í”¼í•´ë¥¼ ë§‰ëŠ” í–‰ë™(ì¦ê±°ë³´ì „/ê¸°í•œì¤€ìˆ˜/ê¶Œë¦¬í™•ë³´)ì„ ìµœìš°ì„ ìœ¼ë¡œ ì•ˆë‚´í•˜ì„¸ìš”.\n",
    "- ë‹¤ë§Œ, [ì°¸ê³  ë¬¸ì„œ]ì— ì—†ëŠ” ë²•ë ¹/ì¡°ë¬¸/ì‚¬ì‹¤ì€ ì ˆëŒ€ ë§Œë“¤ì§€ ë§ˆì„¸ìš”.\n",
    "\n",
    "[ì°¸ê³  ë¬¸ì„œ êµ¬ì„±]\n",
    "[SECTION 0: ì‚¬ìš©ì ê³„ì•½ì„œ OCR (ìˆì„ ê²½ìš°)]\n",
    "- ë³¸ ì‚¬ê±´ì˜ ì‚¬ì‹¤ê´€ê³„ì…ë‹ˆë‹¤. ì¡´ì¬í•˜ë©´ ë°˜ë“œì‹œ ì²« ë²ˆì§¸ë¡œ ìš”ì•½/ì¸ìš©í•˜ì„¸ìš”.\n",
    "- SECTION 0ì— ì—†ëŠ” ë‚´ìš©ì„ ìƒìƒí•´ì„œ ê³„ì•½ì„œ ë‚´ìš©ì„ ì±„ìš°ì§€ ë§ˆì„¸ìš”.\n",
    "\n",
    "[SECTION 1 ì´í›„]\n",
    "- ì¼ë°˜ ë²•ë ¹/ê·œì •/íŒë¡€ì…ë‹ˆë‹¤. SECTION 0 í•´ì„ê³¼ ì‹¤í–‰ ì¡°ì¹˜ë¥¼ ë’·ë°›ì¹¨í•˜ëŠ” ê·¼ê±°ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.\n",
    "\n",
    "[ì¶œì²˜Â·ì¡°í•­ í‘œê¸° ê·œì¹™(ë§¤ìš° ì¤‘ìš”)]\n",
    "- ê° í•­ëª©ì€ \"{{src_title}} {{article}} - {{text}}\" í˜•ì‹ì…ë‹ˆë‹¤.\n",
    "- ì¸ìš© ì‹œ ë°˜ë“œì‹œ \"src_title article\"ì„ í•¨ê»˜ ê·¸ëŒ€ë¡œ ë§í•˜ì„¸ìš”.\n",
    "- ì°¸ê³  ë¬¸ì„œì— ì—†ëŠ” ë²•ë ¹ëª…Â·ì¡°ë¬¸ë²ˆí˜¸ë¥¼ ë§Œë“¤ê±°ë‚˜ ì„ì§€ ë§ˆì„¸ìš”.\n",
    "\n",
    "[ë‹µë³€ ë°©ì‹]\n",
    "- ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ íŒë‹¨ì´ ê°€ëŠ¥í•˜ë©´ íšŒí”¼í•˜ì§€ ë§ê³  ê²°ë¡ ì„ ë§í•˜ì„¸ìš”.\n",
    "- ê·¸ëŸ¬ë‚˜ ê·¼ê±°ê°€ ë¶€ì¡±í•´ ë‹¨ì •ì´ ìœ„í—˜í•˜ë©´, ê²°ë¡ ì„ \"ì¶”ê°€ í™•ì¸ í•„ìš”\"ë¡œ ì „í™˜í•˜ê³  í™•ì¸ ì§ˆë¬¸ 2~4ê°œë¥¼ ì œì‹œí•˜ì„¸ìš”.\n",
    "\n",
    "[ë‹µë³€ êµ¬ì¡° â€” ë°˜ë“œì‹œ ì´ ìˆœì„œ]\n",
    "A. í•œ ì¤„ ê²°ë¡ (ë‘ê´„ì‹)\n",
    "B. ì§€ê¸ˆ ë‹¹ì¥ í•  ì¼(ì²´í¬ë¦¬ìŠ¤íŠ¸ 4~7ê°œ)  â† í”¼í•´ ì˜ˆë°© ì¤‘ì‹¬ìœ¼ë¡œ ë” êµ¬ì²´ì ìœ¼ë¡œ\n",
    "C. ë²•ì  ê·¼ê±°(ê·¼ê±° ë¬¸ì„œ 1~3ê°œë¥¼ \"src_title article\"ë¡œ ì¸ìš©)\n",
    "D. ì ˆì°¨/ì¦ë¹™(ê¸°ê´€Â·ì„œë¥˜Â·ê¸°í•œÂ·ì£¼ì˜ì )  â† 'ì–¸ì œ/ì–´ë””/ë¬´ì—‡' ì¤‘ì‹¬ìœ¼ë¡œ\n",
    "E. ìœ ì‚¬ íŒë¡€/ì‚¬ë¡€(ìˆë‹¤ë©´ 1~2ê°œ + ì ìš© í•œê³„)\n",
    "F. ì¶”ê°€ í™•ì¸ ì§ˆë¬¸(2~4ê°œ)\n",
    "\n",
    "[ì•ˆì „ì¥ì¹˜/ë©´ì±…]\n",
    "- ë§ˆì§€ë§‰ ì¤„ì—: \"ì¼ë°˜ì ì¸ ë²•ë¥ ì •ë³´ì´ë©°, êµ¬ì²´ ì‚¬ê±´ ìë¬¸ì´ ì•„ë‹™ë‹ˆë‹¤.\"ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ê³ ì§€í•˜ì„¸ìš”.\n",
    "- ë¶„ìŸ ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë©´, ì¦ê±°(ê³„ì•½ì„œ/ë“±ê¸°ë¶€/ë¬¸ì/ì´ì²´ë‚´ì—­) ë³´ì „ê³¼ ê¸°í•œ ì¤€ìˆ˜ë¥¼ ìš°ì„  ê°•ì¡°í•˜ì„¸ìš”.\n",
    "\n",
    "[ê¸ˆì§€]\n",
    "- [ì°¸ê³  ë¬¸ì„œ]ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "- ê³¼ë„í•œ ì¼ë°˜ë¡ /ì¡°ê±´ ë‚˜ì—´ë¡œ ë‹µì„ íë¦¬ì§€ ë§ˆì„¸ìš”.\n",
    "\n",
    "[ì°¸ê³  ë¬¸ì„œ]\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "# Baseline prompt: use rag_module's default SYSTEM_PROMPT by passing None\n",
    "SYSTEM_PROMPT_P0 = None\n",
    "\n",
    "# Choose which prompt to compare against baseline:\n",
    "#   - SYSTEM_PROMPT_P1 (AnswerRelevancy ê°•í™”)\n",
    "#   - SYSTEM_PROMPT_P2 (Faithfulness ê°•í™”)\n",
    "PROMPT_BASELINE = SYSTEM_PROMPT_P0\n",
    "PROMPT_EXPERIMENT = SYSTEM_PROMPT_P1\n",
    "\n",
    "print(\"âœ… prompt variants ready:\", \"P1/P2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6959059d-0ad1-4770-a338-6be9623871f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8fd6fa-516a-44f7-aa82-3095b2320555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e6dfb91",
   "metadata": {},
   "source": [
    "## 4) (Optional) Quick trace sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85371f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ Skip or customize depending on your pipeline API.\n"
     ]
    }
   ],
   "source": [
    "# If your rag_module exposes a trace / debug method, call it here.\n",
    "# Otherwise you can skip this cell.\n",
    "\n",
    "# Example (adjust to your actual API):\n",
    "# ans, trace = pipe_p0.answer_with_trace(\"í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ...\")\n",
    "# display(trace)\n",
    "\n",
    "print(\"â„¹ï¸ Skip or customize depending on your pipeline API.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16739f05",
   "metadata": {},
   "source": [
    "## 5) Build RAGAS samples from your pipeline outputs\n",
    "\n",
    "This converts each testset row into the RAGAS format:\n",
    "- `question`\n",
    "- `answer`\n",
    "- `contexts` (list[str])\n",
    "- `ground_truth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a8d6173",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def shrink_contexts(ctxs, max_chars=900, max_contexts=8):\n",
    "    out, seen = [], set()\n",
    "    for c in (ctxs or []):\n",
    "        if c is None:\n",
    "            continue\n",
    "        s = str(c).strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        s = s[:max_chars]\n",
    "        if s in seen:\n",
    "            continue\n",
    "        seen.add(s)\n",
    "        out.append(s)\n",
    "        if len(out) >= max_contexts:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "def row_get_ground_truth(r: dict):\n",
    "    return r.get(\"ground_truth\") or r.get(\"reference\") or r.get(\"gt\") or r.get(\"answer\")\n",
    "\n",
    "def _call_pipe(pipe, q, *, system_prompt=None):\n",
    "    \"\"\"Call various pipeline styles (callable / .invoke / .run / .query / .ask).\n",
    "\n",
    "    âœ… PATCH: If pipe has answer_with_trace, inject system_prompt there.\n",
    "    NOTE: rag_module.RAGPipeline.answer_with_trace must accept system_prompt=... for this to work.\n",
    "    \"\"\"\n",
    "    # 1) Preferred: rag_module style\n",
    "    if hasattr(pipe, \"answer_with_trace\"):\n",
    "        return pipe.answer_with_trace(q, system_prompt=system_prompt)\n",
    "\n",
    "    # 2) Callable pipeline fallback\n",
    "    if callable(pipe):\n",
    "        return pipe(q)\n",
    "\n",
    "    # 3) Other common method names fallback (no prompt injection)\n",
    "    for m in (\"invoke\", \"run\", \"query\", \"ask\", \"call\", \"answer\"):\n",
    "        if hasattr(pipe, m):\n",
    "            return getattr(pipe, m)(q)\n",
    "\n",
    "    raise TypeError(f\"Pipeline is not callable and no known method found: {type(pipe)}\")\n",
    "\n",
    "def _unwrap_answer_contexts(out):\n",
    "    \"\"\"Normalize pipeline output to (answer:str, contexts:list[str]).\"\"\"\n",
    "    ans, ctxs = None, None\n",
    "\n",
    "    # (answer, contexts) tuple OR (answer, trace) tuple\n",
    "    if isinstance(out, tuple) and len(out) >= 2:\n",
    "        ans = out[0]\n",
    "        second = out[1]\n",
    "        # if second looks like a trace dict, try to pull contexts from it\n",
    "        if isinstance(second, dict):\n",
    "            ctxs = (\n",
    "                second.get(\"final_contexts\")\n",
    "                or second.get(\"contexts\")\n",
    "                or second.get(\"context\")\n",
    "                or second.get(\"documents\")\n",
    "                or second.get(\"docs\")\n",
    "                or second.get(\"sources\")\n",
    "            ) or []\n",
    "        else:\n",
    "            ctxs = second\n",
    "\n",
    "    # dict-like\n",
    "    elif isinstance(out, dict):\n",
    "        ans = out.get(\"answer\") or out.get(\"result\") or out.get(\"output\") or out.get(\"final_answer\")\n",
    "        ctxs = (\n",
    "            out.get(\"contexts\")\n",
    "            or out.get(\"context\")\n",
    "            or out.get(\"sources\")\n",
    "            or out.get(\"documents\")\n",
    "            or out.get(\"docs\")\n",
    "        )\n",
    "        if ctxs is None and isinstance(out.get(\"trace\"), dict):\n",
    "            t = out.get(\"trace\")\n",
    "            ctxs = (\n",
    "                t.get(\"final_contexts\")\n",
    "                or t.get(\"contexts\")\n",
    "                or t.get(\"context\")\n",
    "                or t.get(\"documents\")\n",
    "                or t.get(\"docs\")\n",
    "                or t.get(\"sources\")\n",
    "            )\n",
    "\n",
    "    # object-like\n",
    "    else:\n",
    "        # some pipelines return objects with .answer / .contexts\n",
    "        if hasattr(out, \"answer\") and ans is None:\n",
    "            ans = getattr(out, \"answer\")\n",
    "        if hasattr(out, \"contexts\") and ctxs is None:\n",
    "            ctxs = getattr(out, \"contexts\")\n",
    "        if ans is None:\n",
    "            ans = str(out)\n",
    "        if ctxs is None:\n",
    "            ctxs = []\n",
    "\n",
    "    # normalize contexts to list[str]\n",
    "    if ctxs is None:\n",
    "        ctxs = []\n",
    "    if isinstance(ctxs, (str, bytes)):\n",
    "        ctxs = [ctxs]\n",
    "\n",
    "    # if contexts are list of dicts/objects, stringify safely\n",
    "    norm_ctxs = []\n",
    "    if isinstance(ctxs, list):\n",
    "        for c in ctxs:\n",
    "            if c is None:\n",
    "                continue\n",
    "            if isinstance(c, str):\n",
    "                norm_ctxs.append(c)\n",
    "            elif isinstance(c, dict):\n",
    "                # common fields\n",
    "                norm_ctxs.append(str(c.get(\"text\") or c.get(\"content\") or c.get(\"page_content\") or c))\n",
    "            else:\n",
    "                norm_ctxs.append(str(getattr(c, \"page_content\", None) or getattr(c, \"content\", None) or c))\n",
    "    else:\n",
    "        norm_ctxs = [str(ctxs)]\n",
    "\n",
    "    return str(ans) if ans is not None else \"\", norm_ctxs\n",
    "\n",
    "def run_pipe_to_samples(pipe, rows, *, system_prompt=None, max_chars=900, max_contexts=8, limit=10):\n",
    "    \"\"\"Build RAGAS samples from pipeline outputs.\n",
    "\n",
    "    âœ… PATCH: system_prompt is forwarded into _call_pipe, which injects it into answer_with_trace.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    n = len(rows) if limit is None else min(limit, len(rows))\n",
    "    for i in range(n):\n",
    "        r = rows[i]\n",
    "        q = r.get(\"question\") or r.get(\"query\")\n",
    "        if not q:\n",
    "            continue\n",
    "\n",
    "        out = _call_pipe(pipe, q, system_prompt=system_prompt)\n",
    "        ans, ctxs = _unwrap_answer_contexts(out)\n",
    "\n",
    "        samples.append({\n",
    "            \"question_id\": i,\n",
    "            \"question\": q,\n",
    "            \"answer\": ans,\n",
    "            \"contexts\": shrink_contexts(ctxs, max_chars=max_chars, max_contexts=max_contexts),\n",
    "            \"ground_truth\": row_get_ground_truth(r),\n",
    "        })\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e27a5e-7a62-4e4f-bd4b-ef795fa07123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7501cb40-1341-452e-a96f-16e852024e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "286a3c26",
   "metadata": {},
   "source": [
    "## 6) RAGAS evaluation (prepared cell)\n",
    "\n",
    "- Creates per-sample detail dataframe (when supported by your RAGAS version)\n",
    "- Creates summary dataframe (mean over samples)\n",
    "- Keeps timing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10a15582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18412\\271458754.py:32: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  llm = LangchainLLMWrapper(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… METRICS: ['ContextPrecision', 'ContextRecall', 'Faithfulness', 'AnswerRelevancy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18412\\271458754.py:40: DeprecationWarning: Importing context_precision from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_precision\n",
      "  from ragas.metrics import context_precision, context_recall, faithfulness\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18412\\271458754.py:40: DeprecationWarning: Importing context_recall from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_recall\n",
      "  from ragas.metrics import context_precision, context_recall, faithfulness\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18412\\271458754.py:40: DeprecationWarning: Importing faithfulness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import faithfulness\n",
      "  from ragas.metrics import context_precision, context_recall, faithfulness\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18412\\271458754.py:44: DeprecationWarning: Importing answer_relevancy from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import answer_relevancy\n",
      "  from ragas.metrics import answer_relevancy\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# RAGAS compare + clean saving (ragas==0.3.2 compatible)\n",
    "# FIXES:\n",
    "#  1) detail.csvì—ì„œ _trace ì œê±° (traceëŠ” trace.jsonlë¡œë§Œ)\n",
    "#  2) samplesì— run_tagë¥¼ ë¯¸ë¦¬ ì£¼ì…í•´ì„œ trace.jsonlì— íƒœê·¸ê°€ ë‚¨ë„ë¡\n",
    "#  3) ground_truths=[...] ì•ˆì „ì¥ì¹˜ ì¶”ê°€ (ë²„ì „/í™˜ê²½ í˜¸í™˜ì„±â†‘)\n",
    "#  4) samples/detail ì»¬ëŸ¼ ì¶©ëŒ ë°©ì§€(ê°€ëŠ¥í•œ í•œ ì•ˆì „í•˜ê²Œ merge)\n",
    "# ============================================================\n",
    "\n",
    "import time, json, re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.llms import llm_factory\n",
    "\n",
    "# ----------------------------\n",
    "# LLM + METRICS (ragas 0.3.2)\n",
    "# ----------------------------\n",
    "\n",
    "from openai import OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "client = OpenAI()  # OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "llm = LangchainLLMWrapper(\n",
    "    ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    ")\n",
    "\n",
    "# llm = llm_factory(\"gpt-4o-mini\", client=client)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")  # RAGAS metricsìš©\n",
    "\n",
    "def build_metrics_032():\n",
    "    from ragas.metrics import context_precision, context_recall, faithfulness\n",
    "    metrics = [context_precision, context_recall, faithfulness]\n",
    "    # answer_relevancyëŠ” í™˜ê²½ì— ë”°ë¼ ì—†ì„ ìˆ˜ ìˆì–´ optional\n",
    "    try:\n",
    "        from ragas.metrics import answer_relevancy\n",
    "        metrics.append(answer_relevancy)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    return metrics\n",
    "\n",
    "METRICS = build_metrics_032()\n",
    "print(\"âœ… METRICS:\", [getattr(m, \"__name__\", m.__class__.__name__) for m in METRICS])\n",
    "\n",
    "# ----------------------------\n",
    "# utils\n",
    "# ----------------------------\n",
    "def _json_safe(obj):\n",
    "    \"\"\"Make config/meta safe to dump to json.\"\"\"\n",
    "    try:\n",
    "        json.dumps(obj, ensure_ascii=False)\n",
    "        return obj\n",
    "    except TypeError:\n",
    "        if hasattr(obj, \"model_dump\"):\n",
    "            return obj.model_dump()\n",
    "        if hasattr(obj, \"dict\"):\n",
    "            return obj.dict()\n",
    "        if hasattr(obj, \"__dict__\"):\n",
    "            return obj.__dict__\n",
    "        return str(obj)\n",
    "\n",
    "def _write_json(path: Path, data):\n",
    "    path.write_text(json.dumps(_json_safe(data), ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def _write_jsonl(path: Path, rows):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(_json_safe(r), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def _next_run_dir(project_root: Path, prefix: str):\n",
    "    runs_root = Path(project_root) / \"results\" / \"ragas_runs\"\n",
    "    runs_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pat = re.compile(rf\"^{re.escape(prefix)}_(\\d{{4}})_\")\n",
    "    nums = []\n",
    "    for p in runs_root.iterdir():\n",
    "        if p.is_dir():\n",
    "            m = pat.match(p.name)\n",
    "            if m:\n",
    "                nums.append(int(m.group(1)))\n",
    "    next_idx = (max(nums) + 1) if nums else 1\n",
    "\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = runs_root / f\"{prefix}_{next_idx:04d}_{ts}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=False)\n",
    "    return run_dir, next_idx, ts\n",
    "\n",
    "def _strip_trace(samples):\n",
    "    \"\"\"detail.csvì—ëŠ” _traceë¥¼ ë„£ì§€ ì•Šê¸°(íŒŒì¼ í­ë°œ ë°©ì§€).\"\"\"\n",
    "    out = []\n",
    "    for s in samples:\n",
    "        if isinstance(s, dict):\n",
    "            out.append({k: v for k, v in s.items() if k != \"_trace\"})\n",
    "        else:\n",
    "            out.append(s)\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# core eval\n",
    "# ----------------------------\n",
    "def eval_ragas_with_details(samples, run_tag: str):\n",
    "    # âœ… samplesì— run_tagë¥¼ ë¯¸ë¦¬ ì£¼ì… (trace.jsonlì—ì„œ íƒœê·¸ ìœ ì§€)\n",
    "    for s in samples:\n",
    "        if isinstance(s, dict):\n",
    "            s[\"run_tag\"] = run_tag\n",
    "            # âœ… ì•ˆì „ì¥ì¹˜: ground_truthsë„ í•¨ê»˜\n",
    "            if \"ground_truths\" not in s:\n",
    "                gt = s.get(\"ground_truth\") or \"\"\n",
    "                s[\"ground_truths\"] = [gt] if isinstance(gt, str) else (gt or [])\n",
    "\n",
    "    ds = Dataset.from_list(samples)\n",
    "\n",
    "    t0 = time.time()\n",
    "    res = evaluate(\n",
    "        dataset=ds,\n",
    "        metrics=METRICS,\n",
    "        embeddings=embeddings,   # âœ… ì—¬ê¸°\n",
    "    )\n",
    "    eval_sec = time.time() - t0\n",
    "\n",
    "    t1 = time.time()\n",
    "    detail_df = res.to_pandas() if hasattr(res, \"to_pandas\") else pd.DataFrame()\n",
    "    to_pandas_sec = time.time() - t1\n",
    "\n",
    "    # âœ… detailì—ëŠ” _trace ì œì™¸\n",
    "    samples_df = pd.DataFrame(_strip_trace(samples))\n",
    "\n",
    "    # merge per-sample metrics back onto samples (ê¸¸ì´ ë™ì¼í•  ë•Œë§Œ)\n",
    "    if len(detail_df) == len(samples_df) and len(detail_df) > 0:\n",
    "        # ì¶©ëŒ ì»¬ëŸ¼ ë°©ì§€: detail_dfì˜ ì»¬ëŸ¼ì´ samples_dfì— ì´ë¯¸ ìˆìœ¼ë©´ prefix\n",
    "        overlap = set(samples_df.columns) & set(detail_df.columns)\n",
    "        if overlap:\n",
    "            detail_df = detail_df.rename(columns={c: f\"metric__{c}\" for c in overlap})\n",
    "\n",
    "        out_detail = pd.concat(\n",
    "            [samples_df.reset_index(drop=True), detail_df.reset_index(drop=True)],\n",
    "            axis=1\n",
    "        )\n",
    "    else:\n",
    "        out_detail = samples_df.copy()\n",
    "\n",
    "    out_detail[\"eval_seconds\"] = round(eval_sec, 3)\n",
    "    out_detail[\"to_pandas_seconds\"] = round(to_pandas_sec, 3)\n",
    "\n",
    "    # summary (mean of numeric metric columns if available)\n",
    "    summary = {}\n",
    "    if len(detail_df) > 0:\n",
    "        summary = detail_df.mean(numeric_only=True).to_dict()\n",
    "    elif isinstance(res, dict):\n",
    "        summary = {k: float(v) for k, v in res.items() if isinstance(v, (int, float))}\n",
    "\n",
    "    summary[\"run_tag\"] = run_tag\n",
    "    summary[\"eval_seconds\"] = round(eval_sec, 3)\n",
    "    summary[\"to_pandas_seconds\"] = round(to_pandas_sec, 3)\n",
    "\n",
    "    return res, out_detail, pd.DataFrame([summary])\n",
    "\n",
    "# ----------------------------\n",
    "# compare + save (clean)\n",
    "# ----------------------------\n",
    "def run_compare_and_save(\n",
    "    base_samples,\n",
    "    exp_samples,\n",
    "    project_root: Path,\n",
    "    prefix=\"ragas_compare\",\n",
    "    base_cfg=None,\n",
    "    exp_cfg=None,\n",
    "):\n",
    "    # --- sanity ---\n",
    "    print(f\"âœ… base_samples: {len(base_samples)} | exp_samples: {len(exp_samples)}\")\n",
    "\n",
    "    base_res, base_detail_df, base_summary_df = eval_ragas_with_details(base_samples, \"baseline\")\n",
    "    exp_res,  exp_detail_df,  exp_summary_df  = eval_ragas_with_details(exp_samples,  \"experiment\")\n",
    "\n",
    "    summary_df = pd.concat([base_summary_df, exp_summary_df], ignore_index=True)\n",
    "    detail_df  = pd.concat([base_detail_df,  exp_detail_df],  ignore_index=True)\n",
    "\n",
    "    run_dir, run_id, ts = _next_run_dir(project_root, prefix)\n",
    "\n",
    "    out_summary = run_dir / \"summary.csv\"\n",
    "    out_detail  = run_dir / \"detail.csv\"\n",
    "    out_meta    = run_dir / \"meta.json\"\n",
    "    out_config  = run_dir / \"config.json\"\n",
    "    out_base_in = run_dir / \"samples_base.jsonl\"\n",
    "    out_exp_in  = run_dir / \"samples_exp.jsonl\"\n",
    "    out_trace   = run_dir / \"trace.jsonl\"\n",
    "\n",
    "    summary_df.to_csv(out_summary, index=False, encoding=\"utf-8-sig\")\n",
    "    detail_df.to_csv(out_detail, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # config snapshot (best-effort)\n",
    "    cfg_payload = {\n",
    "        \"base_cfg\": _json_safe(base_cfg) if base_cfg is not None else None,\n",
    "        \"exp_cfg\":  _json_safe(exp_cfg)  if exp_cfg  is not None else None,\n",
    "        \"llm\": {\"model\": \"gpt-4o-mini\"},\n",
    "        \"metrics\": [getattr(m, \"__name__\", m.__class__.__name__) for m in METRICS],\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "    }\n",
    "    _write_json(out_config, cfg_payload)\n",
    "\n",
    "    # input snapshots (ì›ë³¸ ìœ ì§€: _trace í¬í•¨)\n",
    "    _write_jsonl(out_base_in, base_samples)\n",
    "    _write_jsonl(out_exp_in,  exp_samples)\n",
    "\n",
    "    # trace snapshot (best-effort) - samplesì˜ _traceë§Œ ëª¨ì•„ì„œ ì €ì¥\n",
    "    trace_rows = []\n",
    "    for s in list(base_samples) + list(exp_samples):\n",
    "        if isinstance(s, dict) and (\"_trace\" in s) and (s.get(\"_trace\") is not None):\n",
    "            trace_rows.append({\n",
    "                \"run_tag\": s.get(\"run_tag\"),\n",
    "                \"question\": s.get(\"question\"),\n",
    "                \"_trace\": s.get(\"_trace\"),\n",
    "            })\n",
    "\n",
    "    # traceê°€ ì•„ì˜ˆ ì—†ìœ¼ë©´ ìµœì†Œ ì •ë³´ë¼ë„ ë‚¨ê¹€\n",
    "    if not trace_rows:\n",
    "        cols = [c for c in [\"run_tag\", \"question\", \"eval_seconds\"] if c in detail_df.columns]\n",
    "        trace_rows = detail_df[cols].to_dict(orient=\"records\") if cols else []\n",
    "\n",
    "    _write_jsonl(out_trace, trace_rows)\n",
    "\n",
    "    meta = {\n",
    "        \"created_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"ragas_version\": \"0.3.2\",\n",
    "        \"run_id\": run_id,\n",
    "        \"timestamp\": ts,\n",
    "        \"run_dir\": str(run_dir),\n",
    "        \"prefix\": prefix,\n",
    "        \"n_base_samples\": len(base_samples),\n",
    "        \"n_exp_samples\": len(exp_samples),\n",
    "        \"saved\": {\n",
    "            \"summary\": str(out_summary),\n",
    "            \"detail\": str(out_detail),\n",
    "            \"meta\": str(out_meta),\n",
    "            \"config\": str(out_config),\n",
    "            \"samples_base\": str(out_base_in),\n",
    "            \"samples_exp\": str(out_exp_in),\n",
    "            \"trace\": str(out_trace),\n",
    "        },\n",
    "    }\n",
    "    _write_json(out_meta, meta)\n",
    "\n",
    "    print(f\"âœ… Saved to: {run_dir}\")\n",
    "    print(f\"   - summary: {out_summary.name}\")\n",
    "    print(f\"   - detail : {out_detail.name}\")\n",
    "    print(f\"   - meta   : {out_meta.name}\")\n",
    "    print(f\"   - config : {out_config.name}\")\n",
    "    print(f\"   - inputs : {out_base_in.name}, {out_exp_in.name}\")\n",
    "    print(f\"   - trace  : {out_trace.name}\")\n",
    "\n",
    "    return {\n",
    "        \"base_res\": base_res,\n",
    "        \"exp_res\": exp_res,\n",
    "        \"summary_df\": summary_df,\n",
    "        \"detail_df\": detail_df,\n",
    "        \"run_dir\": run_dir,\n",
    "        \"out_summary\": out_summary,\n",
    "        \"out_detail\": out_detail,\n",
    "        \"out_meta\": out_meta,\n",
    "        \"out_config\": out_config,\n",
    "        \"out_samples_base\": out_base_in,\n",
    "        \"out_samples_exp\": out_exp_in,\n",
    "        \"out_trace\": out_trace,\n",
    "        \"run_id\": run_id,\n",
    "    }\n",
    "\n",
    "# ============================\n",
    "# USAGE (ì˜ˆì‹œ)\n",
    "# ============================\n",
    "# result = run_compare_and_save(\n",
    "#     base_samples=BASE_SAMPLES,\n",
    "#     exp_samples=EXP_SAMPLES,\n",
    "#     project_root=PROJECT_ROOT,\n",
    "#     prefix=\"ragas_compare\",\n",
    "#     base_cfg=base_cfg,\n",
    "#     exp_cfg=exp_cfg,\n",
    "# )\n",
    "# display(result[\"summary_df\"])\n",
    "# display(result[\"detail_df\"].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee50b62",
   "metadata": {},
   "source": [
    "## 7) Run + compare + save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f39454-f368-4272-a7ed-116ed98f2ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9235d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 16:27:43,521 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-05 16:27:43,538 - rag_module - INFO - ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: ì£¼ë¯¼ë“±ë¡Â·í™•ì •ì¼ì(í™•ì •ì¼ì) í–ˆëŠ”ë°, í™•ì •ì¼ìë¶€(í™•ì •ì¼ìë¶€) ë‚´ìš©ê¹Œì§€ ì¤‘ìš”í•œê°€ìš”?\n",
      "2026-02-05 16:27:43,539 - rag_module - INFO - ğŸ” [Hybrid Retrieval] query='ì£¼ë¯¼ë“±ë¡Â·í™•ì •ì¼ì(í™•ì •ì¼ì) í–ˆëŠ”ë°, í™•ì •ì¼ìë¶€(í™•ì •ì¼ìë¶€) ë‚´ìš©ê¹Œì§€ ì¤‘ìš”í•œê°€ìš”?'\n",
      "2026-02-05 16:27:44,132 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-05 16:27:46,650 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-05 16:27:49,110 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-05 16:27:57,089 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2026-02-05 16:27:57,095 - rag_module - INFO - ğŸ“Œ Rerank selected=2 (threshold=0.2)\n",
      "2026-02-05 16:27:57,097 - rag_module - INFO - ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n",
      "2026-02-05 16:27:57,099 - rag_module - WARNING - âš ï¸ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: \"Input to ChatPromptTemplate is missing variables {'article', 'src_title', 'text'}.  Expected: ['article', 'context', 'question', 'src_title', 'text'] Received: ['context', 'question']\\nNote: if you intended {article} to be part of the string and not a variable, please escape it with double curly braces like: '{{article}}'.\\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/INVALID_PROMPT_INPUT \"\n",
      "2026-02-05 16:27:57,578 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-05 16:27:57,591 - rag_module - INFO - ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: ê³„ì•½ì¦ì„œ(ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ)ì— 1ë…„ì´ë¼ê³  ì¨ ìˆìœ¼ë©´, 1ë…„ ì§€ë‚˜ë©´ ë¬´ì¡°ê±´ ì£¼íƒì˜ì¸ë„(í‡´ê±°)í•´ì•¼ í•˜ë‚˜ìš”?\n",
      "2026-02-05 16:27:57,591 - rag_module - INFO - ğŸ” [Hybrid Retrieval] query='ê³„ì•½ì¦ì„œ(ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ)ì— 1ë…„ì´ë¼ê³  ì¨ ìˆìœ¼ë©´, 1ë…„ ì§€ë‚˜ë©´ ë¬´ì¡°ê±´ ì£¼íƒì˜ì¸ë„(í‡´ê±°)í•´ì•¼ í•˜ë‚˜ìš”?'\n",
      "2026-02-05 16:27:58,073 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-05 16:27:58,941 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-05 16:27:59,859 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-05 16:28:03,127 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2026-02-05 16:28:03,141 - rag_module - INFO - ğŸ“Œ Rerank selected=6 (threshold=0.2)\n",
      "2026-02-05 16:28:03,141 - rag_module - INFO - ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n",
      "2026-02-05 16:28:03,141 - rag_module - WARNING - âš ï¸ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: \"Input to ChatPromptTemplate is missing variables {'article', 'src_title', 'text'}.  Expected: ['article', 'context', 'question', 'src_title', 'text'] Received: ['context', 'question']\\nNote: if you intended {article} to be part of the string and not a variable, please escape it with double curly braces like: '{{article}}'.\\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/INVALID_PROMPT_INPUT \"\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# BUILD SAMPLES (run this BEFORE the compare cell)\n",
    "# ----------------------------\n",
    "# Tip: set limit=25 to run the full 25-question testset.\n",
    "\n",
    "# Baseline: rag_module default prompt (PROMPT_BASELINE=None)\n",
    "BASE_SAMPLES = run_pipe_to_samples(\n",
    "    pipe_p0,\n",
    "    rows,\n",
    "    system_prompt=PROMPT_BASELINE,\n",
    "    limit=25,\n",
    ")\n",
    "\n",
    "# Experiment: choose PROMPT_EXPERIMENT = SYSTEM_PROMPT_P1 or SYSTEM_PROMPT_P2 in the prompt cell\n",
    "EXP_SAMPLES = run_pipe_to_samples(\n",
    "    pipe_p1,\n",
    "    rows,\n",
    "    system_prompt=PROMPT_EXPERIMENT,\n",
    "    limit=25,\n",
    ")\n",
    "\n",
    "print(\"âœ… samples:\", len(BASE_SAMPLES), len(EXP_SAMPLES))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29525fd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6845ece-5d8b-47af-8aaf-0a1dc5ef87c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa7f586-c190-4912-a1c7-b08b1f1703db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0e570-c6a8-44d7-8628-8f2fb44c5755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d03785-23ae-4846-9368-4a45eaf6d3be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ab5d29-e922-4a2c-a0c0-fcb337ae36ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c800f2d",
   "metadata": {},
   "source": [
    "## âœ… Patched compare cell (Faithfulness-only + explicit llm)\n",
    "\n",
    "This cell is appended automatically. Run it after BASE_SAMPLES / EXP_SAMPLES are built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bff0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# One-cell (PATCHED-2): RAGAS compare + save (+ delta outputs + dataset fingerprint)\n",
    "# PATCH-2:\n",
    "#  - Metrics: Faithfulness + AnswerRelevancy\n",
    "#  - evaluate(..., llm=llm) explicit\n",
    "#  - embeddings kept (safe) because AnswerRelevancy may require it depending on ragas version\n",
    "# ============================================================\n",
    "\n",
    "import time, re, json, hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy   # âœ… PATCH-2\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# LLM (explicit judge)\n",
    "# ----------------------------\n",
    "llm = LangchainLLMWrapper(\n",
    "    ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    ")\n",
    "\n",
    "# âœ… keep embeddings (AnswerRelevancy í˜¸í™˜/ì•ˆì •ìš©)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# METRICS (Faithfulness + AnswerRelevancy)\n",
    "# ----------------------------\n",
    "METRICS = [faithfulness, answer_relevancy]   # âœ… PATCH-2\n",
    "print(\"âœ… METRICS:\", [\"faithfulness\", \"answer_relevancy\"])\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# JSON helpers (safe)\n",
    "# ----------------------------\n",
    "def _json_safe(obj):\n",
    "    try:\n",
    "        json.dumps(obj, ensure_ascii=False)\n",
    "        return obj\n",
    "    except TypeError:\n",
    "        if hasattr(obj, \"model_dump\"):\n",
    "            return obj.model_dump()\n",
    "        if hasattr(obj, \"dict\"):\n",
    "            return obj.dict()\n",
    "        if hasattr(obj, \"__dict__\"):\n",
    "            return obj.__dict__\n",
    "        return str(obj)\n",
    "\n",
    "def _write_json(path: Path, data):\n",
    "    path.write_text(json.dumps(_json_safe(data), ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def _write_jsonl(path: Path, rows):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(_json_safe(r), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def _df_to_jsonl_rows(df, prefer_cols):\n",
    "    cols = [c for c in prefer_cols if c in df.columns]\n",
    "    if cols:\n",
    "        df = df[cols].copy()\n",
    "    return df.to_dict(orient=\"records\"), cols\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# dataset fingerprint helpers\n",
    "# ----------------------------\n",
    "def _file_sha1(path: Path, chunk_size=1024 * 1024) -> str:\n",
    "    h = hashlib.sha1()\n",
    "    with path.open(\"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def _count_jsonl_lines(path: Path) -> int:\n",
    "    n = 0\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                n += 1\n",
    "    return n\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# core: eval + detail/summary frames\n",
    "# ----------------------------\n",
    "def eval_ragas_with_details(samples, run_tag: str):\n",
    "    ds = Dataset.from_list(samples)\n",
    "\n",
    "    t0 = time.time()\n",
    "    res = evaluate(\n",
    "        dataset=ds,\n",
    "        metrics=METRICS,\n",
    "        llm=llm,                 # explicit LLM\n",
    "        embeddings=embeddings,   # âœ… keep for AnswerRelevancy stability\n",
    "    )\n",
    "    eval_sec = time.time() - t0\n",
    "\n",
    "    detail_df = res.to_pandas() if hasattr(res, \"to_pandas\") else pd.DataFrame()\n",
    "    samples_df = pd.DataFrame(samples)\n",
    "\n",
    "    # merge\n",
    "    if len(detail_df) == len(samples_df) and len(detail_df) > 0:\n",
    "        out_detail = pd.concat(\n",
    "            [samples_df.reset_index(drop=True), detail_df.reset_index(drop=True)],\n",
    "            axis=1\n",
    "        )\n",
    "    else:\n",
    "        out_detail = samples_df.copy()\n",
    "\n",
    "    out_detail[\"run_tag\"] = run_tag\n",
    "    out_detail[\"eval_seconds\"] = round(eval_sec, 3)\n",
    "\n",
    "    # summary\n",
    "    summary = {}\n",
    "    if len(detail_df) > 0:\n",
    "        summary = detail_df.mean(numeric_only=True).to_dict()\n",
    "    elif isinstance(res, dict):\n",
    "        summary = {k: float(v) for k, v in res.items() if isinstance(v, (int, float))}\n",
    "\n",
    "    summary[\"run_tag\"] = run_tag\n",
    "    summary[\"eval_seconds\"] = round(eval_sec, 3)\n",
    "\n",
    "    return out_detail, pd.DataFrame([summary])\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# run dir allocator\n",
    "# ----------------------------\n",
    "def _next_run_dir(project_root: Path, prefix: str):\n",
    "    runs_root = Path(project_root) / \"results\" / \"ragas_runs\"\n",
    "    runs_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pat = re.compile(rf\"^{re.escape(prefix)}_(\\d{{4}})_\")\n",
    "    nums = []\n",
    "    for p in runs_root.iterdir():\n",
    "        if p.is_dir():\n",
    "            m = pat.match(p.name)\n",
    "            if m:\n",
    "                nums.append(int(m.group(1)))\n",
    "    next_id = (max(nums) + 1) if nums else 1\n",
    "\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = runs_root / f\"{prefix}_{next_id:04d}_{ts}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=False)\n",
    "    return run_dir, next_id, ts\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# delta builders (multi-metric friendly)\n",
    "# ----------------------------\n",
    "def _pick_question_col(df: pd.DataFrame) -> str:\n",
    "    for c in [\"question\", \"normalized_question\", \"normalized_query\", \"query\"]:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return \"\"\n",
    "\n",
    "def _ensure_question_id(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in [\"question_id\", \"id\", \"sample_id\"]:\n",
    "        if c in df.columns:\n",
    "            df = df.copy()\n",
    "            df[\"__qid__\"] = df[c].astype(str)\n",
    "            return df\n",
    "    qcol = _pick_question_col(df)\n",
    "    df = df.copy()\n",
    "    if qcol:\n",
    "        def _h(x: str) -> str:\n",
    "            s = (x or \"\").strip().encode(\"utf-8\")\n",
    "            return hashlib.sha1(s).hexdigest()[:12]\n",
    "        df[\"__qid__\"] = df[qcol].astype(str).map(_h)\n",
    "    else:\n",
    "        df[\"__qid__\"] = [f\"row{i:04d}\" for i in range(len(df))]\n",
    "    return df\n",
    "\n",
    "def _metric_cols(df: pd.DataFrame) -> list:\n",
    "    # âœ… PATCH-2: faithfulness + answer_relevancy ëª¨ë‘ ëŒ€ì‘\n",
    "    prefer = [\"faithfulness\", \"Faithfulness\", \"answer_relevancy\", \"AnswerRelevancy\", \"answer_relevancy_score\"]\n",
    "    cols = [c for c in prefer if c in df.columns]\n",
    "    if cols:\n",
    "        # ì¤‘ë³µ ì œê±° + ìˆœì„œ ìœ ì§€\n",
    "        seen = set()\n",
    "        out = []\n",
    "        for c in cols:\n",
    "            if c not in seen:\n",
    "                out.append(c); seen.add(c)\n",
    "        return out\n",
    "    # fallback\n",
    "    return [c for c in df.columns if any(k in c.lower() for k in [\"faithfulness\", \"relevancy\"])]\n",
    "\n",
    "def _make_delta_summary(summary_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    metric_cols = [c for c in summary_df.columns if c not in [\"run_tag\"]]\n",
    "    base = summary_df[summary_df[\"run_tag\"] == \"baseline\"].iloc[0].to_dict()\n",
    "    exp  = summary_df[summary_df[\"run_tag\"] == \"experiment\"].iloc[0].to_dict()\n",
    "\n",
    "    rows = []\n",
    "    for c in metric_cols:\n",
    "        if c == \"run_tag\":\n",
    "            continue\n",
    "        b = base.get(c)\n",
    "        e = exp.get(c)\n",
    "        if isinstance(b, (int, float)) and isinstance(e, (int, float)):\n",
    "            rows.append({\"metric\": c, \"baseline\": float(b), \"experiment\": float(e), \"delta\": float(e - b)})\n",
    "        else:\n",
    "            rows.append({\"metric\": c, \"baseline\": b, \"experiment\": e, \"delta\": None})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def _make_delta_detail(detail_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = _ensure_question_id(detail_df)\n",
    "\n",
    "    base = df[df[\"run_tag\"] == \"baseline\"].copy()\n",
    "    exp  = df[df[\"run_tag\"] == \"experiment\"].copy()\n",
    "\n",
    "    qcol = _pick_question_col(df)\n",
    "    mcols = _metric_cols(df)\n",
    "\n",
    "    info_cols = [\"__qid__\"]\n",
    "    if qcol:\n",
    "        info_cols.append(qcol)\n",
    "    for c in [\"ground_truth\", \"reference\", \"answer\", \"contexts\"]:\n",
    "        if c in df.columns:\n",
    "            info_cols.append(c)\n",
    "\n",
    "    base_small = base[info_cols + mcols].copy()\n",
    "    exp_small  = exp[info_cols + mcols].copy()\n",
    "\n",
    "    rename_base = {c: f\"{c}_base\" for c in info_cols if c != \"__qid__\"}\n",
    "    rename_exp  = {c: f\"{c}_exp\"  for c in info_cols if c != \"__qid__\"}\n",
    "    rename_base.update({c: f\"{c}_base\" for c in mcols})\n",
    "    rename_exp.update({c: f\"{c}_exp\"  for c in mcols})\n",
    "\n",
    "    base_small = base_small.rename(columns=rename_base)\n",
    "    exp_small  = exp_small.rename(columns=rename_exp)\n",
    "\n",
    "    merged = exp_small.merge(base_small, on=\"__qid__\", how=\"outer\")\n",
    "\n",
    "    for c in mcols:\n",
    "        cb = f\"{c}_base\"\n",
    "        ce = f\"{c}_exp\"\n",
    "        if cb in merged.columns and ce in merged.columns:\n",
    "            merged[f\"{c}_delta\"] = merged[ce] - merged[cb]\n",
    "\n",
    "    if qcol:\n",
    "        qe = f\"{qcol}_exp\"\n",
    "        qb = f\"{qcol}_base\"\n",
    "        if qe in merged.columns or qb in merged.columns:\n",
    "            merged[qcol] = None\n",
    "            if qe in merged.columns:\n",
    "                merged[qcol] = merged[qe]\n",
    "            if qb in merged.columns:\n",
    "                merged[qcol] = merged[qcol].fillna(merged[qb])\n",
    "\n",
    "    return merged\n",
    "\n",
    "def _make_top_changes(delta_detail_df: pd.DataFrame, top_k=10):\n",
    "    # âœ… PATCH-2: ìš°ì„ ìˆœìœ„ = answer_relevancy_delta > faithfulness_delta\n",
    "    cand = []\n",
    "    for key in [\"answer_relevancy_delta\", \"AnswerRelevancy_delta\", \"faithfulness_delta\", \"Faithfulness_delta\"]:\n",
    "        if key in delta_detail_df.columns:\n",
    "            cand.append(key)\n",
    "    sort_col = cand[0] if cand else next((c for c in delta_detail_df.columns if c.endswith(\"_delta\") and pd.api.types.is_numeric_dtype(delta_detail_df[c])), \"\")\n",
    "\n",
    "    if not sort_col:\n",
    "        return pd.DataFrame(), pd.DataFrame(), \"\"\n",
    "\n",
    "    qcol = _pick_question_col(delta_detail_df)\n",
    "    view_cols = [c for c in [\"__qid__\", qcol, sort_col] if c and c in delta_detail_df.columns]\n",
    "    base_col = sort_col.replace(\"_delta\", \"_base\")\n",
    "    exp_col  = sort_col.replace(\"_delta\", \"_exp\")\n",
    "    for c in [base_col, exp_col]:\n",
    "        if c in delta_detail_df.columns and c not in view_cols:\n",
    "            view_cols.append(c)\n",
    "\n",
    "    regress = delta_detail_df.sort_values(sort_col, ascending=True).head(top_k)[view_cols].copy()\n",
    "    improve = delta_detail_df.sort_values(sort_col, ascending=False).head(top_k)[view_cols].copy()\n",
    "    return regress, improve, sort_col\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# main: compare + save\n",
    "# ----------------------------\n",
    "def run_compare_and_save_all(\n",
    "    base_samples,\n",
    "    exp_samples,\n",
    "    project_root: Path,\n",
    "    prefix=\"ragas_compare\",\n",
    "    save_snapshots=True,\n",
    "    save_config=True,\n",
    "    save_samples_jsonl=True,\n",
    "    save_trace_jsonl=True,\n",
    "    trace_cols_priority=None,\n",
    "    save_delta_outputs=True,\n",
    "    top_k=10,\n",
    "):\n",
    "    # 1) evaluate\n",
    "    base_detail_df, base_summary_df = eval_ragas_with_details(base_samples, \"baseline\")\n",
    "    exp_detail_df,  exp_summary_df  = eval_ragas_with_details(exp_samples,  \"experiment\")\n",
    "\n",
    "    summary_df = pd.concat([base_summary_df, exp_summary_df], ignore_index=True)\n",
    "    detail_df  = pd.concat([base_detail_df,  exp_detail_df],  ignore_index=True)\n",
    "\n",
    "    # 2) run dir\n",
    "    run_dir, run_id, ts = _next_run_dir(project_root, prefix)\n",
    "\n",
    "    # 3) basic outputs\n",
    "    out_summary = run_dir / \"summary.csv\"\n",
    "    out_detail  = run_dir / \"detail.csv\"\n",
    "    out_meta    = run_dir / \"meta.json\"\n",
    "\n",
    "    summary_df.to_csv(out_summary, index=False, encoding=\"utf-8-sig\")\n",
    "    detail_df.to_csv(out_detail, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # --- dataset fingerprint (optional) ---\n",
    "    testset_info = None\n",
    "    if \"TESTSET_JSONL\" in globals():\n",
    "        p = Path(globals()[\"TESTSET_JSONL\"])\n",
    "        if p.exists():\n",
    "            testset_info = {\n",
    "                \"testset_path\": str(p),\n",
    "                \"testset_lines\": _count_jsonl_lines(p),\n",
    "                \"testset_sha1\": _file_sha1(p),\n",
    "            }\n",
    "\n",
    "    meta = {\n",
    "        \"created_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"run_id\": run_id,\n",
    "        \"timestamp\": ts,\n",
    "        \"run_dir\": str(run_dir),\n",
    "        \"prefix\": prefix,\n",
    "        \"n_base_samples\": len(base_samples),\n",
    "        \"n_exp_samples\": len(exp_samples),\n",
    "        \"metrics\": [\"faithfulness\", \"answer_relevancy\"],  # âœ… PATCH-2\n",
    "        \"llm\": {\"model\": \"gpt-4o-mini\"},\n",
    "        \"testset\": testset_info,\n",
    "    }\n",
    "    _write_json(out_meta, meta)\n",
    "\n",
    "    extra = {}\n",
    "    if save_delta_outputs:\n",
    "        delta_summary_df = _make_delta_summary(summary_df)\n",
    "        out_delta_summary = run_dir / \"delta_summary.csv\"\n",
    "        delta_summary_df.to_csv(out_delta_summary, index=False, encoding=\"utf-8-sig\")\n",
    "        extra[\"out_delta_summary\"] = str(out_delta_summary)\n",
    "\n",
    "        delta_detail_df = _make_delta_detail(detail_df)\n",
    "        out_delta_detail = run_dir / \"delta_detail.csv\"\n",
    "        delta_detail_df.to_csv(out_delta_detail, index=False, encoding=\"utf-8-sig\")\n",
    "        extra[\"out_delta_detail\"] = str(out_delta_detail)\n",
    "\n",
    "        top_regress, top_improve, sort_col = _make_top_changes(delta_detail_df, top_k=top_k)\n",
    "        out_top_regress = run_dir / \"top_regressions.csv\"\n",
    "        out_top_improve = run_dir / \"top_improvements.csv\"\n",
    "        top_regress.to_csv(out_top_regress, index=False, encoding=\"utf-8-sig\")\n",
    "        top_improve.to_csv(out_top_improve, index=False, encoding=\"utf-8-sig\")\n",
    "        extra[\"out_top_regressions\"] = str(out_top_regress)\n",
    "        extra[\"out_top_improvements\"] = str(out_top_improve)\n",
    "        extra[\"top_rank_metric\"] = sort_col\n",
    "\n",
    "    # snapshots\n",
    "    if save_snapshots:\n",
    "        if save_config:\n",
    "            cfg_payload = {\n",
    "                \"created_at\": datetime.now().isoformat(),\n",
    "                \"llm\": {\"model\": \"gpt-4o-mini\"},\n",
    "                \"metrics\": [\"faithfulness\", \"answer_relevancy\"],  # âœ… PATCH-2\n",
    "                \"base_cfg\": _json_safe(globals().get(\"base_cfg\")) if \"base_cfg\" in globals() else None,\n",
    "                \"exp_cfg\":  _json_safe(globals().get(\"exp_cfg\"))  if \"exp_cfg\"  in globals() else None,\n",
    "            }\n",
    "            out_config = run_dir / \"config.json\"\n",
    "            _write_json(out_config, cfg_payload)\n",
    "            extra[\"out_config\"] = str(out_config)\n",
    "\n",
    "        if save_samples_jsonl:\n",
    "            out_samples_base = run_dir / \"samples_base.jsonl\"\n",
    "            out_samples_exp  = run_dir / \"samples_exp.jsonl\"\n",
    "            _write_jsonl(out_samples_base, base_samples)\n",
    "            _write_jsonl(out_samples_exp,  exp_samples)\n",
    "            extra[\"out_samples_base\"] = str(out_samples_base)\n",
    "            extra[\"out_samples_exp\"]  = str(out_samples_exp)\n",
    "\n",
    "        if save_trace_jsonl:\n",
    "            if trace_cols_priority is None:\n",
    "                trace_cols_priority = [\n",
    "                    \"id\", \"sample_id\", \"__qid__\",\n",
    "                    \"question\", \"normalized_question\", \"normalized_query\", \"query\",\n",
    "                    \"answer\", \"ground_truth\", \"reference\",\n",
    "                    \"contexts\",\n",
    "                    \"_trace\",\n",
    "                    \"run_tag\",\n",
    "                    \"eval_seconds\",\n",
    "                    \"faithfulness\",\n",
    "                    \"answer_relevancy\",\n",
    "                ]\n",
    "            trace_rows, used_cols = _df_to_jsonl_rows(detail_df, trace_cols_priority)\n",
    "            out_trace = run_dir / \"trace.jsonl\"\n",
    "            _write_jsonl(out_trace, trace_rows)\n",
    "            extra[\"out_trace\"] = str(out_trace)\n",
    "            extra[\"trace_cols_used\"] = used_cols\n",
    "\n",
    "    return {\n",
    "        \"summary_df\": summary_df,\n",
    "        \"detail_df\": detail_df,\n",
    "        \"run_dir\": str(run_dir),\n",
    "        \"out_summary\": str(out_summary),\n",
    "        \"out_detail\": str(out_detail),\n",
    "        \"out_meta\": str(out_meta),\n",
    "        **extra,\n",
    "    }\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# RUN\n",
    "# ----------------------------\n",
    "result = run_compare_and_save_all(\n",
    "    base_samples=BASE_SAMPLES,\n",
    "    exp_samples=EXP_SAMPLES,\n",
    "    project_root=PROJECT_ROOT,\n",
    "    prefix=\"ragas_compare\",\n",
    "    save_snapshots=True,\n",
    "    save_config=True,\n",
    "    save_samples_jsonl=True,\n",
    "    save_trace_jsonl=True,\n",
    "    save_delta_outputs=True,\n",
    "    top_k=10,\n",
    ")\n",
    "\n",
    "print(\"âœ… run_dir:\", result[\"run_dir\"])\n",
    "print(\"âœ… saved:\", result[\"out_summary\"], result[\"out_detail\"], result[\"out_meta\"])\n",
    "if \"out_delta_summary\" in result:\n",
    "    print(\"âœ… delta saved:\", result[\"out_delta_summary\"], result.get(\"out_delta_detail\"))\n",
    "    print(\"âœ… top changes metric:\", result.get(\"top_rank_metric\"))\n",
    "    print(\"âœ… top regressions:\", result.get(\"out_top_regressions\"))\n",
    "    print(\"âœ… top improvements:\", result.get(\"out_top_improvements\"))\n",
    "if \"out_trace\" in result:\n",
    "    print(\"âœ… extra saved trace  :\", result[\"out_trace\"])\n",
    "    print(\"âœ… trace columns used :\", result.get(\"trace_cols_used\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4df0aa-9b77-4ba5-a396-9db00f52cbd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51f8d41-c0ec-4767-9653-74c3f6bb5690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23da7e42-6391-4b0a-8774-80ad34e4ebce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8cefbe-7066-47a9-8994-be47bc5bde48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e82399-e57b-4eae-a72d-5d7f75d512f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f202f6-adcf-46ce-9d0e-9bb67032f95e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee9a785-8183-4d2d-b8ad-85c1162c2dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28625cb2-df10-4c7c-aaf3-0ce1d8f1bcc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv chatbot_app)",
   "language": "python",
   "name": "chatbot-app-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
