"""
Unified RAG Module - Final Version
ì£¼íƒì„ëŒ€ì°¨ RAG ì‹œìŠ¤í…œ - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ëª¨ë“ˆ

[ìœµí•© ì¶œì²˜]
- improved_module_cg.py: ìì²´ BM25 êµ¬í˜„, ìƒì„¸ ì„¤ì •, embedding ë°±ì—”ë“œ ì„ íƒ
- improved_module_cl.py: ëª¨ë“ˆí™”ëœ í´ë˜ìŠ¤ êµ¬ì¡° (Tokenizer, BM25Scorer, ScoreFusion)
- improved_module_ge.py: BM25Retriever ì˜µì…˜, ê°„ê²°í•œ íŒŒì´í”„ë¼ì¸

[LLM ì„¤ì •]
- normalize_query: Upstage Solar-Pro2 (ChatUpstage)
- generate_answer: OpenAI GPT-4o-mini (ChatOpenAI)

[í•µì‹¬ ê¸°ëŠ¥]
1. Dense ê²€ìƒ‰: Pinecone + Solar Embedding
2. Sparse ê²€ìƒ‰: BM25 (ìì²´ êµ¬í˜„ ë˜ëŠ” rank_bm25)
3. Hybrid Fusion: RRF / Weighted / Rank-Sum
4. Rerank: Cohere rerank-multilingual-v3.0
5. 2-Stage Case Expansion
6. ë²•ì  ìœ„ê³„(Priority) ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±

[ì˜ì¡´ì„±]
pip install langchain-upstage langchain-openai langchain-pinecone cohere
pip install rank-bm25 kiwipiepy  # ì„ íƒì 
"""

from __future__ import annotations

import logging
import math
import os
import re
from abc import ABC, abstractmethod
from collections import Counter, defaultdict
from dataclasses import dataclass
from typing import (
    Any, Callable, Dict, Iterable, List, 
    Optional, Sequence, Tuple, Union
)

from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

# Embedding
from langchain_upstage import UpstageEmbeddings

# LLM backends
try:
    from langchain_upstage import ChatUpstage
    UPSTAGE_CHAT_AVAILABLE = True
except ImportError:
    ChatUpstage = None
    UPSTAGE_CHAT_AVAILABLE = False

try:
    from langchain_openai import ChatOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    ChatOpenAI = None
    OPENAI_AVAILABLE = False

# Fallback LLM (Ollama)
try:
    from langchain_ollama import ChatOllama
    OLLAMA_AVAILABLE = True
except ImportError:
    try:
        from langchain_community.chat_models import ChatOllama
        OLLAMA_AVAILABLE = True
    except ImportError:
        ChatOllama = None
        OLLAMA_AVAILABLE = False

# Vector Store
from langchain_pinecone import PineconeVectorStore

# BM25 (optional - use built-in if not available)
try:
    from rank_bm25 import BM25Okapi, BM25Plus
    RANK_BM25_AVAILABLE = True
except ImportError:
    BM25Okapi = None
    BM25Plus = None
    RANK_BM25_AVAILABLE = False

# Korean Tokenizer (optional)
try:
    from kiwipiepy import Kiwi
    KIWI_AVAILABLE = True
except ImportError:
    Kiwi = None
    KIWI_AVAILABLE = False

# Cohere Rerank (optional)
try:
    import cohere
    COHERE_AVAILABLE = True
except ImportError:
    cohere = None
    COHERE_AVAILABLE = False

# --------------------------------------------------------------------------------------
# Logging
# --------------------------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# --------------------------------------------------------------------------------------
# Constants
# --------------------------------------------------------------------------------------
INDEX_NAMES: Dict[str, str] = {
    "law": "law-index-final",
    "rule": "rule-index-final",
    "case": "case-index-final",
}

# ë²•ë¥  ìš©ì–´ ì‚¬ì „ (ì§ˆë¬¸ í‘œì¤€í™”ìš©) - í†µí•© ë²„ì „
KEYWORD_DICT: Dict[str, str] = {
    # 1. ê³„ì•½ ì£¼ì²´ ë° ëŒ€ìƒ
    "ì§‘ì£¼ì¸": "ì„ëŒ€ì¸", "ê±´ë¬¼ì£¼": "ì„ëŒ€ì¸", "ì£¼ì¸ì§‘": "ì„ëŒ€ì¸",
    "ì„ëŒ€ì—…ì": "ì„ëŒ€ì¸", "ìƒˆì£¼ì¸": "ì„ëŒ€ì¸",
    "ì„¸ì…ì": "ì„ì°¨ì¸", "ì›”ì„¸ì…ì": "ì„ì°¨ì¸", "ì„¸ë“¤ì–´ì‚¬ëŠ”ì‚¬ëŒ": "ì„ì°¨ì¸",
    "ì„ì°¨ì": "ì„ì°¨ì¸", "ì…ì£¼ì": "ì„ì°¨ì¸",
    "ë¶€ë™ì‚°": "ê³µì¸ì¤‘ê°œì‚¬", "ì¤‘ê°œì¸": "ê³µì¸ì¤‘ê°œì‚¬", "ì¤‘ê°œì†Œ": "ê³µì¸ì¤‘ê°œì‚¬",
    "ë¹Œë¼": "ì„ì°¨ì£¼íƒ", "ì•„íŒŒíŠ¸": "ì„ì°¨ì£¼íƒ", "ì˜¤í”¼ìŠ¤í…”": "ì„ì°¨ì£¼íƒ",
    "ìš°ë¦¬ì§‘": "ì„ì°¨ì£¼íƒ", "ê±°ì£¼ì§€": "ì„ì°¨ì£¼íƒ",
    "ê³„ì•½ì„œ": "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ", "ì§‘ë¬¸ì„œ": "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ", "ì¢…ì´": "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ",

    # 2. ë³´ì¦ê¸ˆ ë° ê¸ˆì „
    "ë³´ì¦ê¸ˆ": "ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ", "ì „ì„¸ê¸ˆ": "ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ", "ë³´ì¦ë³´í—˜": "ë³´ì¦ê¸ˆë°˜í™˜ë³´ì¦",
    "ëˆëª»ë°›ìŒ": "ë³´ì¦ê¸ˆë¯¸ë°˜í™˜", "ì•ˆëŒë ¤ì¤Œ": "ë³´ì¦ê¸ˆë¯¸ë°˜í™˜", "ëª»ëŒë ¤ë°›ìŒ": "ë³´ì¦ê¸ˆë¯¸ë°˜í™˜",
    "ì›”ì„¸": "ì°¨ì„", "ê´€ë¦¬ë¹„": "ê´€ë¦¬ë¹„", "ì—°ì²´": "ì°¨ì„ì—°ì²´", "ë°€ë¦¼": "ì°¨ì„ì—°ì²´",
    "ë³µë¹„": "ì¤‘ê°œë³´ìˆ˜", "ìˆ˜ìˆ˜ë£Œ": "ì¤‘ê°œë³´ìˆ˜", "ì¤‘ê°œë¹„": "ì¤‘ê°œë³´ìˆ˜",
    "ì›”ì„¸ì˜¬ë¦¬ê¸°": "ì°¨ì„ì¦ì•¡", "ì¸ìƒ": "ì¦ì•¡", "ë”ë‹¬ë¼ê³ í•¨": "ì¦ì•¡",
    "ì›”ì„¸ê¹ê¸°": "ì°¨ì„ê°ì•¡", "í• ì¸": "ê°ì•¡", "ë‚´ë¦¬ê¸°": "ê°ì•¡",
    "ëˆë¨¼ì €ë°›ê¸°": "ìš°ì„ ë³€ì œê¶Œ", "ìˆœìœ„": "ìš°ì„ ë³€ì œê¶Œ", "ì•ˆì „ì¥ì¹˜": "ëŒ€í•­ë ¥",
    "ëŒë ¤ë°›ê¸°": "ë³´ì¦ê¸ˆë°˜í™˜",

    # 3. ê¸°ê°„ ë° ì¢…ë£Œ/ê°±ì‹ 
    "ì¬ê³„ì•½": "ê³„ì•½ê°±ì‹ ", "ì—°ì¥": "ê³„ì•½ê°±ì‹ ", "ê°±ì‹ ": "ê³„ì•½ê°±ì‹ ",
    "ê°±ì‹ ì²­êµ¬": "ê³„ì•½ê°±ì‹ ìš”êµ¬ê¶Œ", "2ë…„ë”": "ê³„ì•½ê°±ì‹ ìš”êµ¬ê¶Œ", "2í”ŒëŸ¬ìŠ¤2": "ê³„ì•½ê°±ì‹ ìš”êµ¬ê¶Œ",
    "ìë™ì—°ì¥": "ë¬µì‹œì ê°±ì‹ ", "ë¬µì‹œ": "ë¬µì‹œì ê°±ì‹ ", "ì—°ë½ì—†ìŒ": "ë¬µì‹œì ê°±ì‹ ",
    "ì´ì‚¬": "ì£¼íƒì˜ì¸ë„", "ì§ë¹¼ê¸°": "ì£¼íƒì˜ì¸ë„", "í‡´ê±°": "ì£¼íƒì˜ì¸ë„",
    "ë°©ë¹¼": "ê³„ì•½í•´ì§€", "ì¤‘ë„í•´ì§€": "ê³„ì•½í•´ì§€",
    "ì£¼ì†Œì˜®ê¸°ê¸°": "ì£¼ë¯¼ë“±ë¡", "ì „ì…ì‹ ê³ ": "ì£¼ë¯¼ë“±ë¡", "ì£¼ì†Œì§€ì´ì „": "ì£¼ë¯¼ë“±ë¡",
    "ì§‘ì£¼ì¸ë°”ë€œ": "ì„ëŒ€ì¸ì§€ìœ„ìŠ¹ê³„", "ì£¼ì¸ë°”ë€œ": "ì„ëŒ€ì¸ì§€ìœ„ìŠ¹ê³„",
    "ë§¤ë§¤": "ì„ëŒ€ì¸ì§€ìœ„ìŠ¹ê³„",
    "ë‚˜ê°€ë¼ê³ í•¨": "ê³„ì•½ê°±ì‹ ê±°ì ˆ", "ì«“ê²¨ë‚¨": "ëª…ë„", "ë¹„ì›Œë‹¬ë¼": "ëª…ë„",

    # 4. ìˆ˜ë¦¬ ë° ìƒí™œí™˜ê²½
    "ì§‘ê³ ì¹˜ê¸°": "ìˆ˜ì„ ì˜ë¬´", "ìˆ˜ë¦¬": "ìˆ˜ì„ ì˜ë¬´", "ê³ ì³ì¤˜": "ìˆ˜ì„ ì˜ë¬´",
    "ì•ˆê³ ì³ì¤Œ": "ìˆ˜ì„ ì˜ë¬´ìœ„ë°˜",
    "ê³°íŒ¡ì´": "í•˜ì", "ë¬¼ìƒ˜": "ëˆ„ìˆ˜", "ë³´ì¼ëŸ¬ê³ ì¥": "í•˜ì", "íŒŒì†": "í›¼ì†",
    "ê¹¨ë—ì´ì¹˜ìš°ê¸°": "ì›ìƒíšŒë³µì˜ë¬´", "ì›ë˜ëŒ€ë¡œí•´ë†“ê¸°": "ì›ìƒíšŒë³µ",
    "ì²­ì†Œë¹„": "ì›ìƒíšŒë³µë¹„ìš©", "ì²­ì†Œ": "ì›ìƒíšŒë³µ",
    "ì¸µê°„ì†ŒìŒ": "ê³µë™ìƒí™œí‰ì˜¨", "ì˜†ì§‘ì†ŒìŒ": "ë°©ìŒ", "ê°œí‚¤ìš°ê¸°": "ë°˜ë ¤ë™ë¬¼íŠ¹ì•½",
    "ë‹´ë°°": "í¡ì—°ê¸ˆì§€íŠ¹ì•½",

    # 5. ê¶Œë¦¬/ëŒ€í•­ë ¥/í™•ì •ì¼ì
    "í™•ì •ì¼ì": "í™•ì •ì¼ì", "ì „ì…": "ì£¼ë¯¼ë“±ë¡", "ëŒ€í•­ë ¥": "ëŒ€í•­ë ¥",
    "ìš°ì„ ë³€ì œ": "ìš°ì„ ë³€ì œê¶Œ", "ìµœìš°ì„ ": "ìµœìš°ì„ ë³€ì œê¶Œ",
    "ê²½ë§¤": "ê²½ë§¤ì ˆì°¨", "ê³µë§¤": "ê³µë§¤ì ˆì°¨",
    "ë“±ê¸°": "ë“±ê¸°ë¶€ë“±ë³¸", "ë“±ë³¸": "ë“±ê¸°ë¶€ë“±ë³¸",
    "ê·¼ì €ë‹¹": "ê·¼ì €ë‹¹ê¶Œ", "ê°€ì••ë¥˜": "ê°€ì••ë¥˜", "ê°€ì²˜ë¶„": "ê°€ì²˜ë¶„",
    "ê¹¡í†µì „ì„¸": "ì „ì„¸í”¼í•´", "ì‚¬ê¸°": "ì „ì„¸ì‚¬ê¸°", "ê²½ë§¤ë„˜ì–´ê°": "ê¶Œë¦¬ë¦¬ìŠ¤í¬",

    # 6. ë¶„ìŸ í•´ê²°
    "ë‚´ìš©ì¦ëª…": "ë‚´ìš©ì¦ëª…", "ì†Œì†¡": "ì†Œì†¡", "ë¯¼ì‚¬": "ë¯¼ì‚¬ì†Œì†¡",
    "ì¡°ì •ìœ„": "ì£¼íƒì„ëŒ€ì°¨ë¶„ìŸì¡°ì •ìœ„ì›íšŒ", "ì†Œì†¡ë§ê³ ": "ë¶„ìŸì¡°ì •",
    "ë²•ì›ê°€ê¸°ì‹«ìŒ": "ë¶„ìŸì¡°ì •",
    "ì§‘ì£¼ì¸ì‚¬ë§": "ì„ì°¨ê¶ŒìŠ¹ê³„", "ìì‹ìƒì†": "ì„ì°¨ê¶ŒìŠ¹ê³„",
    "íŠ¹ì•½": "íŠ¹ì•½ì‚¬í•­", "ë¶ˆê³µì •": "ê°•í–‰ê·œì •ìœ„ë°˜", "ë…ì†Œì¡°í•­": "ë¶ˆë¦¬í•œì•½ì •",
    "íš¨ë ¥ìˆë‚˜": "ë¬´íš¨ì—¬ë¶€",
}

# --------------------------------------------------------------------------------------
# Prompts
# --------------------------------------------------------------------------------------
NORMALIZATION_PROMPT: str = """ë‹¹ì‹ ì€ ë²•ë¥  AI ì±—ë´‡ì˜ ì „ì²˜ë¦¬ ë‹´ë‹¹ìì…ë‹ˆë‹¤.
ì•„ë˜ [ìš©ì–´ ì‚¬ì „]ì„ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ 'ë²•ë¥  í‘œì¤€ì–´'ë¡œ ë³€í™˜í•´ ì£¼ì„¸ìš”.

[ìˆ˜í–‰ ì§€ì¹¨]
1. ì‚¬ì „ì— ìˆëŠ” ë‹¨ì–´ëŠ” ë°˜ë“œì‹œ ë§¤í•‘ëœ ë²•ë¥  ìš©ì–´ë¡œ ë³€ê²½í•˜ì„¸ìš”.
2. ë‹¨ì–´ë¥¼ ë³€ê²½í•  ë•Œ ë¬¸ë§¥ì— ë§ê²Œ ì¡°ì‚¬(ì´/ê°€, ì„/ë¥¼ ë“±)ë‚˜ ì„œìˆ ì–´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ìˆ˜ì •í•˜ì„¸ìš”.
3. ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ì™œê³¡í•˜ê±°ë‚˜ ì¶”ê°€ì ì¸ ë‹µë³€, ë³„ë„ì˜ ì„¤ëª…ì„ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.
4. ë³€ê²½ ì „ ë‹¨ì–´ ë’¤ì— ë³€ê²½ëœ ë‹¨ì–´ë¥¼ ê´„í˜¸ë¡œ ë§ë¶™ì—¬ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ex. "ì§‘ì£¼ì¸(ì„ëŒ€ì¸)ì´..."

[ìš©ì–´ ì‚¬ì „]
{dictionary}

ì‚¬ìš©ì ì§ˆë¬¸: {question}
ë³€ê²½ëœ ì§ˆë¬¸:"""

SYSTEM_PROMPT: str = """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ 'ì£¼íƒ ì „ì›”ì„¸ ì‚¬ê¸° ì˜ˆë°© ë° ì„ëŒ€ì°¨ ë²•ë¥  ì „ë¬¸ê°€ AI'ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ [ë²•ì  ìœ„ê³„ê°€ ì •ë¦¬ëœ ì°¸ê³  ë¬¸ì„œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

[ë‹µë³€ ìƒì„± ì›ì¹™]
1. **ë²•ì  ìœ„ê³„ ì¤€ìˆ˜**:
   - ë°˜ë“œì‹œ [SECTION 1: í•µì‹¬ ë²•ë ¹]ì˜ ë‚´ìš©ì„ ìµœìš°ì„  íŒë‹¨ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ìœ¼ì„¸ìš”.
   - [SECTION 1]ì˜ ë‚´ìš©ì´ ëª¨í˜¸í•  ë•Œë§Œ [SECTION 2]ì™€ [SECTION 3]ë¥¼ ë³´ì¶© ê·¼ê±°ë¡œ í™œìš©í•˜ì„¸ìš”.
   - ë§Œì•½ [SECTION 3: íŒë¡€]ê°€ [SECTION 1: ë²•ë ¹]ê³¼ ë‹¤ë¥´ê²Œ í•´ì„ë˜ëŠ” íŠ¹ìˆ˜í•œ ê²½ìš°ë¼ë©´,
     "ì›ì¹™ì€ ë²•ë ¹ì— ë”°ë¥´ë‚˜, íŒë¡€ëŠ” ì˜ˆì™¸ì ìœ¼ë¡œ..."ë¼ê³  ì„¤ëª…í•˜ì„¸ìš”.

2. **ë‹µë³€ êµ¬ì¡°**:
   - **í•µì‹¬ ê²°ë¡ **: ì§ˆë¬¸ì— ëŒ€í•œ ê²°ë¡ (ê°€ëŠ¥/ë¶ˆê°€ëŠ¥/ìœ íš¨/ë¬´íš¨)ì„ ë‘ê´„ì‹ìœ¼ë¡œ ìš”ì•½.
   - **ë²•ì  ê·¼ê±°**: "ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì œOì¡°ì— ë”°ë¥´ë©´..." (SECTION 1 ì¸ìš©)
   - **ì‹¤ë¬´ ì ˆì°¨**: í•„ìš”ì‹œ ì‹ ê³  ë°©ë²•, ì„œë¥˜ ë“± ì•ˆë‚´ (SECTION 2 ì¸ìš©)
   - **ì°¸ê³  ì‚¬ë¡€**: ìœ ì‚¬í•œ ìƒí™©ì—ì„œì˜ íŒê²°ì´ë‚˜ í•´ì„ (SECTION 3 ì¸ìš©)

3. **ì£¼ì˜ì‚¬í•­**:
   - ì‚¬ìš©ìì˜ ê³„ì•½ì„œ ë‚´ìš©ì´ ë²•ë ¹(ê°•í–‰ê·œì •)ì— ìœ„ë°˜ë˜ë©´ "íš¨ë ¥ì´ ì—†ë‹¤(ë¬´íš¨)"ê³  ëª…í™•íˆ ê²½ê³ í•˜ì„¸ìš”.
   - ë²•ë¥ ì  ì¡°ì–¸ì¼ ë¿ì´ë¯€ë¡œ, ìµœì¢…ì ìœ¼ë¡œëŠ” ë³€í˜¸ì‚¬ ë“±ì˜ ì „ë¬¸ê°€ í™•ì¸ì´ í•„ìš”í•¨ì„ ê³ ì§€í•˜ì„¸ìš”.

[ë²•ì  ìœ„ê³„ê°€ ì •ë¦¬ëœ ì°¸ê³  ë¬¸ì„œ]
{context}"""

# --------------------------------------------------------------------------------------
# Utility Functions
# --------------------------------------------------------------------------------------
def _safe_int(x: object, default: int = 99) -> int:
    try:
        return int(x)
    except Exception:
        return default


def _truncate(text: str, max_chars: int) -> str:
    if not text:
        return ""
    if len(text) <= max_chars:
        return text
    return text[: max_chars - 1] + "â€¦"


def _dedupe_docs(
    docs: Iterable[Document],
    key_fields: Sequence[str] = ("chunk_id", "id"),
) -> List[Document]:
    """ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì¤‘ë³µ ì œê±°"""
    seen: set = set()
    out: List[Document] = []
    for d in docs:
        md = d.metadata or {}
        key = None
        for f in key_fields:
            v = md.get(f)
            if v:
                key = f"{f}:{v}"
                break
        if key is None:
            key = f"content:{hash(d.page_content)}"
        if key in seen:
            continue
        seen.add(key)
        out.append(d)
    return out


# --------------------------------------------------------------------------------------
# Tokenizer Classes (from improved_module_cl.py)
# --------------------------------------------------------------------------------------
_TOKEN_RE = re.compile(r"[0-9A-Za-zê°€-í£]+")


class Tokenizer(ABC):
    """í† í¬ë‚˜ì´ì € ì¶”ìƒ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def tokenize(self, text: str) -> List[str]:
        pass


class SimpleTokenizer(Tokenizer):
    """ì •ê·œì‹ ê¸°ë°˜ ë‹¨ìˆœ í† í¬ë‚˜ì´ì €"""
    
    def __init__(self, min_length: int = 1):
        self.min_length = min_length
    
    def tokenize(self, text: str) -> List[str]:
        if not text:
            return []
        tokens = _TOKEN_RE.findall(text.lower())
        return [t for t in tokens if len(t) >= self.min_length]


class KiwiTokenizer(Tokenizer):
    """Kiwi ê¸°ë°˜ í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ í† í¬ë‚˜ì´ì €"""
    
    def __init__(
        self,
        pos_tags: Optional[Tuple[str, ...]] = None,
        min_length: int = 1
    ):
        if not KIWI_AVAILABLE:
            raise ImportError("kiwipiepyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install kiwipiepy")
        
        self._kiwi = Kiwi()
        self.pos_tags = pos_tags or ('NNG', 'NNP', 'VV', 'VA', 'SL', 'SH')
        self.min_length = min_length
    
    def tokenize(self, text: str) -> List[str]:
        if not text:
            return []
        tokens = []
        for token in self._kiwi.tokenize(text):
            if token.tag in self.pos_tags and len(token.form) >= self.min_length:
                tokens.append(token.form.lower())
        return tokens


def get_default_tokenizer() -> Tokenizer:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ìµœì ì˜ í† í¬ë‚˜ì´ì € ë°˜í™˜"""
    if KIWI_AVAILABLE:
        return KiwiTokenizer()
    return SimpleTokenizer()


# --------------------------------------------------------------------------------------
# BM25 Implementation (from improved_module_cg.py - no external dependency)
# --------------------------------------------------------------------------------------
def _bm25_scores_builtin(
    query_tokens: List[str],
    docs_tokens: List[List[str]],
    *,
    k1: float = 1.5,
    b: float = 0.75,
) -> List[float]:
    """
    Built-in BM25Okapi implementation (no external dependency)
    """
    N = len(docs_tokens)
    if N == 0:
        return []
    if not query_tokens:
        return [0.0] * N

    doc_lens = [len(toks) for toks in docs_tokens]
    avgdl = sum(doc_lens) / N if N else 1.0
    if avgdl <= 0:
        avgdl = 1.0

    # Document frequency
    df: Dict[str, int] = defaultdict(int)
    for toks in docs_tokens:
        for t in set(toks):
            df[t] += 1

    # IDF
    idf: Dict[str, float] = {}
    for t, dfi in df.items():
        idf[t] = math.log(1.0 + (N - dfi + 0.5) / (dfi + 0.5))

    # Query term frequency
    qtf = Counter(query_tokens)

    scores: List[float] = []
    for toks, dl in zip(docs_tokens, doc_lens):
        tf = Counter(toks)
        score = 0.0
        norm = (1.0 - b) + b * (dl / avgdl)
        for term, qf in qtf.items():
            if term not in tf:
                continue
            f = tf[term]
            denom = f + k1 * norm
            if denom == 0:
                continue
            score += (idf.get(term, 0.0) * (f * (k1 + 1.0) / denom)) * (1.0 + 0.1 * (qf - 1))
        scores.append(float(score))
    return scores


# --------------------------------------------------------------------------------------
# BM25 Scorer Class (from improved_module_cl.py)
# --------------------------------------------------------------------------------------
class BM25Scorer:
    """BM25 ê¸°ë°˜ ë¬¸ì„œ ìŠ¤ì½”ì–´ë§"""
    
    def __init__(
        self,
        tokenizer: Optional[Tokenizer] = None,
        algorithm: str = "okapi",  # "okapi", "plus", or "builtin"
        k1: float = 1.5,
        b: float = 0.75,
    ):
        self.tokenizer = tokenizer or get_default_tokenizer()
        self.algorithm = algorithm
        self.k1 = k1
        self.b = b
        
        self._bm25: Optional[Any] = None
        self._corpus_tokens: List[List[str]] = []
        self._use_builtin = (
            algorithm == "builtin" or 
            not RANK_BM25_AVAILABLE
        )
    
    def fit(self, documents: List[Document]) -> "BM25Scorer":
        """ë¬¸ì„œ ì½”í¼ìŠ¤ë¡œ BM25 ì¸ë±ìŠ¤ êµ¬ì¶•"""
        self._corpus_tokens = [
            self.tokenizer.tokenize(doc.page_content or "")
            for doc in documents
        ]
        
        if not self._use_builtin and RANK_BM25_AVAILABLE:
            BM25Class = BM25Plus if self.algorithm == "plus" else BM25Okapi
            self._bm25 = BM25Class(self._corpus_tokens, k1=self.k1, b=self.b)
        
        return self
    
    def score(self, query: str) -> List[float]:
        """ì¿¼ë¦¬ì— ëŒ€í•œ ê° ë¬¸ì„œì˜ BM25 ì ìˆ˜ ë°˜í™˜"""
        query_tokens = self.tokenizer.tokenize(query)
        
        if self._use_builtin or self._bm25 is None:
            return _bm25_scores_builtin(
                query_tokens, 
                self._corpus_tokens,
                k1=self.k1,
                b=self.b
            )
        
        return self._bm25.get_scores(query_tokens).tolist()


# --------------------------------------------------------------------------------------
# Score Fusion (from improved_module_cl.py + improved_module_cg.py)
# --------------------------------------------------------------------------------------
class ScoreFusion:
    """Denseì™€ Sparse ì ìˆ˜ë¥¼ ê²°í•©í•˜ëŠ” ì „ëµ"""
    
    @staticmethod
    def reciprocal_rank_fusion(
        dense_ranks: Dict[str, int],
        sparse_ranks: Dict[str, int],
        k: int = 60,
        w_dense: float = 1.0,
        w_sparse: float = 1.0,
    ) -> Dict[str, float]:
        """Reciprocal Rank Fusion (RRF)"""
        all_docs = set(dense_ranks.keys()) | set(sparse_ranks.keys())
        scores: Dict[str, float] = {}
        
        for doc_id in all_docs:
            score = 0.0
            if doc_id in dense_ranks:
                score += w_dense / (k + dense_ranks[doc_id])
            if doc_id in sparse_ranks:
                score += w_sparse / (k + sparse_ranks[doc_id])
            scores[doc_id] = score
        
        return scores
    
    @staticmethod
    def weighted_sum(
        dense_scores: Dict[str, float],
        sparse_scores: Dict[str, float],
        alpha: float = 0.5,
        normalize: bool = True,
    ) -> Dict[str, float]:
        """ê°€ì¤‘ í•©ì‚°: alpha * dense + (1-alpha) * sparse"""
        if normalize:
            dense_scores = ScoreFusion._normalize(dense_scores)
            sparse_scores = ScoreFusion._normalize(sparse_scores)
        
        all_docs = set(dense_scores.keys()) | set(sparse_scores.keys())
        scores: Dict[str, float] = {}
        
        for doc_id in all_docs:
            d_score = dense_scores.get(doc_id, 0.0)
            s_score = sparse_scores.get(doc_id, 0.0)
            scores[doc_id] = alpha * d_score + (1 - alpha) * s_score
        
        return scores
    
    @staticmethod
    def rank_sum(
        dense_ranks: Dict[str, int],
        sparse_ranks: Dict[str, int],
        w_dense: float = 0.6,
        w_sparse: float = 0.4,
    ) -> Dict[str, float]:
        """Rank ê¸°ë°˜ í•©ì‚° (0~1 ì •ê·œí™” í›„ ê°€ì¤‘ í•©)"""
        all_docs = set(dense_ranks.keys()) | set(sparse_ranks.keys())
        n = len(all_docs)
        if n <= 1:
            return {doc_id: w_dense + w_sparse for doc_id in all_docs}
        
        def to_unit(r: int) -> float:
            return 1.0 - (r - 1) / (n - 1)
        
        scores: Dict[str, float] = {}
        for doc_id in all_docs:
            d_rank = dense_ranks.get(doc_id, n)
            s_rank = sparse_ranks.get(doc_id, n)
            scores[doc_id] = w_dense * to_unit(d_rank) + w_sparse * to_unit(s_rank)
        
        return scores
    
    @staticmethod
    def _normalize(scores: Dict[str, float]) -> Dict[str, float]:
        """Min-Max ì •ê·œí™”"""
        if not scores:
            return scores
        values = list(scores.values())
        min_val, max_val = min(values), max(values)
        if max_val == min_val:
            return {k: 1.0 for k in scores}
        return {k: (v - min_val) / (max_val - min_val) for k, v in scores.items()}


# --------------------------------------------------------------------------------------
# Configuration
# --------------------------------------------------------------------------------------
@dataclass
class RAGConfig:
    """RAG íŒŒì´í”„ë¼ì¸ ì„¤ì •"""
    
    # ============ LLM Settings ============
    # Normalize: Upstage Solar-Pro2
    normalize_model: str = "solar-pro2"
    normalize_temperature: float = 0.0
    
    # Generate: OpenAI GPT-4o-mini
    generation_model: str = "gpt-4o-mini"
    generation_temperature: float = 0.1
    
    # Fallback LLM (Ollama)
    fallback_model: str = "exaone3.5:2.4b"
    
    # ============ Embedding Settings ============
    embedding_model: str = "solar-embedding-1-large-passage"
    
    # ============ Retrieval Settings ============
    k_law: int = 5
    k_rule: int = 5
    k_case: int = 3
    search_multiplier: int = 2
    
    # ============ Hybrid Search Settings ============
    enable_hybrid: bool = True
    hybrid_method: str = "rrf"  # "rrf", "weighted", "rank_sum"
    hybrid_alpha: float = 0.5   # For weighted method
    rrf_k: int = 60
    hybrid_dense_weight: float = 0.6
    hybrid_sparse_weight: float = 0.4
    
    # ============ BM25 Settings ============
    bm25_algorithm: str = "builtin"  # "builtin", "okapi", "plus"
    bm25_k1: float = 1.5
    bm25_b: float = 0.75
    bm25_max_doc_chars: int = 4000
    use_kiwi_tokenizer: bool = True
    
    # ============ Rerank Settings ============
    enable_rerank: bool = True
    rerank_threshold: float = 0.2
    rerank_model: str = "rerank-multilingual-v3.0"
    rerank_max_documents: int = 80
    rerank_doc_max_chars: int = 2000
    
    # ============ Case Expansion Settings ============
    case_candidate_k: int = 40
    case_expand_top_n: Optional[int] = None
    case_context_top_k: int = 50
    
    # ============ Deduplication Settings ============
    dedupe_key_fields: Tuple[str, ...] = ("chunk_id", "id")
    
    def __post_init__(self) -> None:
        if not (0 <= self.generation_temperature <= 2):
            raise ValueError("generation_temperatureëŠ” 0~2 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        if not (0 <= self.rerank_threshold <= 1):
            raise ValueError("rerank_thresholdëŠ” 0~1 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        if self.hybrid_method not in ("rrf", "weighted", "rank_sum"):
            raise ValueError("hybrid_methodëŠ” 'rrf', 'weighted', 'rank_sum' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")


# --------------------------------------------------------------------------------------
# RAG Pipeline
# --------------------------------------------------------------------------------------
class RAGPipeline:
    """
    Unified RAG Pipeline
    
    - normalize_query: Upstage Solar-Pro2
    - generate_answer: OpenAI GPT-4o-mini
    - Hybrid Search: Dense (Solar Embedding) + Sparse (BM25)
    
    Usage:
        pipeline = RAGPipeline()
        answer = pipeline.generate_answer("ì§‘ì£¼ì¸ì´ ë³´ì¦ê¸ˆì„ ì•ˆ ëŒë ¤ì¤˜ìš”")
    """
    
    def __init__(
        self,
        config: Optional[RAGConfig] = None,
        *,
        pc_api_key: Optional[str] = None,
        upstage_api_key: Optional[str] = None,
        openai_api_key: Optional[str] = None,
        cohere_api_key: Optional[str] = None,
    ) -> None:
        self.config = config or RAGConfig()
        
        # API Keys
        self._pc_api_key = pc_api_key or os.getenv("PINECONE_API_KEY")
        self._upstage_api_key = upstage_api_key or os.getenv("UPSTAGE_API_KEY")
        self._openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        self._cohere_api_key = cohere_api_key or os.getenv("COHERE_API_KEY")
        
        if not self._pc_api_key:
            raise ValueError("PINECONE_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # Initialize components
        self._init_embedding()
        self._init_vector_stores()
        self._init_llms()
        self._init_tokenizer()
        self._init_cohere()
    
    def _init_embedding(self) -> None:
        """Embedding ì´ˆê¸°í™”"""
        if not self._upstage_api_key:
            raise ValueError("UPSTAGE_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤ (Embeddingìš©).")
        
        os.environ.setdefault("UPSTAGE_API_KEY", self._upstage_api_key)
        self._embedding = UpstageEmbeddings(model=self.config.embedding_model)
        logger.info("âœ… Upstage Embedding ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _init_vector_stores(self) -> None:
        """Vector Store ì´ˆê¸°í™”"""
        logger.info("ğŸ”— Pinecone 3ì¤‘ ì¸ë±ìŠ¤ ì—°ê²° ì¤‘...")
        
        self._law_store = PineconeVectorStore(
            index_name=INDEX_NAMES["law"],
            embedding=self._embedding,
            pinecone_api_key=self._pc_api_key,
        )
        self._rule_store = PineconeVectorStore(
            index_name=INDEX_NAMES["rule"],
            embedding=self._embedding,
            pinecone_api_key=self._pc_api_key,
        )
        self._case_store = PineconeVectorStore(
            index_name=INDEX_NAMES["case"],
            embedding=self._embedding,
            pinecone_api_key=self._pc_api_key,
        )
        
        logger.info("âœ… [Law / Rule / Case] 3ê°œ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ!")
    
    def _init_llms(self) -> None:
        """LLM ì´ˆê¸°í™” - Solar-Pro2 (normalize) + GPT-4o-mini (generate)"""
        cfg = self.config
        
        # 1. Normalize LLM: Upstage Solar-Pro2
        if UPSTAGE_CHAT_AVAILABLE and self._upstage_api_key:
            os.environ.setdefault("UPSTAGE_API_KEY", self._upstage_api_key)
            self._normalize_llm = ChatUpstage(
                model=cfg.normalize_model,
                temperature=cfg.normalize_temperature,
            )
            logger.info(f"âœ… Normalize LLM: Upstage {cfg.normalize_model}")
        elif OLLAMA_AVAILABLE:
            logger.warning(f"âš ï¸ Upstage ì‚¬ìš© ë¶ˆê°€. Fallback: {cfg.fallback_model}")
            self._normalize_llm = ChatOllama(
                model=cfg.fallback_model,
                temperature=cfg.normalize_temperature,
            )
        else:
            raise ImportError("LLM ë°±ì—”ë“œê°€ ì—†ìŠµë‹ˆë‹¤. langchain-upstage ë˜ëŠ” langchain-ollamaë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.")
        
        # 2. Generation LLM: OpenAI GPT-4o-mini
        if OPENAI_AVAILABLE and self._openai_api_key:
            os.environ.setdefault("OPENAI_API_KEY", self._openai_api_key)
            self._generation_llm = ChatOpenAI(
                model=cfg.generation_model,
                temperature=cfg.generation_temperature,
            )
            logger.info(f"âœ… Generation LLM: OpenAI {cfg.generation_model}")
        elif OLLAMA_AVAILABLE:
            logger.warning(f"âš ï¸ OpenAI ì‚¬ìš© ë¶ˆê°€. Fallback: {cfg.fallback_model}")
            self._generation_llm = ChatOllama(
                model=cfg.fallback_model,
                temperature=cfg.generation_temperature,
            )
        else:
            raise ImportError("LLM ë°±ì—”ë“œê°€ ì—†ìŠµë‹ˆë‹¤. langchain-openai ë˜ëŠ” langchain-ollamaë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.")
    
    def _init_tokenizer(self) -> None:
        """BM25ìš© í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”"""
        self._tokenizer: Optional[Tokenizer] = None
        
        if self.config.enable_hybrid:
            if self.config.use_kiwi_tokenizer and KIWI_AVAILABLE:
                self._tokenizer = KiwiTokenizer()
                logger.info("âœ… Kiwi í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                self._tokenizer = SimpleTokenizer()
                logger.info("â„¹ï¸ SimpleTokenizer ì‚¬ìš© (ê³µë°± ê¸°ë°˜)")
    
    def _init_cohere(self) -> None:
        """Cohere Rerank í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self._cohere_client: Optional[Any] = None
        
        if self.config.enable_rerank:
            if COHERE_AVAILABLE and self._cohere_api_key:
                self._cohere_client = cohere.Client(self._cohere_api_key)
                logger.info("âœ… Cohere Reranking í™œì„±í™”")
            else:
                logger.warning("âš ï¸ Cohere ì‚¬ìš© ë¶ˆê°€. Rerank ë¹„í™œì„±í™”ë¨.")
    
    # ----------------------------
    # Properties
    # ----------------------------
    @property
    def law_store(self) -> PineconeVectorStore:
        return self._law_store
    
    @property
    def rule_store(self) -> PineconeVectorStore:
        return self._rule_store
    
    @property
    def case_store(self) -> PineconeVectorStore:
        return self._case_store
    
    # ----------------------------
    # Core Methods
    # ----------------------------
    def normalize_query(self, user_query: str) -> str:
        """ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²•ë¥  ìš©ì–´ë¡œ í‘œì¤€í™” (Solar-Pro2 ì‚¬ìš©)"""
        prompt = ChatPromptTemplate.from_template(NORMALIZATION_PROMPT)
        chain = prompt | self._normalize_llm | StrOutputParser()
        
        try:
            normalized = chain.invoke({
                "dictionary": KEYWORD_DICT,
                "question": user_query
            })
            return str(normalized).strip()
        except Exception as e:
            logger.warning(f"âš ï¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨ (ì›ë³¸ ì‚¬ìš©): {e}")
            return user_query
    
    def get_full_case_context(self, case_no: str) -> str:
        """íŠ¹ì • ì‚¬ê±´ë²ˆí˜¸ì˜ íŒë¡€ ì „ë¬¸ì„ ê°€ì ¸ì˜´"""
        try:
            results = self.case_store.similarity_search(
                query="íŒë¡€ ì „ë¬¸ ê²€ìƒ‰",
                k=self.config.case_context_top_k,
                filter={"case_no": {"$eq": case_no}},
            )
            sorted_docs = sorted(
                results,
                key=lambda x: str(x.metadata.get("chunk_id", ""))
            )
            unique_docs = _dedupe_docs(sorted_docs, self.config.dedupe_key_fields)
            return "\n".join([d.page_content for d in unique_docs]).strip()
        except Exception as e:
            logger.warning(f"âš ï¸ íŒë¡€ ì „ë¬¸ ë¡œë”© ì‹¤íŒ¨ ({case_no}): {e}")
            return ""
    
    def _attach_source(self, docs: List[Document], source: str) -> List[Document]:
        """ê²€ìƒ‰ ì¶œì²˜ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì£¼ì…"""
        for d in docs:
            if d.metadata is None:
                d.metadata = {}
            d.metadata["__source_index"] = source
        return docs
    
    def _get_doc_id(self, doc: Document) -> str:
        """ë¬¸ì„œì˜ ê³ ìœ  ID ìƒì„±"""
        md = doc.metadata or {}
        for field in self.config.dedupe_key_fields:
            if md.get(field):
                return f"{field}:{md[field]}"
        return f"hash:{hash(doc.page_content)}"
    
    def _search_with_dense_rank(
        self, 
        store: PineconeVectorStore, 
        query: str, 
        k: int
    ) -> List[Document]:
        """Dense ê²€ìƒ‰ í›„ ìˆœìœ„ ë©”íƒ€ë°ì´í„° ì¶”ê°€"""
        try:
            pairs = store.similarity_search_with_score(query, k=k)
            docs: List[Document] = []
            for rank, (doc, score) in enumerate(pairs, start=1):
                if doc.metadata is None:
                    doc.metadata = {}
                doc.metadata["__dense_score"] = float(score)
                doc.metadata["__dense_rank"] = int(rank)
                docs.append(doc)
            return docs
        except Exception:
            docs = store.similarity_search(query, k=k)
            for rank, doc in enumerate(docs, start=1):
                if doc.metadata is None:
                    doc.metadata = {}
                doc.metadata["__dense_rank"] = int(rank)
            return docs
    
    def _hybrid_fusion(self, query: str, docs: List[Document]) -> List[Document]:
        """Dense + BM25 í•˜ì´ë¸Œë¦¬ë“œ ìœµí•©"""
        cfg = self.config
        
        if not cfg.enable_hybrid or not docs or not self._tokenizer:
            return docs
        
        docs = _dedupe_docs(docs, cfg.dedupe_key_fields)
        if len(docs) <= 1:
            return docs
        
        # Dense ranks
        dense_ranks: Dict[str, int] = {}
        dense_scores: Dict[str, float] = {}
        for i, d in enumerate(docs, start=1):
            doc_id = self._get_doc_id(d)
            dense_ranks[doc_id] = d.metadata.get("__dense_rank", i)
            dense_scores[doc_id] = 1.0 / dense_ranks[doc_id]
        
        # BM25 scores
        scorer = BM25Scorer(
            tokenizer=self._tokenizer,
            algorithm=cfg.bm25_algorithm,
            k1=cfg.bm25_k1,
            b=cfg.bm25_b,
        )
        
        # Truncate for BM25
        truncated_docs = []
        for d in docs:
            truncated_doc = Document(
                page_content=_truncate(d.page_content or "", cfg.bm25_max_doc_chars),
                metadata=d.metadata
            )
            truncated_docs.append(truncated_doc)
        
        scorer.fit(truncated_docs)
        bm25_scores_list = scorer.score(query)
        
        bm25_scores: Dict[str, float] = {}
        for d, score in zip(docs, bm25_scores_list):
            bm25_scores[self._get_doc_id(d)] = score
        
        # BM25 ranks
        sorted_bm25 = sorted(bm25_scores.items(), key=lambda x: x[1], reverse=True)
        sparse_ranks = {doc_id: rank for rank, (doc_id, _) in enumerate(sorted_bm25, start=1)}
        
        # Fusion
        if cfg.hybrid_method == "rrf":
            fused = ScoreFusion.reciprocal_rank_fusion(
                dense_ranks, sparse_ranks,
                k=cfg.rrf_k,
                w_dense=cfg.hybrid_dense_weight,
                w_sparse=cfg.hybrid_sparse_weight,
            )
        elif cfg.hybrid_method == "weighted":
            fused = ScoreFusion.weighted_sum(
                dense_scores, bm25_scores,
                alpha=cfg.hybrid_alpha,
            )
        else:  # rank_sum
            fused = ScoreFusion.rank_sum(
                dense_ranks, sparse_ranks,
                w_dense=cfg.hybrid_dense_weight,
                w_sparse=cfg.hybrid_sparse_weight,
            )
        
        # Reorder by fused score
        doc_map = {self._get_doc_id(d): d for d in docs}
        sorted_ids = sorted(fused.keys(), key=lambda x: fused[x], reverse=True)
        
        reordered = []
        for rank, doc_id in enumerate(sorted_ids, start=1):
            if doc_id in doc_map:
                d = doc_map[doc_id]
                d.metadata["__hybrid_score"] = fused[doc_id]
                d.metadata["__hybrid_rank"] = rank
                reordered.append(d)
        
        return reordered
    
    def _rerank(
        self,
        query: str,
        docs: List[Document]
    ) -> Optional[List[Tuple[int, float]]]:
        """Cohere Rerank ì‹¤í–‰"""
        if not self._cohere_client:
            return None
        
        cfg = self.config
        texts = [_truncate(d.page_content or "", cfg.rerank_doc_max_chars) for d in docs]
        
        try:
            rerank_results = self._cohere_client.rerank(
                model=cfg.rerank_model,
                query=query,
                documents=texts,
                top_n=len(texts),
            )
            return [(r.index, float(r.relevance_score)) for r in rerank_results.results]
        except Exception as e:
            logger.warning(f"âš ï¸ Rerank ì‹¤íŒ¨: {e}")
            return None
    
    def _cap_for_rerank(
        self,
        law: List[Document],
        rule: List[Document],
        case: List[Document]
    ) -> List[Document]:
        """Rerank ì…ë ¥ ë¬¸ì„œ ìˆ˜ ì œí•œ"""
        cfg = self.config
        law = _dedupe_docs(law, cfg.dedupe_key_fields)
        rule = _dedupe_docs(rule, cfg.dedupe_key_fields)
        case = _dedupe_docs(case, cfg.dedupe_key_fields)
        
        base = law + rule
        if len(base) >= cfg.rerank_max_documents:
            return base[:cfg.rerank_max_documents]
        
        remaining = cfg.rerank_max_documents - len(base)
        return base + case[:remaining]
    
    def triple_hybrid_retrieval(self, query: str) -> List[Document]:
        """
        3ì¤‘ ì¸ë±ìŠ¤ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        
        1. Dense ê²€ìƒ‰ (Pinecone + Solar Embedding)
        2. Hybrid Fusion (Dense + BM25)
        3. Rerank (Cohere)
        4. 2-Stage Case Expansion
        5. Priority ì •ë ¬
        """
        cfg = self.config
        mult = cfg.search_multiplier
        
        logger.info(f"ğŸ” [Hybrid ê²€ìƒ‰] query='{query}'")
        
        # 1) Dense Retrieval
        docs_law = self._attach_source(
            self._search_with_dense_rank(self.law_store, query, k=cfg.k_law * mult),
            "law",
        )
        docs_rule = self._attach_source(
            self._search_with_dense_rank(self.rule_store, query, k=cfg.k_rule * mult),
            "rule",
        )
        docs_case_chunks = self._attach_source(
            self._search_with_dense_rank(self.case_store, query, k=cfg.case_candidate_k),
            "case",
        )
        
        # 2) Hybrid Fusion (per index)
        if cfg.enable_hybrid:
            docs_law = self._hybrid_fusion(query, docs_law)
            docs_rule = self._hybrid_fusion(query, docs_rule)
            docs_case_chunks = self._hybrid_fusion(query, docs_case_chunks)
        
        # 3) Prepare for Rerank
        combined = self._cap_for_rerank(docs_law, docs_rule, docs_case_chunks)
        
        # 4) Rerank
        selected_docs: List[Document]
        ranked = self._rerank(query, combined) if cfg.enable_rerank else None
        
        if ranked:
            filtered = [(i, s) for (i, s) in ranked if s >= cfg.rerank_threshold]
            if not filtered:
                desired = min(cfg.k_law + cfg.k_rule + cfg.k_case, len(ranked))
                filtered = ranked[:desired]
            selected_docs = [combined[i] for (i, _) in filtered]
            logger.info(f"ğŸ“Œ Rerank ì™„ë£Œ: {len(selected_docs)}ê°œ ì„ íƒ")
        else:
            selected_docs = combined
        
        # 5) Deduplicate
        selected_docs = _dedupe_docs(selected_docs, cfg.dedupe_key_fields)
        
        # 6) Select top docs per source + Case Expansion
        law_ranked = [d for d in selected_docs if d.metadata.get("__source_index") == "law"]
        rule_ranked = [d for d in selected_docs if d.metadata.get("__source_index") == "rule"]
        case_ranked = [d for d in selected_docs if d.metadata.get("__source_index") == "case"]
        
        final_law = law_ranked[:cfg.k_law]
        final_rule = rule_ranked[:cfg.k_rule]
        
        # Case Expansion
        top_n = cfg.case_expand_top_n or cfg.k_case
        seen_case_no: set = set()
        chosen_cases: List[Document] = []
        
        for d in case_ranked:
            case_no = d.metadata.get("case_no")
            if not case_no or case_no in seen_case_no:
                continue
            seen_case_no.add(case_no)
            chosen_cases.append(d)
            if len(chosen_cases) >= top_n:
                break
        
        expanded_cases: List[Document] = []
        for d in chosen_cases:
            case_no = d.metadata.get("case_no")
            if not case_no:
                continue
            full_text = self.get_full_case_context(str(case_no))
            if not full_text:
                expanded_cases.append(d)
                continue
            
            title = d.metadata.get("title") or d.metadata.get("case_name") or str(case_no)
            md = dict(d.metadata)
            md["__expanded"] = True
            expanded_cases.append(
                Document(
                    page_content=f"[íŒë¡€ ì „ë¬¸: {title}]\n{full_text}",
                    metadata=md,
                )
            )
        
        final_case = expanded_cases[:cfg.k_case]
        
        # 7) Priority sort
        final_docs = final_law + final_rule + final_case
        final_docs = sorted(
            final_docs,
            key=lambda x: _safe_int((x.metadata or {}).get("priority", 99), 99)
        )
        
        logger.info(
            f"ğŸ“Š ìµœì¢… ê²€ìƒ‰ ê²°ê³¼: Law={len(final_law)}, "
            f"Rule={len(final_rule)}, Case={len(final_case)}"
        )
        
        return final_docs
    
    # ----------------------------
    # Context Formatting
    # ----------------------------
    @staticmethod
    def format_context_with_hierarchy(docs: List[Document]) -> str:
        """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë²•ì  ìœ„ê³„ì— ë”°ë¼ ì„¹ì…˜ë³„ë¡œ ì¬êµ¬ì„±"""
        section_1_law: List[str] = []
        section_2_rule: List[str] = []
        section_3_case: List[str] = []
        
        for doc in docs:
            md = doc.metadata or {}
            p = _safe_int(md.get("priority", 99), 99)
            src = md.get("src_title", md.get("__source_index", "ìë£Œ"))
            title = md.get("title", "")
            content = doc.page_content or ""
            
            entry = f"[{src}] {title}\n{content}".strip()
            
            if p in (1, 2, 4, 5):
                section_1_law.append(entry)
            elif p in (3, 6, 7, 8, 11):
                section_2_rule.append(entry)
            else:
                section_3_case.append(entry)
        
        parts: List[str] = []
        if section_1_law:
            parts.append(
                "## [SECTION 1: í•µì‹¬ ë²•ë ¹ (ìµœìš°ì„  ë²•ì  ê·¼ê±°)]\n"
                + "\n\n".join(section_1_law)
            )
        if section_2_rule:
            parts.append(
                "## [SECTION 2: ê´€ë ¨ ê·œì • ë° ì ˆì°¨ (ì„¸ë¶€ ê¸°ì¤€)]\n"
                + "\n\n".join(section_2_rule)
            )
        if section_3_case:
            parts.append(
                "## [SECTION 3: íŒë¡€ ë° í•´ì„ ì‚¬ë¡€ (ì ìš© ì˜ˆì‹œ)]\n"
                + "\n\n".join(section_3_case)
            )
        
        return "\n\n".join(parts).strip()
    
    # ----------------------------
    # Answer Generation
    # ----------------------------
    def generate_answer(
        self,
        user_input: str,
        *,
        skip_normalization: bool = False
    ) -> str:
        """
        ìµœì¢… ë‹µë³€ ìƒì„± (GPT-4o-mini ì‚¬ìš©)
        
        Args:
            user_input: ì‚¬ìš©ì ì§ˆë¬¸
            skip_normalization: ì§ˆë¬¸ í‘œì¤€í™” ê±´ë„ˆë›°ê¸°
        
        Returns:
            ìƒì„±ëœ ë‹µë³€
        """
        # 1) Normalize (Solar-Pro2)
        normalized_query = (
            user_input if skip_normalization
            else self.normalize_query(user_input)
        )
        if not skip_normalization:
            logger.info(f"ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: {normalized_query}")
        
        # 2) Retrieve (Hybrid)
        retrieved_docs = self.triple_hybrid_retrieval(normalized_query)
        if not retrieved_docs:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë²•ë ¹ì´ë‚˜ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # 3) Context
        hierarchical_context = self.format_context_with_hierarchy(retrieved_docs)
        
        # 4) Generate (GPT-4o-mini)
        prompt = ChatPromptTemplate.from_messages([
            ("system", SYSTEM_PROMPT),
            ("human", "{question}"),
        ])
        chain = prompt | self._generation_llm | StrOutputParser()
        
        logger.info("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘ (GPT-4o-mini)...")
        try:
            return str(chain.invoke({
                "context": hierarchical_context,
                "question": normalized_query
            })).strip()
        except Exception as e:
            logger.error(f"âš ï¸ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


# --------------------------------------------------------------------------------------
# Convenience Functions
# --------------------------------------------------------------------------------------
def create_pipeline(
    enable_hybrid: bool = True,
    hybrid_method: str = "rrf",
    enable_rerank: bool = True,
    **kwargs
) -> RAGPipeline:
    """
    íŒŒì´í”„ë¼ì¸ ìƒì„± í—¬í¼ í•¨ìˆ˜
    
    Examples:
        # ê¸°ë³¸ ì„¤ì •
        pipeline = create_pipeline()
        
        # Hybrid ë¹„í™œì„±í™”
        pipeline = create_pipeline(enable_hybrid=False)
        
        # Rerank ë¹„í™œì„±í™”
        pipeline = create_pipeline(enable_rerank=False)
    """
    config = RAGConfig(
        enable_hybrid=enable_hybrid,
        hybrid_method=hybrid_method,
        enable_rerank=enable_rerank,
        **kwargs
    )
    return RAGPipeline(config)


# --------------------------------------------------------------------------------------
# Exports
# --------------------------------------------------------------------------------------
__all__ = [
    # Config & Pipeline
    "RAGConfig",
    "RAGPipeline",
    "create_pipeline",
    # Tokenizers
    "Tokenizer",
    "SimpleTokenizer",
    "KiwiTokenizer",
    "get_default_tokenizer",
    # BM25
    "BM25Scorer",
    # Fusion
    "ScoreFusion",
    # Constants
    "INDEX_NAMES",
    "KEYWORD_DICT",
    "NORMALIZATION_PROMPT",
    "SYSTEM_PROMPT",
    # Availability Flags
    "RANK_BM25_AVAILABLE",
    "KIWI_AVAILABLE",
    "COHERE_AVAILABLE",
    "UPSTAGE_CHAT_AVAILABLE",
    "OPENAI_AVAILABLE",
    "OLLAMA_AVAILABLE",
]


# --------------------------------------------------------------------------------------
# Main (Test)
# --------------------------------------------------------------------------------------
if __name__ == "__main__":
    print("=" * 70)
    print("ğŸš€ Unified RAG Pipeline í…ŒìŠ¤íŠ¸")
    print("   - Normalize: Upstage Solar-Pro2")
    print("   - Generate: OpenAI GPT-4o-mini")
    print("=" * 70)
    
    # ì˜ì¡´ì„± ì²´í¬
    print("\nğŸ“¦ ì˜ì¡´ì„± ìƒíƒœ:")
    print(f"  - Upstage Chat: {'âœ…' if UPSTAGE_CHAT_AVAILABLE else 'âŒ'}")
    print(f"  - OpenAI: {'âœ…' if OPENAI_AVAILABLE else 'âŒ'}")
    print(f"  - Ollama (Fallback): {'âœ…' if OLLAMA_AVAILABLE else 'âŒ'}")
    print(f"  - rank_bm25: {'âœ…' if RANK_BM25_AVAILABLE else 'âŒ (builtin ì‚¬ìš©)'}")
    print(f"  - kiwipiepy: {'âœ…' if KIWI_AVAILABLE else 'âŒ (SimpleTokenizer ì‚¬ìš©)'}")
    print(f"  - cohere: {'âœ…' if COHERE_AVAILABLE else 'âŒ'}")
    
    try:
        print("\nğŸ”§ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
        pipeline = create_pipeline()
        
        test_queries = [
            "ì§‘ì£¼ì¸ì´ ë³´ì¦ê¸ˆì„ ì•ˆ ëŒë ¤ì¤˜ìš”. ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
            "ì „ì…ì‹ ê³ ëŠ” ì–¸ì œê¹Œì§€ í•´ì•¼ ëŒ€í•­ë ¥ì´ ìƒê¸°ë‚˜ìš”?",
            "ê³„ì•½ ê°±ì‹ ì„ ìš”êµ¬í–ˆëŠ”ë° ì§‘ì£¼ì¸ì´ ê±°ì ˆí–ˆì–´ìš”.",
        ]
        
        for i, query in enumerate(test_queries, 1):
            print(f"\n{'='*70}")
            print(f"ğŸ“ í…ŒìŠ¤íŠ¸ {i}: {query}")
            print("=" * 70)
            
            answer = pipeline.generate_answer(query)
            print(f"\nğŸ’¬ ë‹µë³€:\n{answer}")
        
    except Exception as e:
        logger.error(f"ğŸ”¥ ì—ëŸ¬ ë°œìƒ: {e}")
        raise
