"""
Final Unified RAG Module for Real-Estate Legal Chatbot

[ìœµí•© ë° ê°œì„  ì‚¬í•­]
1. Hybrid Search Architecture:
   - Dense: Upstage Solar Embedding (via Pinecone)
   - Sparse: BM25 (with Kiwi Morphological Analysis)
   - Fusion: RRF (Reciprocal Rank Fusion) ì•Œê³ ë¦¬ì¦˜ ì ìš©
2. Model Specification (User Request):
   - Query Normalization: Upstage Solar-Pro2
   - Answer Generation: OpenAI GPT-4o-mini
3. Advanced Context Processing:
   - 2-Stage Case Expansion: íŒë¡€ ê²€ìƒ‰ ì‹œ ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ í›„ ìƒìœ„ ê±´ë§Œ ì „ë¬¸(Full-text) ë¡œë”©
   - Hierarchical Context: ë²•ë ¹ > ê·œì¹™ > íŒë¡€ ìˆœìœ¼ë¡œ ìœ„ê³„í™”ëœ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
4. Reranking: Cohere Rerank v3 (Multilingual)

[í•„ìˆ˜ ì˜ì¡´ì„±]
pip install langchain-core langchain-community langchain-openai langchain-upstage langchain-pinecone
pip install rank_bm25 kiwipiepy pinecone-client cohere
"""

from __future__ import annotations

import logging
import os
import math
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Set, Tuple

# LangChain & Core
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Models (Upstage, OpenAI)
from langchain_upstage import UpstageEmbeddings, ChatUpstage
from langchain_openai import ChatOpenAI

# Vector Stores & Retrievers
from langchain_pinecone import PineconeVectorStore
from langchain_community.retrievers import BM25Retriever
from pinecone import Pinecone
import cohere

# Morphological Analyzer (for Korean BM25)
try:
    from kiwipiepy import Kiwi
    KIWI_AVAILABLE = True
except ImportError:
    KIWI_AVAILABLE = False

# Logging Setup
logger = logging.getLogger("RAG_Pipeline")
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")


# ==========================================
# 0. Constants & Prompts
# ==========================================

# ë²•ë¥  ìš©ì–´ ì‚¬ì „ (ì§ˆë¬¸ í‘œì¤€í™” ë³´ì¡°ìš©)
LEGAL_KEYWORD_MAP = {
    "ì§‘ì£¼ì¸": "ì„ëŒ€ì¸", "ê±´ë¬¼ì£¼": "ì„ëŒ€ì¸", "ì„¸ì…ì": "ì„ì°¨ì¸", "ì›”ì„¸ì…ì": "ì„ì°¨ì¸",
    "ë¶€ë™ì‚°": "ê³µì¸ì¤‘ê°œì‚¬", "ë³µë¹„": "ì¤‘ê°œë³´ìˆ˜", "ê³„ì•½ì„œ": "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ",
    "ì „ì„¸ê¸ˆ": "ì„ì°¨ë³´ì¦ê¸ˆ", "ë³´ì¦ê¸ˆ": "ì„ì°¨ë³´ì¦ê¸ˆ", "ì›”ì„¸": "ì°¨ì„", "ë°©ì„¸": "ì°¨ì„",
    "ì›”ì„¸ì˜¬ë¦¬ê¸°": "ì°¨ì„ì¦ì•¡", "ì¸ìƒ": "ì¦ì•¡", "ì›”ì„¸ê¹ê¸°": "ì°¨ì„ê°ì•¡", "í• ì¸": "ê°ì•¡",
    "ëˆë¨¼ì €ë°›ê¸°": "ìš°ì„ ë³€ì œê¶Œ", "ìˆœìœ„": "ìš°ì„ ë³€ì œê¶Œ", "ì•ˆì „ì¥ì¹˜": "ëŒ€í•­ë ¥",
    "ì—°ì¥í•˜ê¸°": "ê³„ì•½ê°±ì‹ ìš”êµ¬ê¶Œ", "ì¬ê³„ì•½": "ê³„ì•½ê°±ì‹ ", "ìë™ì—°ì¥": "ë¬µì‹œì ê°±ì‹ ",
    "ë°©ë¹¼": "ê³„ì•½í•´ì§€", "ë‚˜ê°€ë¼ê³ ": "ê³„ì•½ê°±ì‹ ê±°ì ˆ", "ë¹„ì›Œë‹¬ë¼": "ëª…ë„", "ì´ì‚¬": "ì£¼íƒì˜ì¸ë„",
    "ì „ì…ì‹ ê³ ": "ì£¼ë¯¼ë“±ë¡", "ì§‘ê³ ì¹˜ê¸°": "ìˆ˜ì„ ì˜ë¬´", "ë¬¼ìƒ˜": "ëˆ„ìˆ˜", "ì²­ì†Œë¹„": "ì›ìƒíšŒë³µë¹„ìš©",
    "ê¹¡í†µì „ì„¸": "ì „ì„¸í”¼í•´", "ì‚¬ê¸°": "ì „ì„¸ì‚¬ê¸°", "ì¡°ì •ìœ„": "ì£¼íƒì„ëŒ€ì°¨ë¶„ìŸì¡°ì •ìœ„ì›íšŒ"
}

# ë‹µë³€ ìƒì„± ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ 'ì£¼íƒ ì „ì›”ì„¸ ì‚¬ê¸° ì˜ˆë°© ë° ì„ëŒ€ì°¨ ë²•ë¥  ì „ë¬¸ê°€ AI'ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ [ë²•ì  ìœ„ê³„ê°€ ì •ë¦¬ëœ ì°¸ê³  ë¬¸ì„œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

[ë‹µë³€ ìƒì„± ì›ì¹™]
1. **ë²•ì  ìœ„ê³„ ì¤€ìˆ˜**: 
   - ë°˜ë“œì‹œ [SECTION 1: í•µì‹¬ ë²•ë ¹]ì˜ ë‚´ìš©ì„ ìµœìš°ì„  íŒë‹¨ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ìœ¼ì„¸ìš”.
   - [SECTION 1]ì˜ ë‚´ìš©ì´ ëª¨í˜¸í•  ë•Œë§Œ [SECTION 2]ì™€ [SECTION 3]ë¥¼ ë³´ì¶© ê·¼ê±°ë¡œ í™œìš©í•˜ì„¸ìš”.
   - ë§Œì•½ [SECTION 3: íŒë¡€]ê°€ [SECTION 1: ë²•ë ¹]ê³¼ ë‹¤ë¥´ê²Œ í•´ì„ë˜ëŠ” íŠ¹ìˆ˜í•œ ê²½ìš°ë¼ë©´, "ì›ì¹™ì€ ë²•ë ¹ì— ë”°ë¥´ë‚˜, íŒë¡€ëŠ” ì˜ˆì™¸ì ìœ¼ë¡œ..."ë¼ê³  ì„¤ëª…í•˜ì„¸ìš”.

2. **ë‹µë³€ êµ¬ì¡°**:
   - **í•µì‹¬ ê²°ë¡ **: ì§ˆë¬¸ì— ëŒ€í•œ ê²°ë¡ (ê°€ëŠ¥/ë¶ˆê°€ëŠ¥/ìœ íš¨/ë¬´íš¨)ì„ ë‘ê´„ì‹ìœ¼ë¡œ ëª…í™•íˆ ìš”ì•½í•˜ì„¸ìš”.
   - **ë²•ì  ê·¼ê±°**: "ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì œOì¡°ì— ë”°ë¥´ë©´..." (SECTION 1 ì¸ìš©)
   - **ì‹¤ë¬´ ì ˆì°¨**: í•„ìš”ì‹œ ì‹ ê³  ë°©ë²•, ì„œë¥˜, ìˆ˜ìˆ˜ë£Œ ë“± ì•ˆë‚´ (SECTION 2 ì¸ìš©)
   - **ì°¸ê³  ì‚¬ë¡€**: ìœ ì‚¬í•œ ìƒí™©ì—ì„œì˜ íŒê²°ì´ë‚˜ í•´ì„ (SECTION 3 ì¸ìš©)
   - **ì£¼ì˜ì‚¬í•­**: ê°•í–‰ê·œì • ìœ„ë°˜ ì‹œ "íš¨ë ¥ì´ ì—†ë‹¤"ê³  ê²½ê³ í•˜ê³ , ë²•ì  ë¶„ìŸ ì‹œ ì „ë¬¸ê°€(ë³€í˜¸ì‚¬ ë“±)ì˜ í™•ì¸ì´ í•„ìš”í•¨ì„ ê³ ì§€í•˜ì„¸ìš”.

[ë²•ì  ìœ„ê³„ê°€ ì •ë¦¬ëœ ì°¸ê³  ë¬¸ì„œ]
{context}
"""


# ==========================================
# 1. Configuration Class
# ==========================================

@dataclass
class RAGConfig:
    """RAG íŒŒì´í”„ë¼ì¸ ì„¤ì • ê´€ë¦¬"""
    # API Keys (Environment Variables preferred)
    pinecone_api_key: str = field(default_factory=lambda: os.getenv("PINECONE_API_KEY", ""))
    upstage_api_key: str = field(default_factory=lambda: os.getenv("UPSTAGE_API_KEY", ""))
    openai_api_key: str = field(default_factory=lambda: os.getenv("OPENAI_API_KEY", ""))
    cohere_api_key: str = field(default_factory=lambda: os.getenv("COHERE_API_KEY", ""))
    
    # Models
    embedding_model: str = "solar-embedding-1-large-passage"
    normalization_model: str = "solar-pro2"     # User Request: Upstage Solar-Pro2
    generation_model: str = "gpt-4o-mini"      # User Request: GPT-4o-mini
    generation_temperature: float = 0.1
    
    # Retrieval Settings
    top_k_dense: int = 5    # Dense ê²€ìƒ‰ ê°œìˆ˜ (ì¸ë±ìŠ¤ ë‹¹)
    top_k_sparse: int = 10  # Sparse ê²€ìƒ‰ ê°œìˆ˜ (ì „ì²´)
    rrf_k: int = 60         # RRF ìƒìˆ˜
    
    # Index Names
    index_names: Dict[str, str] = field(default_factory=lambda: {
        "law": "law-index-final",
        "rule": "rule-index-final",
        "case": "case-index-final"
    })

    def validate(self):
        if not self.pinecone_api_key: raise ValueError("PINECONE_API_KEY is missing.")
        if not self.upstage_api_key: raise ValueError("UPSTAGE_API_KEY is missing.")
        if not self.openai_api_key: raise ValueError("OPENAI_API_KEY is missing.")


# ==========================================
# 2. RAG Pipeline Class
# ==========================================

class RAGPipeline:
    def __init__(self, config: RAGConfig):
        self.config = config
        self.config.validate()
        
        self.bm25_retriever = None
        self.kiwi = Kiwi() if KIWI_AVAILABLE else None
        
        self._init_models()
        self._init_vector_stores()
        self._init_cohere()
        
    def _init_models(self):
        """LLM ë° Embedding ëª¨ë¸ ì´ˆê¸°í™”"""
        # Embedding (Upstage Solar)
        self.embedding = UpstageEmbeddings(
            model=self.config.embedding_model,
            upstage_api_key=self.config.upstage_api_key
        )
        
        # Normalization LLM (Upstage Solar-Pro2)
        self.normalization_llm = ChatUpstage(
            model=self.config.normalization_model,
            upstage_api_key=self.config.upstage_api_key,
            temperature=0
        )
        
        # Generation LLM (OpenAI GPT-4o-mini)
        self.generation_llm = ChatOpenAI(
            model=self.config.generation_model,
            openai_api_key=self.config.openai_api_key,
            temperature=self.config.generation_temperature
        )
        
    def _init_vector_stores(self):
        """Pinecone Vector Stores ì—°ê²°"""
        logger.info("ğŸ”— Pinecone ì¸ë±ìŠ¤ ì—°ê²° ì‹œë„...")
        self.pc = Pinecone(api_key=self.config.pinecone_api_key)
        self.stores = {}
        
        for key, index_name in self.config.index_names.items():
            try:
                self.stores[key] = PineconeVectorStore(
                    index_name=index_name,
                    embedding=self.embedding,
                    pinecone_api_key=self.config.pinecone_api_key
                )
            except Exception as e:
                logger.error(f"âŒ Index '{index_name}' ì—°ê²° ì‹¤íŒ¨: {e}")
                
    def _init_cohere(self):
        """Cohere Rerank í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        if self.config.cohere_api_key:
            self.cohere_client = cohere.Client(api_key=self.config.cohere_api_key)
            logger.info("âœ… Cohere Rerank í™œì„±í™”ë¨")
        else:
            self.cohere_client = None
            logger.warning("âš ï¸ Cohere API Key ì—†ìŒ. Rerank ê¸°ëŠ¥ ë¹„í™œì„±í™”.")

    # ---------------------------------------------------------
    # BM25 Logic (Sparse Retrieval)
    # ---------------------------------------------------------
    def _kiwi_tokenizer(self, text: str) -> List[str]:
        """í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ í† í¬ë‚˜ì´ì €"""
        if self.kiwi:
            return [token.form for token in self.kiwi.tokenize(text)]
        return text.split()

    def build_bm25(self, documents: List[Document]):
        """
        ì™¸ë¶€ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ë¡œì»¬ BM25 ì¸ë±ìŠ¤ ìƒì„±
        Note: ì„œë²„ ì‹œì‘ ì‹œ ì „ì²´ ë¬¸ì„œ(Law+Rule+Case)ë¥¼ ë¡œë“œí•´ì„œ í˜¸ì¶œí•´ì•¼ í•¨
        """
        if not documents:
            logger.warning("âš ï¸ BM25 ë¹Œë“œ ì‹¤íŒ¨: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŒ.")
            return

        logger.info(f"ğŸ—ï¸ BM25 ì¸ë±ìŠ¤ ë¹Œë“œ ì‹œì‘ (ë¬¸ì„œ {len(documents)}ê°œ)...")
        self.bm25_retriever = BM25Retriever.from_documents(
            documents,
            preprocess_func=self._kiwi_tokenizer if KIWI_AVAILABLE else None
        )
        self.bm25_retriever.k = self.config.top_k_sparse
        logger.info("âœ… BM25 ì¸ë±ìŠ¤ ë¹Œë“œ ì™„ë£Œ")

    # ---------------------------------------------------------
    # Helper: RRF (Reciprocal Rank Fusion)
    # ---------------------------------------------------------
    def _apply_rrf(self, dense_results: List[Document], sparse_results: List[Document]) -> List[Document]:
        """
        Dense ê²°ê³¼ì™€ Sparse ê²°ê³¼ë¥¼ RRF ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í†µí•©
        Score = 1 / (k + rank)
        """
        rrf_score_map = {}

        # 1. Dense Score ê³„ì‚°
        for rank, doc in enumerate(dense_results):
            # chunk_idë¥¼ ê³ ìœ  í‚¤ë¡œ ì‚¬ìš© (ì—†ìœ¼ë©´ content ì¼ë¶€)
            doc_id = doc.metadata.get("chunk_id", doc.page_content[:50])
            score = 1 / (self.config.rrf_k + rank + 1)
            
            if doc_id not in rrf_score_map:
                rrf_score_map[doc_id] = {"doc": doc, "score": 0.0}
            rrf_score_map[doc_id]["score"] += score

        # 2. Sparse Score ê³„ì‚° (ê°€ì‚°)
        for rank, doc in enumerate(sparse_results):
            doc_id = doc.metadata.get("chunk_id", doc.page_content[:50])
            score = 1 / (self.config.rrf_k + rank + 1)
            
            if doc_id not in rrf_score_map:
                rrf_score_map[doc_id] = {"doc": doc, "score": 0.0}
            rrf_score_map[doc_id]["score"] += score

        # 3. ì •ë ¬ ë° ë¦¬ìŠ¤íŠ¸ ë³€í™˜
        sorted_items = sorted(rrf_score_map.values(), key=lambda x: x["score"], reverse=True)
        return [item["doc"] for item in sorted_items]

    # ---------------------------------------------------------
    # Retrieval Logic
    # ---------------------------------------------------------
    def _get_full_case_context(self, case_no: str) -> str:
        """íŒë¡€ ì‚¬ê±´ë²ˆí˜¸ë¡œ ì „ë¬¸(Full Text) ì¡°íšŒ"""
        try:
            # Upstage Embedding requires non-empty query
            results = self.stores['case'].similarity_search(
                query="íŒë¡€ ì „ë¬¸ ê²€ìƒ‰", 
                k=50, 
                filter={"case_no": {"$eq": case_no}}
            )
            # chunk_id ìˆœ ì •ë ¬
            sorted_docs = sorted(results, key=lambda x: x.metadata.get('chunk_id', ''))
            
            # ì¤‘ë³µ ì œê±° ë° ë³‘í•©
            seen = set()
            unique_contents = []
            for doc in sorted_docs:
                cid = doc.metadata.get('chunk_id')
                if cid and cid not in seen:
                    unique_contents.append(doc.page_content)
                    seen.add(cid)
            
            return "\n".join(unique_contents)
        except Exception as e:
            logger.warning(f"âš ï¸ íŒë¡€ í™•ì¥ ì‹¤íŒ¨ ({case_no}): {e}")
            return ""

    def triple_hybrid_retrieval(self, query: str) -> List[Document]:
        """
        [Hybrid Retrieval Strategy]
        1. Dense Search: Law, Rule, Case ì¸ë±ìŠ¤ ë³‘ë ¬ ê²€ìƒ‰
        2. Sparse Search: BM25 ê²€ìƒ‰ (ì „ì²´ ëŒ€ìƒ)
        3. Fusion: RRF ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í†µí•©
        4. Expansion: íŒë¡€(Case)ì¸ ê²½ìš° ì „ë¬¸ í™•ì¥
        5. Rerank: Cohereë¡œ ìµœì¢… ìˆœìœ„ ì¬ì¡°ì •
        """
        logger.info(f"ğŸ” [Hybrid ê²€ìƒ‰] Query: {query}")

        # 1. Dense Search (Pinecone)
        docs_law = self.stores['law'].similarity_search(query, k=self.config.top_k_dense)
        docs_rule = self.stores['rule'].similarity_search(query, k=self.config.top_k_dense)
        docs_case = self.stores['case'].similarity_search(query, k=self.config.top_k_dense * 2)
        dense_results = docs_law + docs_rule + docs_case
        
        # 2. Sparse Search (BM25)
        sparse_results = []
        if self.bm25_retriever:
            sparse_results = self.bm25_retriever.invoke(query)
            logger.info(f"  - Dense: {len(dense_results)}ê±´, Sparse: {len(sparse_results)}ê±´")
        
        # 3. Fusion (RRF)
        fused_docs = self._apply_rrf(dense_results, sparse_results)
        
        # 4. Case Expansion (Top N í›„ë³´ì— ëŒ€í•´ ìˆ˜í–‰)
        # Rerank ë¹„ìš© ì ˆê°ì„ ìœ„í•´ ìƒìœ„ 20ê°œ ì •ë„ë§Œ í™•ì¥ ê³ ë ¤
        candidates = fused_docs[:20]
        final_candidates = []
        seen_cases = set()

        for doc in candidates:
            case_no = doc.metadata.get('case_no')
            
            # íŒë¡€ì´ê³  ì•„ì§ í™•ì¥í•˜ì§€ ì•Šì€ ê²½ìš°
            if case_no:
                if case_no not in seen_cases:
                    full_text = self._get_full_case_context(case_no)
                    if full_text:
                        # ì „ë¬¸ìœ¼ë¡œ êµì²´ (ë©”íƒ€ë°ì´í„° ìœ ì§€)
                        new_doc = Document(
                            page_content=f"[íŒë¡€ ì „ë¬¸: {doc.metadata.get('title')}]\n{full_text}",
                            metadata=doc.metadata
                        )
                        final_candidates.append(new_doc)
                        seen_cases.add(case_no)
            else:
                # ë²•ë ¹/ê·œì¹™ì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                final_candidates.append(doc)

        # 5. Rerank (Cohere)
        if self.cohere_client and final_candidates:
            try:
                docs_content = [d.page_content for d in final_candidates]
                rerank_results = self.cohere_client.rerank(
                    model="rerank-multilingual-v3.0",
                    query=query,
                    documents=docs_content,
                    top_n=len(final_candidates)
                )
                
                reranked_docs = []
                logger.info("ğŸ“Š Rerank Scores (Top 5):")
                for i, r in enumerate(rerank_results.results):
                    if r.relevance_score > 0.10:  # Threshold
                        doc = final_candidates[r.index]
                        reranked_docs.append(doc)
                        if i < 5:
                            logger.info(f"  - [{r.relevance_score:.4f}] {doc.metadata.get('title')}")
                return reranked_docs
                
            except Exception as e:
                logger.error(f"âš ï¸ Rerank Failed: {e}")
                return final_candidates

        return final_candidates

    # ---------------------------------------------------------
    # Context Processing & Generation
    # ---------------------------------------------------------
    def normalize_query(self, user_query: str) -> str:
        """
        [Model: Upstage Solar-Pro2]
        ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²•ë¥  ìš©ì–´ë¡œ í‘œì¤€í™”
        """
        prompt = ChatPromptTemplate.from_template("""
        ë‹¹ì‹ ì€ ë²•ë¥  AI ì±—ë´‡ì˜ ì „ì²˜ë¦¬ ë‹´ë‹¹ìì…ë‹ˆë‹¤.
        ì•„ë˜ [ìš©ì–´ ì‚¬ì „]ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ 'ë²•ë¥  í‘œì¤€ì–´'ë¡œ ë³€í™˜í•´ ì£¼ì„¸ìš”.
        
        [ìš©ì–´ ì‚¬ì „]
        {dictionary}
        
        [ì§€ì¹¨]
        1. ì‚¬ì „ì˜ ë‹¨ì–´ê°€ ì§ˆë¬¸ì— ìˆë‹¤ë©´ ë°˜ë“œì‹œ ë²•ë¥  ìš©ì–´ë¡œ ë³€ê²½í•˜ì„¸ìš”.
        2. ì¡°ì‚¬ë‚˜ ì„œìˆ ì–´ë¥¼ ë¬¸ë§¥ì— ë§ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ìˆ˜ì •í•˜ì„¸ìš”.
        3. ì˜¤ì§ 'ë³€ê²½ëœ ì§ˆë¬¸' í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
        
        ì‚¬ìš©ì ì§ˆë¬¸: {question}
        ë³€ê²½ëœ ì§ˆë¬¸:""")
        
        chain = prompt | self.normalization_llm | StrOutputParser()
        
        try:
            return chain.invoke({"dictionary": LEGAL_KEYWORD_MAP, "question": user_query}).strip()
        except Exception as e:
            logger.warning(f"âš ï¸ Normalization ì‹¤íŒ¨: {e}")
            return user_query

    def format_context_with_hierarchy(self, docs: List[Document]) -> str:
        """
        ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë²•ì  ìœ„ê³„(Priority)ì— ë”°ë¼ ì„¹ì…˜ë³„ë¡œ ì¬êµ¬ì„±
        Priority: 1(ìµœìƒìœ„) -> 11(ìµœí•˜ìœ„)
        """
        sorted_docs = sorted(docs, key=lambda x: int(x.metadata.get('priority', 99)))
        
        sections = {1: [], 2: [], 3: []}
        for doc in sorted_docs:
            p = int(doc.metadata.get('priority', 99))
            src = doc.metadata.get('src_title', 'ìë£Œ')
            title = doc.metadata.get('title', '')
            # ê°€ë…ì„±ì„ ìœ„í•´ ë³¸ë¬¸ ê¸¸ì´ ì¼ë¶€ ì œí•œ ê°€ëŠ¥í•˜ì§€ë§Œ, ì—¬ê¸°ì„  ì „ë¬¸ í¬í•¨
            entry = f"[{src}] {title}\n{doc.page_content}"
            
            if p in [1, 2, 4, 5]:      # ë²•ë¥ , ì‹œí–‰ë ¹
                sections[1].append(entry)
            elif p in [3, 6, 7, 8, 11]: # ê·œì¹™, ì¡°ë¡€, ì†Œì†¡ì ˆì°¨
                sections[2].append(entry)
            else:                       # íŒë¡€ (9), ê¸°íƒ€
                sections[3].append(entry)
            
        context = ""
        if sections[1]: 
            context += "## [SECTION 1: í•µì‹¬ ë²•ë ¹ (ìµœìš°ì„  ë²•ì  ê·¼ê±°)]\n" + "\n\n".join(sections[1]) + "\n\n"
        if sections[2]: 
            context += "## [SECTION 2: ê´€ë ¨ ê·œì • ë° ì ˆì°¨ (ì„¸ë¶€ ê¸°ì¤€)]\n" + "\n\n".join(sections[2]) + "\n\n"
        if sections[3]: 
            context += "## [SECTION 3: íŒë¡€ ë° í•´ì„ ì‚¬ë¡€ (ì ìš© ì˜ˆì‹œ)]\n" + "\n\n".join(sections[3]) + "\n\n"
            
        return context

    def generate_answer(self, user_input: str, *, skip_normalization: bool = False) -> str:
        """
        [Model: GPT-4o-mini]
        ìµœì¢… ë‹µë³€ ìƒì„± íŒŒì´í”„ë¼ì¸
        """
        # 1. Normalize
        normalized_query = user_input if skip_normalization else self.normalize_query(user_input)
        if not skip_normalization:
            logger.info(f"ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: {normalized_query}")

        # 2. Hybrid Retrieval (Dense + Sparse + RRF + Rerank)
        retrieved_docs = self.triple_hybrid_retrieval(normalized_query)
        
        if not retrieved_docs:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë²•ë ¹ì´ë‚˜ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # 3. Context Formatting
        hierarchical_context = self.format_context_with_hierarchy(retrieved_docs)

        # 4. Generate Answer
        prompt = ChatPromptTemplate.from_messages([
            ("system", SYSTEM_PROMPT),
            ("human", "{question}"),
        ])
        
        chain = prompt | self.generation_llm | StrOutputParser()

        logger.info("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘ (Model: GPT-4o-mini)...")
        try:
            return chain.invoke({"context": hierarchical_context, "question": normalized_query}).strip()
        except Exception as e:
            logger.error(f"âš ï¸ ë‹µë³€ ìƒì„± ì—ëŸ¬: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë„ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


# ==========================================
# Main Execution Block (Example)
# ==========================================
if __name__ == "__main__":
    # 1. í™˜ê²½ ë³€ìˆ˜ ì²´í¬
    if not os.getenv("UPSTAGE_API_KEY") or not os.getenv("OPENAI_API_KEY"):
        print("âŒ UPSTAGE_API_KEY ë˜ëŠ” OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        exit(1)

    # 2. íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    config = RAGConfig()
    pipeline = RAGPipeline(config)
    
    # 3. BM25 ì¸ë±ìŠ¤ ë¹Œë“œ (í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„°)
    # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” DBë‚˜ CSVì—ì„œ ì „ì²´ ë²•ë¥ /íŒë¡€ í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•´ì„œ ì£¼ì…í•´ì•¼ í•©ë‹ˆë‹¤.
    print("ğŸ—ï¸ BM25 í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¹Œë“œ...")
    dummy_docs = [
        Document(page_content="ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì œ3ì¡°(ëŒ€í•­ë ¥) ì„ì°¨ì¸ì´ ì£¼íƒì˜ ì¸ë„ì™€ ì£¼ë¯¼ë“±ë¡ì„ ë§ˆì¹œ ë•Œì—ëŠ”...", metadata={"chunk_id": "LAW_001", "priority": 1}),
        Document(page_content="í™•ì •ì¼ì ë¶€ì—¬ ë° ì •ë³´ì œê³µì— ê´€í•œ ê·œì¹™ ì œ2ì¡°(ìˆ˜ìˆ˜ë£Œ) ìˆ˜ìˆ˜ë£ŒëŠ” 600ì›ì´ë‹¤.", metadata={"chunk_id": "RULE_001", "priority": 3}),
        Document(page_content="[íŒë¡€] ë³´ì¦ê¸ˆ ë°˜í™˜ ì˜ë¬´ì™€ ëª©ì ë¬¼ ë°˜í™˜ ì˜ë¬´ëŠ” ë™ì‹œì´í–‰ ê´€ê³„ì— ìˆë‹¤.", metadata={"chunk_id": "CASE_001", "priority": 9, "case_no": "2023ë‹¤12345"}),
    ]
    pipeline.build_bm25(dummy_docs)
    
    # 4. ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
    test_query = "í™•ì •ì¼ì ë°›ëŠ”ë° ìˆ˜ìˆ˜ë£Œ ì–¼ë§ˆì•¼? ê·¸ë¦¬ê³  ì§‘ì£¼ì¸ ì‹¤ê±°ì£¼ í™•ì¸ì€ ì–´ë–»ê²Œ í•´?"
    print("\n" + "="*60)
    print(f"Q: {test_query}")
    print("-" * 60)
    
    answer = pipeline.generate_answer(test_query)
    
    print(f"A:\n{answer}")
    print("="*60)