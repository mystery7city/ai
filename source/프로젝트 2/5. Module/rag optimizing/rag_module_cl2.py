"""
RAG LLM Pipeline for Django Web Application (Unified Version)
ì£¼íƒì„ëŒ€ì°¨ RAG ì‹œìŠ¤í…œ - í†µí•© ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ëª¨ë“ˆ

[ìœµí•© ì¶œì²˜]
- rag_module_cl.py: íƒ€ì… íŒíŠ¸, íŒŒë¼ë¯¸í„° ìœ ì—°ì„±, Django í†µí•© ê°€ì´ë“œ
- rag_module_ge.py: í’ë¶€í•œ í‚¤ì›Œë“œ ì‚¬ì „, ê²€ìƒ‰ ë°°ìˆ˜ ì „ëµ
- rag_module_ge2.py: INDEX_NAMES ì¤‘ì•™ ê´€ë¦¬, ê¹”ë”í•œ êµ¬ì¡°

[ì£¼ìš” ê¸°ëŠ¥]
1. RAGConfig: ëª¨ë“  ì„¤ì •ì„ ì¤‘ì•™ì—ì„œ ê´€ë¦¬í•˜ëŠ” ì„¤ì • í´ë˜ìŠ¤
2. normalize_query: ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²•ë¥  ìš©ì–´ë¡œ í‘œì¤€í™”
3. triple_hybrid_retrieval: 3ì¤‘ ì¸ë±ìŠ¤ í†µí•© ê²€ìƒ‰ + Reranking
4. format_context_with_hierarchy: ë²•ì  ìœ„ê³„ì— ë”°ë¥¸ ì»¨í…ìŠ¤íŠ¸ ì¬ì •ë ¬
5. generate_final_answer: ìµœì¢… ë‹µë³€ ìƒì„±

[ì‚¬ìš© ì˜ˆì‹œ]
    from rag_module_unified import RAGPipeline, RAGConfig
    
    # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ íŒŒì´í”„ë¼ì¸ ìƒì„±
    pipeline = RAGPipeline()
    
    # ë˜ëŠ” ì»¤ìŠ¤í…€ ì„¤ì •
    config = RAGConfig(llm_model="exaone3.5:7.8b", temperature=0.2)
    pipeline = RAGPipeline(config)
    
    # ë‹µë³€ ìƒì„±
    answer = pipeline.generate_answer("ì§‘ì£¼ì¸ì´ ì›”ì„¸ë¥¼ ì˜¬ë ¤ë‹¬ë¼ê³  í•´ìš”")
"""

import os
import logging
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Tuple
from dotenv import load_dotenv

# LangChain imports
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document
from langchain_ollama import ChatOllama
from langchain_upstage import UpstageEmbeddings
from langchain_pinecone import PineconeVectorStore

# Vector DB imports
from pinecone import Pinecone

# Reranking import (Optional)
try:
    import cohere
    COHERE_AVAILABLE = True
except ImportError:
    COHERE_AVAILABLE = False

# ==========================================
# ë¡œê¹… ì„¤ì •
# ==========================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ==========================================
# 0. ì„¤ì • ë° ìƒìˆ˜ ì •ì˜
# ==========================================

# ì¸ë±ìŠ¤ ì´ë¦„ ì¤‘ì•™ ê´€ë¦¬ (from ge2.py)
INDEX_NAMES: Dict[str, str] = {
    "law": "law-index-final",    # Priority 1,2,4,5: ì£¼ì„ë²•, ë¯¼ë²• ë“± í•µì‹¬ ë²•ë¥ 
    "rule": "rule-index-final",  # Priority 3,6,7,8,11: ì‹œí–‰ê·œì¹™, ì¡°ë¡€, ì ˆì°¨
    "case": "case-index-final"   # Priority 9: íŒë¡€, ìƒë‹´ì‚¬ë¡€
}

# ì£¼íƒì„ëŒ€ì°¨ ì±—ë´‡ ì§ˆë¬¸ í‘œì¤€í™” ì‚¬ì „ (from ge.py/cl.py - í’ë¶€í•œ ë²„ì „)
KEYWORD_DICT: Dict[str, str] = {
    # 1. ê³„ì•½ ì£¼ì²´ ë° ëŒ€ìƒ
    "ì§‘ì£¼ì¸": "ì„ëŒ€ì¸", "ê±´ë¬¼ì£¼": "ì„ëŒ€ì¸", "ì£¼ì¸ì§‘": "ì„ëŒ€ì¸", 
    "ì„ëŒ€ì—…ì": "ì„ëŒ€ì¸", "ìƒˆì£¼ì¸": "ì„ëŒ€ì¸",
    "ì„¸ì…ì": "ì„ì°¨ì¸", "ì›”ì„¸ì…ì": "ì„ì°¨ì¸", "ì„¸ë“¤ì–´ì‚¬ëŠ”ì‚¬ëŒ": "ì„ì°¨ì¸", 
    "ì„ì°¨ì": "ì„ì°¨ì¸", "ì…ì£¼ì": "ì„ì°¨ì¸",
    "ë¶€ë™ì‚°": "ê³µì¸ì¤‘ê°œì‚¬", "ì¤‘ê°œì¸": "ê³µì¸ì¤‘ê°œì‚¬", "ì¤‘ê°œì†Œ": "ê³µì¸ì¤‘ê°œì‚¬",
    "ë¹Œë¼": "ì„ì°¨ì£¼íƒ", "ì•„íŒŒíŠ¸": "ì„ì°¨ì£¼íƒ", "ì˜¤í”¼ìŠ¤í…”": "ì„ì°¨ì£¼íƒ", 
    "ìš°ë¦¬ì§‘": "ì„ì°¨ì£¼íƒ", "ê±°ì£¼ì§€": "ì„ì°¨ì£¼íƒ",
    "ê³„ì•½ì„œ": "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ", "ì§‘ë¬¸ì„œ": "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ", "ì¢…ì´": "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ",

    # 2. ë³´ì¦ê¸ˆ ë° ê¸ˆì „ (ë³´ì¦ê¸ˆ_ëŒ€í•­ë ¥, ì„ëŒ€ë£Œ_ì¦ê°)
    "ì „ì„¸ê¸ˆ": "ì„ì°¨ë³´ì¦ê¸ˆ", "ë³´ì¦ê¸ˆ": "ì„ì°¨ë³´ì¦ê¸ˆ", "ë§¡ê¸´ëˆ": "ì„ì°¨ë³´ì¦ê¸ˆ", 
    "ë–¼ì¸ëˆ": "ì„ì°¨ë³´ì¦ê¸ˆ",
    "ì›”ì„¸": "ì°¨ì„", "ë°©ì„¸": "ì°¨ì„", "ë‹¤ë‹¬ì´ë‚´ëŠ”ì§€ì¶œ": "ì°¨ì„", 
    "ë ŒíŠ¸ë¹„": "ì°¨ì„", "ì„ëŒ€ë£Œ": "ì°¨ì„",
    "ë³µë¹„": "ì¤‘ê°œë³´ìˆ˜", "ìˆ˜ìˆ˜ë£Œ": "ì¤‘ê°œë³´ìˆ˜", "ì¤‘ê°œë¹„": "ì¤‘ê°œë³´ìˆ˜",
    "ì›”ì„¸ì˜¬ë¦¬ê¸°": "ì°¨ì„ì¦ì•¡", "ì¸ìƒ": "ì¦ì•¡", "ë”ë‹¬ë¼ê³ í•¨": "ì¦ì•¡", 
    "5í”„ë¡œ": "5í¼ì„¼íŠ¸ìƒí•œ",
    "ì›”ì„¸ê¹ê¸°": "ì°¨ì„ê°ì•¡", "í• ì¸": "ê°ì•¡", "ë‚´ë¦¬ê¸°": "ê°ì•¡",
    "ëˆë¨¼ì €ë°›ê¸°": "ìš°ì„ ë³€ì œê¶Œ", "ìˆœìœ„": "ìš°ì„ ë³€ì œê¶Œ", "ì•ˆì „ì¥ì¹˜": "ëŒ€í•­ë ¥", 
    "ëŒë ¤ë°›ê¸°": "ë³´ì¦ê¸ˆë°˜í™˜",
    "ë³´í—˜": "ë°˜í™˜ë³´ì¦", "í—ˆê·¸": "HUG", "ë‚˜ë¼ë³´ì¦": "ë³´ì¦ë³´í—˜",

    # 3. ê³„ì•½ ìƒíƒœ ë° ë³€í™” (ê³„ì•½ê°±ì‹ , ê³„ì•½í•´ì§€_ëª…ë„)
    "ì—°ì¥í•˜ê¸°": "ê³„ì•½ê°±ì‹ ìš”êµ¬ê¶Œ", "í•œë²ˆë”ì‚´ê¸°": "ê³„ì•½ê°±ì‹ ", 
    "2í”ŒëŸ¬ìŠ¤2": "ê³„ì•½ê°±ì‹ ìš”êµ¬ê¶Œ", "ê°±ì‹ ": "ê³„ì•½ê°±ì‹ ",
    "ì¬ê³„ì•½": "ê³„ì•½ê°±ì‹ ", "ìë™ì—°ì¥": "ë¬µì‹œì ê°±ì‹ ", "ì—°ë½ì—†ìŒ": "ë¬µì‹œì ê°±ì‹ ", 
    "ê·¸ëƒ¥ì—°ì¥": "ë¬µì‹œì ê°±ì‹ ",
    "ì´ì‚¬": "ì£¼íƒì˜ì¸ë„", "ì§ë¹¼ê¸°": "ì£¼íƒì˜ì¸ë„", "í‡´ê±°": "ì£¼íƒì˜ì¸ë„", 
    "ë°©ë¹¼": "ê³„ì•½í•´ì§€",
    "ì£¼ì†Œì˜®ê¸°ê¸°": "ì£¼ë¯¼ë“±ë¡", "ì „ì…ì‹ ê³ ": "ì£¼ë¯¼ë“±ë¡", "ì£¼ì†Œì§€ì´ì „": "ì£¼ë¯¼ë“±ë¡",
    "ì§‘ì£¼ì¸ë°”ë€œ": "ì„ëŒ€ì¸ì§€ìœ„ìŠ¹ê³„", "ì£¼ì¸ë°”ë€œ": "ì„ëŒ€ì¸ì§€ìœ„ìŠ¹ê³„", 
    "ë§¤ë§¤": "ì„ëŒ€ì¸ì§€ìœ„ìŠ¹ê³„",
    "ë‚˜ê°€ë¼ê³ í•¨": "ê³„ì•½ê°±ì‹ ê±°ì ˆ", "ì«“ê²¨ë‚¨": "ëª…ë„", "ë¹„ì›Œë‹¬ë¼": "ëª…ë„", 
    "ì¤‘ë„í•´ì§€": "ê³„ì•½í•´ì§€",

    # 4. ìˆ˜ë¦¬ ë° ìƒí™œí™˜ê²½ (ìˆ˜ì„ _ì›ìƒíšŒë³µ, ìƒí™œí™˜ê²½_íŠ¹ì•½)
    "ì§‘ê³ ì¹˜ê¸°": "ìˆ˜ì„ ì˜ë¬´", "ìˆ˜ë¦¬": "ìˆ˜ì„ ì˜ë¬´", "ê³ ì³ì¤˜": "ìˆ˜ì„ ì˜ë¬´", 
    "ì•ˆê³ ì³ì¤Œ": "ìˆ˜ì„ ì˜ë¬´ìœ„ë°˜",
    "ê³°íŒ¡ì´": "í•˜ì", "ë¬¼ìƒ˜": "ëˆ„ìˆ˜", "ë³´ì¼ëŸ¬ê³ ì¥": "í•˜ì", "íŒŒì†": "í›¼ì†",
    "ê¹¨ë—ì´ì¹˜ìš°ê¸°": "ì›ìƒíšŒë³µì˜ë¬´", "ì›ë˜ëŒ€ë¡œí•´ë†“ê¸°": "ì›ìƒíšŒë³µ", 
    "ì²­ì†Œë¹„": "ì›ìƒíšŒë³µë¹„ìš©", "ì²­ì†Œ": "ì›ìƒíšŒë³µ",
    "ì¸µê°„ì†ŒìŒ": "ê³µë™ìƒí™œí‰ì˜¨", "ì˜†ì§‘ì†ŒìŒ": "ë°©ìŒ", "ê°œí‚¤ìš°ê¸°": "ë°˜ë ¤ë™ë¬¼íŠ¹ì•½", 
    "ë‹´ë°°": "í¡ì—°ê¸ˆì§€íŠ¹ì•½",

    # 5. ë¦¬ìŠ¤í¬ ë° ë¶„ìŸ (ê¶Œë¦¬_ì •ë³´ë¦¬ìŠ¤í¬, ë¶„ìŸí•´ê²°)
    "ê¹¡í†µì „ì„¸": "ì „ì„¸í”¼í•´", "ì‚¬ê¸°": "ì „ì„¸ì‚¬ê¸°", "ê²½ë§¤ë„˜ì–´ê°": "ê¶Œë¦¬ë¦¬ìŠ¤í¬", 
    "ë¹š": "ê·¼ì €ë‹¹",
    "ì„¸ê¸ˆì•ˆëƒ„": "ì²´ë‚©", "ë‚˜ë¼ë¹š": "ì¡°ì„¸ì±„ê¶Œ", "ë¹Œë¦°ëˆ": "ê°€ì••ë¥˜", 
    "ì‹ íƒ": "ì‹ íƒë¶€ë™ì‚°",
    "íŠ¹ì•½": "íŠ¹ì•½ì‚¬í•­", "ë¶ˆê³µì •": "ê°•í–‰ê·œì •ìœ„ë°˜", "ë…ì†Œì¡°í•­": "ë¶ˆë¦¬í•œì•½ì •", 
    "íš¨ë ¥ìˆë‚˜": "ë¬´íš¨ì—¬ë¶€",
    "ì¡°ì •ìœ„": "ì£¼íƒì„ëŒ€ì°¨ë¶„ìŸì¡°ì •ìœ„ì›íšŒ", "ì†Œì†¡ë§ê³ ": "ë¶„ìŸì¡°ì •", 
    "ë²•ì›ê°€ê¸°ì‹«ìŒ": "ë¶„ìŸì¡°ì •",
    "ì§‘ì£¼ì¸ì‚¬ë§": "ì„ì°¨ê¶ŒìŠ¹ê³„", "ìì‹ìƒì†": "ì„ì°¨ê¶ŒìŠ¹ê³„"
}

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ë‹µë³€ ìƒì„±ìš©)
SYSTEM_PROMPT: str = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ 'ì£¼íƒ ì „ì›”ì„¸ ì‚¬ê¸° ì˜ˆë°© ë° ì„ëŒ€ì°¨ ë²•ë¥  ì „ë¬¸ê°€ AI'ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ [ë²•ì  ìœ„ê³„ê°€ ì •ë¦¬ëœ ì°¸ê³  ë¬¸ì„œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

[ë‹µë³€ ìƒì„± ì›ì¹™]
1. **ë²•ì  ìœ„ê³„ ì¤€ìˆ˜**: 
   - ë°˜ë“œì‹œ [SECTION 1: í•µì‹¬ ë²•ë ¹]ì˜ ë‚´ìš©ì„ ìµœìš°ì„  íŒë‹¨ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ìœ¼ì„¸ìš”.
   - [SECTION 1]ì˜ ë‚´ìš©ì´ ëª¨í˜¸í•  ë•Œë§Œ [SECTION 2]ì™€ [SECTION 3]ë¥¼ ë³´ì¶© ê·¼ê±°ë¡œ í™œìš©í•˜ì„¸ìš”.
   - ë§Œì•½ [SECTION 3: íŒë¡€]ê°€ [SECTION 1: ë²•ë ¹]ê³¼ ë‹¤ë¥´ê²Œ í•´ì„ë˜ëŠ” íŠ¹ìˆ˜í•œ ê²½ìš°ë¼ë©´, 
     "ì›ì¹™ì€ ë²•ë ¹ì— ë”°ë¥´ë‚˜, íŒë¡€ëŠ” ì˜ˆì™¸ì ìœ¼ë¡œ..."ë¼ê³  ì„¤ëª…í•˜ì„¸ìš”.

2. **ë‹µë³€ êµ¬ì¡°**:
   - **í•µì‹¬ ê²°ë¡ **: ì§ˆë¬¸ì— ëŒ€í•œ ê²°ë¡ (ê°€ëŠ¥/ë¶ˆê°€ëŠ¥/ìœ íš¨/ë¬´íš¨)ì„ ë‘ê´„ì‹ìœ¼ë¡œ ìš”ì•½.
   - **ë²•ì  ê·¼ê±°**: "ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì œOì¡°ì— ë”°ë¥´ë©´..." (SECTION 1 ì¸ìš©)
   - **ì‹¤ë¬´ ì ˆì°¨**: í•„ìš”ì‹œ ì‹ ê³  ë°©ë²•, ì„œë¥˜ ë“± ì•ˆë‚´ (SECTION 2 ì¸ìš©)
   - **ì°¸ê³  ì‚¬ë¡€**: ìœ ì‚¬í•œ ìƒí™©ì—ì„œì˜ íŒê²°ì´ë‚˜ í•´ì„ (SECTION 3 ì¸ìš©)
   
3. **ì£¼ì˜ì‚¬í•­**:
   - ì‚¬ìš©ìì˜ ê³„ì•½ì„œ ë‚´ìš©ì´ ë²•ë ¹(ê°•í–‰ê·œì •)ì— ìœ„ë°˜ë˜ë©´ "íš¨ë ¥ì´ ì—†ë‹¤(ë¬´íš¨)"ê³  ëª…í™•íˆ ê²½ê³ í•˜ì„¸ìš”.
   - ë²•ë¥ ì  ì¡°ì–¸ì¼ ë¿ì´ë¯€ë¡œ, ìµœì¢…ì ìœ¼ë¡œëŠ” ë³€í˜¸ì‚¬ ë“±ì˜ ì „ë¬¸ê°€ í™•ì¸ì´ í•„ìš”í•¨ì„ ê³ ì§€í•˜ì„¸ìš”.

[ë²•ì  ìœ„ê³„ê°€ ì •ë¦¬ëœ ì°¸ê³  ë¬¸ì„œ]
{context}
"""

# ì§ˆë¬¸ í‘œì¤€í™” í”„ë¡¬í”„íŠ¸
NORMALIZATION_PROMPT: str = """
ë‹¹ì‹ ì€ ë²•ë¥  AI ì±—ë´‡ì˜ ì „ì²˜ë¦¬ ë‹´ë‹¹ìì…ë‹ˆë‹¤. 
ì•„ë˜ [ìš©ì–´ ì‚¬ì „]ì„ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ 'ë²•ë¥  í‘œì¤€ì–´'ë¡œ ë³€í™˜í•´ ì£¼ì„¸ìš”.

[ìˆ˜í–‰ ì§€ì¹¨]
1. ì‚¬ì „ì— ìˆëŠ” ë‹¨ì–´ëŠ” ë°˜ë“œì‹œ ë§¤í•‘ëœ ë²•ë¥  ìš©ì–´ë¡œ ë³€ê²½í•˜ì„¸ìš”.
2. ë‹¨ì–´ë¥¼ ë³€ê²½í•  ë•Œ ë¬¸ë§¥ì— ë§ê²Œ ì¡°ì‚¬(ì´/ê°€, ì„/ë¥¼ ë“±)ë‚˜ ì„œìˆ ì–´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ìˆ˜ì •í•˜ì„¸ìš”.
3. ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ì™œê³¡í•˜ê±°ë‚˜ ì¶”ê°€ì ì¸ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.
4. ì˜¤ì§ 'ë³€ê²½ëœ ì§ˆë¬¸' í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”. (ì„¤ëª… ê¸ˆì§€)

[ìš©ì–´ ì‚¬ì „]
{dictionary}

ì‚¬ìš©ì ì§ˆë¬¸: {question}
ë³€ê²½ëœ ì§ˆë¬¸:"""


# ==========================================
# 1. ì„¤ì • í´ë˜ìŠ¤ (Dataclass)
# ==========================================

@dataclass
class RAGConfig:
    """
    RAG íŒŒì´í”„ë¼ì¸ì˜ ëª¨ë“  ì„¤ì •ì„ ì¤‘ì•™ì—ì„œ ê´€ë¦¬í•˜ëŠ” ì„¤ì • í´ë˜ìŠ¤.
    
    Attributes:
        llm_model: ì‚¬ìš©í•  LLM ëª¨ë¸ëª… (ê¸°ë³¸: exaone3.5:2.4b)
        temperature: LLM temperature (ê¸°ë³¸: 0.1)
        normalize_temperature: ì „ì²˜ë¦¬ LLM temperature (ê¸°ë³¸: 0)
        embedding_model: ì„ë² ë”© ëª¨ë¸ëª…
        k_law: Law ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
        k_rule: Rule ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
        k_case: Case ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
        search_multiplier: ì´ˆê¸° ê²€ìƒ‰ ì‹œ ë°°ìˆ˜ (Rerank ì „)
        rerank_threshold: Rerank ê´€ë ¨ë„ ì ìˆ˜ ì„ê³„ê°’
        enable_rerank: Reranking í™œì„±í™” ì—¬ë¶€
        rerank_model: Cohere Rerank ëª¨ë¸ëª…
    """
    # LLM ì„¤ì •
    llm_model: str = "exaone3.5:2.4b"
    temperature: float = 0.1
    normalize_temperature: float = 0.0
    
    # ì„ë² ë”© ì„¤ì •
    embedding_model: str = "solar-embedding-1-large-passage"
    
    # ê²€ìƒ‰ ì„¤ì •
    k_law: int = 5
    k_rule: int = 5
    k_case: int = 3
    search_multiplier: int = 2  # from ge.py: ì´ˆê¸° ê²€ìƒ‰ ì‹œ k * multiplier
    
    # Reranking ì„¤ì •
    enable_rerank: bool = True
    rerank_threshold: float = 0.2
    rerank_model: str = "rerank-multilingual-v3.0"
    
    # íŒë¡€ ê²€ìƒ‰ ì„¤ì •
    case_context_top_k: int = 50
    
    def __post_init__(self):
        """ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬"""
        if self.temperature < 0 or self.temperature > 2:
            raise ValueError("temperatureëŠ” 0~2 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        if self.rerank_threshold < 0 or self.rerank_threshold > 1:
            raise ValueError("rerank_thresholdëŠ” 0~1 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")


# ==========================================
# 2. RAG íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤
# ==========================================

class RAGPipeline:
    """
    ì£¼íƒì„ëŒ€ì°¨ RAG ì‹œìŠ¤í…œì˜ ë©”ì¸ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤.
    
    ì´ í´ë˜ìŠ¤ëŠ” VectorStore ì´ˆê¸°í™”, ì§ˆë¬¸ í‘œì¤€í™”, ê²€ìƒ‰, ë‹µë³€ ìƒì„±ì„ 
    í•˜ë‚˜ì˜ ì¸í„°í˜ì´ìŠ¤ë¡œ ì œê³µí•©ë‹ˆë‹¤.
    
    Usage:
        # ê¸°ë³¸ ì‚¬ìš©
        pipeline = RAGPipeline()
        answer = pipeline.generate_answer("ì§‘ì£¼ì¸ì´ ì›”ì„¸ë¥¼ ì˜¬ë ¤ë‹¬ë¼ê³  í•´ìš”")
        
        # ì»¤ìŠ¤í…€ ì„¤ì • ì‚¬ìš©
        config = RAGConfig(llm_model="exaone3.5:7.8b", k_law=7)
        pipeline = RAGPipeline(config)
    """
    
    def __init__(
        self, 
        config: Optional[RAGConfig] = None,
        pc_api_key: Optional[str] = None,
        cohere_api_key: Optional[str] = None
    ):
        """
        RAG íŒŒì´í”„ë¼ì¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            config: RAGConfig ê°ì²´ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            pc_api_key: Pinecone API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
            cohere_api_key: Cohere API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
        """
        # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
        load_dotenv(override=True)
        
        # ì„¤ì • ì´ˆê¸°í™”
        self.config = config or RAGConfig()
        
        # API í‚¤ ì„¤ì •
        self._pc_api_key = pc_api_key or os.getenv("PINECONE_API_KEY")
        self._cohere_api_key = cohere_api_key or os.getenv("COHERE_API_KEY")
        
        if not self._pc_api_key:
            raise ValueError("PINECONE_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # VectorStore ì´ˆê¸°í™”
        self._law_store: Optional[PineconeVectorStore] = None
        self._rule_store: Optional[PineconeVectorStore] = None
        self._case_store: Optional[PineconeVectorStore] = None
        
        # LLM ì¸ìŠ¤í„´ìŠ¤ (ì¬ì‚¬ìš©ì„ ìœ„í•´ ìºì‹±)
        self._normalize_llm: Optional[ChatOllama] = None
        self._generation_llm: Optional[ChatOllama] = None
        
        # Cohere í´ë¼ì´ì–¸íŠ¸
        self._cohere_client: Optional[Any] = None
        
        # ì´ˆê¸°í™” ì‹¤í–‰
        self._initialize()
    
    def _initialize(self) -> None:
        """ë‚´ë¶€ ì´ˆê¸°í™”: VectorStore ë° LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        # ì„ë² ë”© ì´ˆê¸°í™”
        embedding = UpstageEmbeddings(model=self.config.embedding_model)
        
        logger.info("ğŸ”— Pinecone 3ì¤‘ ì¸ë±ìŠ¤ ì—°ê²° ì¤‘...")
        
        # VectorStore ì´ˆê¸°í™”
        for key, index_name in INDEX_NAMES.items():
            store = PineconeVectorStore(
                index_name=index_name,
                embedding=embedding,
                pinecone_api_key=self._pc_api_key
            )
            setattr(self, f"_{key}_store", store)
        
        logger.info("âœ… [Law / Rule / Case] 3ê°œ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ!")
        
        # LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì¬ì‚¬ìš©)
        self._normalize_llm = ChatOllama(
            model=self.config.llm_model, 
            temperature=self.config.normalize_temperature
        )
        self._generation_llm = ChatOllama(
            model=self.config.llm_model, 
            temperature=self.config.temperature
        )
        
        # Cohere í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì„ íƒì )
        if self.config.enable_rerank and COHERE_AVAILABLE and self._cohere_api_key:
            self._cohere_client = cohere.Client(api_key=self._cohere_api_key)
            logger.info("âœ… Cohere Reranking í™œì„±í™”")
        elif self.config.enable_rerank:
            logger.warning("âš ï¸ Cohereë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Rerankingì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
            self.config.enable_rerank = False
    
    # ==========================================
    # ì†ì„± (Properties)
    # ==========================================
    
    @property
    def law_store(self) -> PineconeVectorStore:
        """Law VectorStore ë°˜í™˜"""
        if self._law_store is None:
            raise RuntimeError("VectorStoreê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return self._law_store
    
    @property
    def rule_store(self) -> PineconeVectorStore:
        """Rule VectorStore ë°˜í™˜"""
        if self._rule_store is None:
            raise RuntimeError("VectorStoreê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return self._rule_store
    
    @property
    def case_store(self) -> PineconeVectorStore:
        """Case VectorStore ë°˜í™˜"""
        if self._case_store is None:
            raise RuntimeError("VectorStoreê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return self._case_store
    
    # ==========================================
    # í•µì‹¬ ê¸°ëŠ¥ ë©”ì„œë“œ
    # ==========================================
    
    def normalize_query(self, user_query: str) -> str:
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²•ë¥  ìš©ì–´ë¡œ í‘œì¤€í™”í•©ë‹ˆë‹¤.
        
        Args:
            user_query: ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸
            
        Returns:
            í‘œì¤€í™”ëœ ì§ˆë¬¸ ë¬¸ìì—´
        """
        prompt = ChatPromptTemplate.from_template(NORMALIZATION_PROMPT)
        chain = prompt | self._normalize_llm | StrOutputParser()
        
        try:
            normalized = chain.invoke({
                "dictionary": KEYWORD_DICT,
                "question": user_query
            })
            return normalized.strip()
        except Exception as e:
            logger.warning(f"âš ï¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨ (ì›ë³¸ ì‚¬ìš©): {e}")
            return user_query
    
    def get_full_case_context(self, case_no: str) -> str:
        """
        íŠ¹ì • ì‚¬ê±´ë²ˆí˜¸ì˜ íŒë¡€ ì „ë¬¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        
        Args:
            case_no: ì‚¬ê±´ë²ˆí˜¸ (ì˜ˆ: "2020ë‚˜56247")
            
        Returns:
            íŒë¡€ ì „ë¬¸ í…ìŠ¤íŠ¸
        """
        try:
            results = self.case_store.similarity_search(
                query="íŒë¡€ ì „ë¬¸ ê²€ìƒ‰",  # API ìš”êµ¬ì‚¬í•­ì„ ìœ„í•œ ë”ë¯¸ ì¿¼ë¦¬
                k=self.config.case_context_top_k,
                filter={"case_no": {"$eq": case_no}}
            )
            
            # chunk_id ìˆœ ì •ë ¬
            sorted_docs = sorted(
                results, 
                key=lambda x: x.metadata.get('chunk_id', '')
            )
            
            # ì¤‘ë³µ ì œê±°
            seen_chunks: set = set()
            unique_docs: List[Document] = []
            for doc in sorted_docs:
                cid = doc.metadata.get('chunk_id')
                if cid and cid not in seen_chunks:
                    unique_docs.append(doc)
                    seen_chunks.add(cid)
            
            return "\n".join([doc.page_content for doc in unique_docs])
            
        except Exception as e:
            logger.warning(f"âš ï¸ íŒë¡€ ì „ë¬¸ ë¡œë”© ì‹¤íŒ¨ ({case_no}): {e}")
            return ""
    
    def triple_hybrid_retrieval(self, query: str) -> List[Document]:
        """
        Law, Rule, Case ì¸ë±ìŠ¤ì—ì„œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  Rerankingì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬ (í‘œì¤€í™”ëœ ì§ˆë¬¸ ê¶Œì¥)
            
        Returns:
            ë²•ì  ìœ„ê³„ ìˆœìœ¼ë¡œ ì •ë ¬ëœ Document ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ” [í†µí•© ê²€ìƒ‰] ì¿¼ë¦¬: '{query}'")
        
        cfg = self.config
        multiplier = cfg.search_multiplier
        
        # 1. ë³‘ë ¬ ê²€ìƒ‰ (Parallel Retrieval) - from ge.py: Ã—2 ë°°ìˆ˜ë¡œ ë„‰ë„‰íˆ ê²€ìƒ‰
        docs_law = self.law_store.similarity_search(
            query, k=cfg.k_law * multiplier
        )
        docs_rule = self.rule_store.similarity_search(
            query, k=cfg.k_rule * multiplier
        )
        docs_case_initial = self.case_store.similarity_search(
            query, k=cfg.k_case * multiplier
        )
        
        # 2. íŒë¡€ ë¬¸ë§¥ í™•ì¥ (Context Expansion)
        docs_case_expanded: List[Document] = []
        seen_cases: set = set()
        
        for doc in docs_case_initial:
            case_no = doc.metadata.get('case_no')
            if case_no and case_no not in seen_cases:
                full_text = self.get_full_case_context(case_no)
                if full_text:
                    # íŒë¡€ ì „ë¬¸ìœ¼ë¡œ êµì²´ (ë©”íƒ€ë°ì´í„° ìœ ì§€)
                    doc.page_content = (
                        f"[íŒë¡€ ì „ë¬¸: {doc.metadata.get('title')}]\n{full_text}"
                    )
                    docs_case_expanded.append(doc)
                    seen_cases.add(case_no)
                
                if len(docs_case_expanded) >= cfg.k_case:
                    break
        
        # 3. ë¬¸ì„œ í†µí•© (Law + Rule + Case)
        combined_docs = docs_law + docs_rule + docs_case_expanded
        
        # 4. Reranking (ì„ íƒì )
        selected_docs = combined_docs
        
        if cfg.enable_rerank and self._cohere_client:
            try:
                docs_content = [d.page_content for d in combined_docs]
                rerank_results = self._cohere_client.rerank(
                    model=cfg.rerank_model,
                    query=query,
                    documents=docs_content,
                    top_n=len(combined_docs)
                )
                
                filtered_docs: List[Document] = []
                logger.info(
                    f"ğŸ“Š Rerank ê²°ê³¼ (ì´ {len(combined_docs)}ê°œ, "
                    f"Threshold: {cfg.rerank_threshold}):"
                )
                
                for r in rerank_results.results:
                    if r.relevance_score > cfg.rerank_threshold:
                        doc = combined_docs[r.index]
                        p = doc.metadata.get('priority', 99)
                        t = doc.metadata.get('title', 'Untitled')
                        logger.info(
                            f" - [Score: {r.relevance_score:.4f}] [P-{p}] {t}"
                        )
                        filtered_docs.append(doc)
                
                selected_docs = filtered_docs
                
            except Exception as e:
                logger.warning(f"âš ï¸ Rerank ì‹¤íŒ¨ (ê¸°ë³¸ ë³‘í•© ë°˜í™˜): {e}")
        
        # 5. Priority Sorting (ë²•ì  ìœ„ê³„ ì •ë ¬)
        sorted_docs = sorted(
            selected_docs, 
            key=lambda x: int(x.metadata.get('priority', 99))
        )
        
        return sorted_docs
    
    @staticmethod
    def format_context_with_hierarchy(docs: List[Document]) -> str:
        """
        ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë²•ì  ìœ„ê³„(Priority)ì— ë”°ë¼ ì„¹ì…˜ë³„ë¡œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.
        
        Args:
            docs: Document ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ìœ„ê³„ êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
        """
        section_1_law: List[str] = []   # Priority 1, 2, 4, 5 (ë²•ë¥ , ì‹œí–‰ë ¹)
        section_2_rule: List[str] = []  # Priority 3, 6, 7, 8, 11 (ê·œì¹™, ì¡°ë¡€)
        section_3_case: List[str] = []  # Priority 9 (íŒë¡€, í•´ì„)
        
        for doc in docs:
            p = int(doc.metadata.get('priority', 99))
            src = doc.metadata.get('src_title', 'ìë£Œ')
            title = doc.metadata.get('title', '')
            content = doc.page_content
            
            entry = f"[{src}] {title}\n{content}"
            
            if p in [1, 2, 4, 5]:
                section_1_law.append(entry)
            elif p in [3, 6, 7, 8, 11]:
                section_2_rule.append(entry)
            else:
                section_3_case.append(entry)
        
        # ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ì¡°ë¦½
        formatted_text = ""
        
        if section_1_law:
            formatted_text += (
                "## [SECTION 1: í•µì‹¬ ë²•ë ¹ (ìµœìš°ì„  ë²•ì  ê·¼ê±°)]\n" 
                + "\n\n".join(section_1_law) + "\n\n"
            )
        if section_2_rule:
            formatted_text += (
                "## [SECTION 2: ê´€ë ¨ ê·œì • ë° ì ˆì°¨ (ì„¸ë¶€ ê¸°ì¤€)]\n" 
                + "\n\n".join(section_2_rule) + "\n\n"
            )
        if section_3_case:
            formatted_text += (
                "## [SECTION 3: íŒë¡€ ë° í•´ì„ ì‚¬ë¡€ (ì ìš© ì˜ˆì‹œ)]\n" 
                + "\n\n".join(section_3_case) + "\n\n"
            )
        
        return formatted_text
    
    def generate_answer(
        self, 
        user_input: str,
        skip_normalization: bool = False
    ) -> str:
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            user_input: ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸
            skip_normalization: Trueë©´ ì§ˆë¬¸ í‘œì¤€í™” ê³¼ì •ì„ ê±´ë„ˆëœ€
            
        Returns:
            ìµœì¢… ë‹µë³€ ë¬¸ìì—´
        """
        # 1. ì§ˆë¬¸ í‘œì¤€í™”
        if skip_normalization:
            normalized_query = user_input
        else:
            normalized_query = self.normalize_query(user_input)
            logger.info(f"ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: {normalized_query}")
        
        # 2. í†µí•© ê²€ìƒ‰ ë° ìœ„ê³„ ì •ë ¬
        retrieved_docs = self.triple_hybrid_retrieval(normalized_query)
        
        if not retrieved_docs:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë²•ë ¹ì´ë‚˜ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # 3. ìœ„ê³„ êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        hierarchical_context = self.format_context_with_hierarchy(retrieved_docs)
        
        # 4. LLM ë‹µë³€ ìƒì„±
        prompt = ChatPromptTemplate.from_messages([
            ("system", SYSTEM_PROMPT),
            ("human", "{question}"),
        ])
        
        chain = prompt | self._generation_llm | StrOutputParser()
        
        logger.info("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...")
        return chain.invoke({
            "context": hierarchical_context, 
            "question": normalized_query
        })


# ==========================================
# 3. ë ˆê±°ì‹œ í˜¸í™˜ í•¨ìˆ˜ (Backward Compatibility)
# ==========================================

# ì „ì—­ íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ (ë ˆê±°ì‹œ ì½”ë“œ ì§€ì›ìš©)
_global_pipeline: Optional[RAGPipeline] = None


def initialize_vector_stores(
    pc_api_key: Optional[str] = None,
    up_api_key: Optional[str] = None
) -> Tuple[PineconeVectorStore, PineconeVectorStore, PineconeVectorStore]:
    """
    ë ˆê±°ì‹œ í˜¸í™˜ í•¨ìˆ˜: VectorStore 3ê°œë¥¼ ì´ˆê¸°í™”í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    ìƒˆ ì½”ë“œì—ì„œëŠ” RAGPipeline í´ë˜ìŠ¤ë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
    
    Returns:
        (law_store, rule_store, case_store) íŠœí”Œ
    """
    global _global_pipeline
    _global_pipeline = RAGPipeline(pc_api_key=pc_api_key)
    return (
        _global_pipeline.law_store,
        _global_pipeline.rule_store,
        _global_pipeline.case_store
    )


def normalize_query(user_query: str, llm_model: str = "exaone3.5:2.4b") -> str:
    """
    ë ˆê±°ì‹œ í˜¸í™˜ í•¨ìˆ˜: ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë²•ë¥  ìš©ì–´ë¡œ í‘œì¤€í™”í•©ë‹ˆë‹¤.
    
    ìƒˆ ì½”ë“œì—ì„œëŠ” RAGPipeline.normalize_query()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    """
    if _global_pipeline:
        return _global_pipeline.normalize_query(user_query)
    
    # íŒŒì´í”„ë¼ì¸ ì—†ì´ ë‹¨ë… ì‹¤í–‰
    llm = ChatOllama(model=llm_model, temperature=0)
    prompt = ChatPromptTemplate.from_template(NORMALIZATION_PROMPT)
    chain = prompt | llm | StrOutputParser()
    
    try:
        return chain.invoke({
            "dictionary": KEYWORD_DICT,
            "question": user_query
        }).strip()
    except Exception as e:
        logger.warning(f"âš ï¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return user_query


def generate_final_answer(
    user_input: str,
    law_store: PineconeVectorStore,
    rule_store: PineconeVectorStore,
    case_store: PineconeVectorStore,
    llm_model: str = "exaone3.5:2.4b",
    temperature: float = 0.1,
    k_law: int = 3,
    k_rule: int = 3,
    k_case: int = 2,
    score_threshold: float = 0.2
) -> str:
    """
    ë ˆê±°ì‹œ í˜¸í™˜ í•¨ìˆ˜: ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    ìƒˆ ì½”ë“œì—ì„œëŠ” RAGPipeline.generate_answer()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    """
    if _global_pipeline:
        return _global_pipeline.generate_answer(user_input)
    
    # íŒŒì´í”„ë¼ì¸ ì—†ì´ ë‹¨ë… ì‹¤í–‰ (ì„ì‹œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±)
    config = RAGConfig(
        llm_model=llm_model,
        temperature=temperature,
        k_law=k_law,
        k_rule=k_rule,
        k_case=k_case,
        rerank_threshold=score_threshold
    )
    pipeline = RAGPipeline(config)
    return pipeline.generate_answer(user_input)


# ==========================================
# 4. Django/FastAPI í†µí•© ê°€ì´ë“œ
# ==========================================

"""
=== Django í†µí•© ì˜ˆì‹œ ===

# settings.py
RAG_CONFIG = {
    'llm_model': 'exaone3.5:2.4b',
    'temperature': 0.1,
    'k_law': 5,
    'k_rule': 5,
    'k_case': 3,
}

# apps.py
from django.apps import AppConfig
from rag_module_unified import RAGPipeline, RAGConfig

class ChatbotConfig(AppConfig):
    name = 'chatbot'
    pipeline = None
    
    def ready(self):
        from django.conf import settings
        config = RAGConfig(**settings.RAG_CONFIG)
        ChatbotConfig.pipeline = RAGPipeline(config)

# views.py
from django.http import JsonResponse
from .apps import ChatbotConfig

def chat_view(request):
    if request.method == 'POST':
        question = request.POST.get('question', '')
        answer = ChatbotConfig.pipeline.generate_answer(question)
        return JsonResponse({'answer': answer})


=== FastAPI í†µí•© ì˜ˆì‹œ ===

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from rag_module_unified import RAGPipeline, RAGConfig
from contextlib import asynccontextmanager

# ì „ì—­ íŒŒì´í”„ë¼ì¸
pipeline: RAGPipeline = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global pipeline
    config = RAGConfig(llm_model="exaone3.5:2.4b")
    pipeline = RAGPipeline(config)
    yield
    # Cleanup if needed

app = FastAPI(lifespan=lifespan)

class Question(BaseModel):
    text: str

@app.post("/chat")
async def chat(question: Question):
    if not pipeline:
        raise HTTPException(status_code=503, detail="Pipeline not initialized")
    answer = pipeline.generate_answer(question.text)
    return {"answer": answer}
"""


# ==========================================
# 5. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë¸”ë¡
# ==========================================

if __name__ == "__main__":
    print("=" * 70)
    print("ğŸš€ RAG LLM Pipeline (Unified) í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 70)
    
    try:
        # ë°©ë²• 1: í´ë˜ìŠ¤ ê¸°ë°˜ (ê¶Œì¥)
        print("\n[ë°©ë²• 1] RAGPipeline í´ë˜ìŠ¤ ì‚¬ìš©")
        print("-" * 50)
        
        # ì»¤ìŠ¤í…€ ì„¤ì •ìœ¼ë¡œ íŒŒì´í”„ë¼ì¸ ìƒì„±
        config = RAGConfig(
            llm_model="exaone3.5:2.4b",
            temperature=0.1,
            k_law=3,
            k_rule=3,
            k_case=2
        )
        pipeline = RAGPipeline(config)
        
        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
        test_queries = [
            "ì§‘ì£¼ì¸ì´ ì›”ì„¸ë¥¼ ì˜¬ë ¤ ë‹¬ë˜ìš”. ê±°ì ˆí•˜ë‹ˆê¹Œ ë‚˜ê°€ë¼ê³  í•˜ëŠ”ë° ì–´ë–¡í•˜ì£ ?",
            "ì „ì…ì‹ ê³ ëŠ” ì–¸ì œê¹Œì§€ í•´ì•¼ í•˜ë‚˜ìš”?",
            "ì§‘ì£¼ì¸ì´ ì‹¤ê±°ì£¼í•œë‹¤ê³  ë‚˜ê°€ë¼ê³  í•˜ëŠ”ë°, ì§„ì§œì¸ì§€ ì˜ì‹¬ìŠ¤ëŸ¬ì›Œìš”."
        ]
        
        for i, query in enumerate(test_queries, 1):
            print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ {i}: {query}")
            print("-" * 50)
            answer = pipeline.generate_answer(query)
            print(answer)
            print("=" * 70)
        
        # ë°©ë²• 2: ë ˆê±°ì‹œ í•¨ìˆ˜ ì‚¬ìš© (ì´ì „ ì½”ë“œ í˜¸í™˜)
        print("\n[ë°©ë²• 2] ë ˆê±°ì‹œ í•¨ìˆ˜ ì‚¬ìš© (Backward Compatibility)")
        print("-" * 50)
        
        law, rule, case = initialize_vector_stores()
        answer = generate_final_answer(
            "ë³´ì¦ê¸ˆ ëŒë ¤ë°›ìœ¼ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
            law, rule, case
        )
        print(answer)
        
    except Exception as e:
        logger.error(f"ğŸ”¥ ì—ëŸ¬ ë°œìƒ: {e}")
        raise
