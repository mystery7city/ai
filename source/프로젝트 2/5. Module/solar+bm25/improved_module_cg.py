"""
Unified RAG module (No FastAPI integration)

ì£¼íƒì„ëŒ€ì°¨ RAG ì‹œìŠ¤í…œ - í†µí•© ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ëª¨ë“ˆ

í•µì‹¬ ì„¤ê³„
- RAGConfig: ì„¤ì •ì„ ì¤‘ì•™ ê´€ë¦¬
- RAGPipeline: (1) ì§ˆë¬¸ í‘œì¤€í™” â†’ (2) 3ì¤‘ ì¸ë±ìŠ¤ ê²€ìƒ‰ â†’ (3) ì„ íƒì  Rerank â†’ (4) 2-stage íŒë¡€ í™•ì¥ â†’ (5) ë²•ì  ìœ„ê³„ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± â†’ (6) ë‹µë³€ ìƒì„±
- 2-stage case ì „ëµ:
  1) case-indexëŠ” 'ì²­í¬' ë‹¨ìœ„ë¡œ ë¨¼ì € í›„ë³´ë¥¼ ë§ì´ ê°€ì ¸ì™€ rerank/ì„ ë³„
  2) ìµœì¢… ì„ íƒëœ ìƒìœ„ ì‚¬ê±´ë²ˆí˜¸(top N)ì— ëŒ€í•´ì„œë§Œ ì „ë¬¸(context) í™•ì¥

í•„ìˆ˜ ì™¸ë¶€ ì˜ì¡´ì„±(ê¸°ë³¸ ê²½ë¡œ)
- langchain_core, langchain_community, langchain_pinecone
- cohere (ì„ íƒ: rerank ì‚¬ìš© ì‹œ í•„ìš”)
- Pinecone ì¸ë±ìŠ¤ 3ê°œ: law/rule/case (INDEX_NAMES ì°¸ê³ )

í™˜ê²½ë³€ìˆ˜
- PINECONE_API_KEY: PineconeVectorStore ì ‘ê·¼ìš©
- UPSTAGE_API_KEY: UpstageEmbeddings(SOLAR embedding)ìš© (upstage ì„ë² ë”© ì‚¬ìš© ì‹œ)
- COHERE_API_KEY: CohereEmbeddings / Cohere Rerankìš© (cohere ì„ë² ë”© ë˜ëŠ” rerank ì‚¬ìš© ì‹œ)

ì‘ì„±: unified from rag_module_cl2.py (+ 2-stage case rerank ê°œì„ , ë ˆê±°ì‹œ/í”„ë ˆì„ì›Œí¬ ê°€ì´ë“œ ì œê±°)
"""

from __future__ import annotations

import logging
import os
import math
import re
from collections import Counter, defaultdict
from dataclasses import dataclass
from typing import Dict, List, Optional, Sequence, Tuple, Iterable, Callable, Any, Mapping

from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.chat_models import ChatOllama

# Embeddings backends (SOLAR: Upstage, fallback: Cohere)
try:
    from langchain_upstage import UpstageEmbeddings  # type: ignore
    UPSTAGE_AVAILABLE = True
except Exception:
    UpstageEmbeddings = None  # type: ignore
    UPSTAGE_AVAILABLE = False

try:
    from langchain_community.embeddings import CohereEmbeddings  # type: ignore
    COHERE_EMBED_AVAILABLE = True
except Exception:
    CohereEmbeddings = None  # type: ignore
    COHERE_EMBED_AVAILABLE = False

from langchain_pinecone import PineconeVectorStore

# Optional: Cohere Rerank
try:
    import cohere  # type: ignore
    COHERE_AVAILABLE = True
except Exception:
    cohere = None
    COHERE_AVAILABLE = False

# --------------------------------------------------------------------------------------
# Logging
# --------------------------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# --------------------------------------------------------------------------------------
# Index names
# --------------------------------------------------------------------------------------
INDEX_NAMES: Dict[str, str] = {
    "law": "law-index-final",    # Priority 1,2,4,5: ì£¼ì„ë²•, ë¯¼ë²• ë“± í•µì‹¬ ë²•ë¥ 
    "rule": "rule-index-final",  # Priority 3,6,7,8,11: ì‹œí–‰ê·œì¹™, ì¡°ë¡€, ì ˆì°¨
    "case": "case-index-final",  # Priority 9: íŒë¡€, ìƒë‹´ì‚¬ë¡€
}

# --------------------------------------------------------------------------------------
# Keyword dictionary (query normalization)
# --------------------------------------------------------------------------------------
KEYWORD_DICT: Dict[str, str] = {
    # 1. ê³„ì•½ ì£¼ì²´ ë° ëŒ€ìƒ
    "ì§‘ì£¼ì¸": "ì„ëŒ€ì¸", "ê±´ë¬¼ì£¼": "ì„ëŒ€ì¸", "ì£¼ì¸ì§‘": "ì„ëŒ€ì¸",
    "ì„ëŒ€ì—…ì": "ì„ëŒ€ì¸", "ìƒˆì£¼ì¸": "ì„ëŒ€ì¸",
    "ì„¸ì…ì": "ì„ì°¨ì¸", "ì›”ì„¸ì…ì": "ì„ì°¨ì¸", "ì„¸ë“¤ì–´ì‚¬ëŠ”ì‚¬ëŒ": "ì„ì°¨ì¸",
    "ì„ì°¨ì": "ì„ì°¨ì¸", "ì…ì£¼ì": "ì„ì°¨ì¸",
    "ë¶€ë™ì‚°": "ê³µì¸ì¤‘ê°œì‚¬", "ì¤‘ê°œì¸": "ê³µì¸ì¤‘ê°œì‚¬", "ì¤‘ê°œì†Œ": "ê³µì¸ì¤‘ê°œì‚¬",
    "ë¹Œë¼": "ì„ì°¨ì£¼íƒ", "ì•„íŒŒíŠ¸": "ì„ì°¨ì£¼íƒ", "ì˜¤í”¼ìŠ¤í…”": "ì„ì°¨ì£¼íƒ",
    "ìš°ë¦¬ì§‘": "ì„ì°¨ì£¼íƒ", "ê±°ì£¼ì§€": "ì„ì°¨ì£¼íƒ",
    "ê³„ì•½ì„œ": "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ", "ì§‘ë¬¸ì„œ": "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ", "ì¢…ì´": "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ",

    # 2. ë³´ì¦ê¸ˆ ë° ê¸ˆì „
    "ë³´ì¦ê¸ˆ": "ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ", "ì „ì„¸ê¸ˆ": "ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ", "ë³´ì¦ë³´í—˜": "ë³´ì¦ê¸ˆë°˜í™˜ë³´ì¦",
    "ëˆëª»ë°›ìŒ": "ë³´ì¦ê¸ˆë¯¸ë°˜í™˜", "ì•ˆëŒë ¤ì¤Œ": "ë³´ì¦ê¸ˆë¯¸ë°˜í™˜", "ëª»ëŒë ¤ë°›ìŒ": "ë³´ì¦ê¸ˆë¯¸ë°˜í™˜",
    "ì›”ì„¸": "ì°¨ì„", "ê´€ë¦¬ë¹„": "ê´€ë¦¬ë¹„", "ì—°ì²´": "ì°¨ì„ì—°ì²´", "ë°€ë¦¼": "ì°¨ì„ì—°ì²´",

    # 3. ê¸°ê°„ ë° ì¢…ë£Œ/ê°±ì‹ 
    "ì¬ê³„ì•½": "ê³„ì•½ê°±ì‹ ", "ì—°ì¥": "ê³„ì•½ê°±ì‹ ", "ê°±ì‹ ": "ê³„ì•½ê°±ì‹ ",
    "ê°±ì‹ ì²­êµ¬": "ê³„ì•½ê°±ì‹ ìš”êµ¬ê¶Œ", "2ë…„ë”": "ê³„ì•½ê°±ì‹ ìš”êµ¬ê¶Œ",
    "ìë™ì—°ì¥": "ë¬µì‹œì ê°±ì‹ ", "ë¬µì‹œ": "ë¬µì‹œì ê°±ì‹ ",
    "ì´ì‚¬": "ì£¼íƒì˜ì¸ë„", "ì§ë¹¼ê¸°": "ì£¼íƒì˜ì¸ë„", "í‡´ê±°": "ì£¼íƒì˜ì¸ë„",
    "ë°©ë¹¼": "ê³„ì•½í•´ì§€",
    "ì£¼ì†Œì˜®ê¸°ê¸°": "ì£¼ë¯¼ë“±ë¡", "ì „ì…ì‹ ê³ ": "ì£¼ë¯¼ë“±ë¡", "ì£¼ì†Œì§€ì´ì „": "ì£¼ë¯¼ë“±ë¡",
    "ì§‘ì£¼ì¸ë°”ë€œ": "ì„ëŒ€ì¸ì§€ìœ„ìŠ¹ê³„", "ì£¼ì¸ë°”ë€œ": "ì„ëŒ€ì¸ì§€ìœ„ìŠ¹ê³„",
    "ë§¤ë§¤": "ì„ëŒ€ì¸ì§€ìœ„ìŠ¹ê³„",
    "ë‚˜ê°€ë¼ê³ í•¨": "ê³„ì•½ê°±ì‹ ê±°ì ˆ", "ì«“ê²¨ë‚¨": "ëª…ë„", "ë¹„ì›Œë‹¬ë¼": "ëª…ë„",
    "ì¤‘ë„í•´ì§€": "ê³„ì•½í•´ì§€",

    # 4. ìˆ˜ë¦¬ ë° ìƒí™œí™˜ê²½
    "ì§‘ê³ ì¹˜ê¸°": "ìˆ˜ì„ ì˜ë¬´", "ìˆ˜ë¦¬": "ìˆ˜ì„ ì˜ë¬´", "ê³ ì³ì¤˜": "ìˆ˜ì„ ì˜ë¬´",
    "ì•ˆê³ ì³ì¤Œ": "ìˆ˜ì„ ì˜ë¬´ìœ„ë°˜",
    "ê³°íŒ¡ì´": "í•˜ì", "ë¬¼ìƒ˜": "ëˆ„ìˆ˜", "ë³´ì¼ëŸ¬ê³ ì¥": "í•˜ì", "íŒŒì†": "í›¼ì†",
    "ê¹¨ë—ì´ì¹˜ìš°ê¸°": "ì›ìƒíšŒë³µì˜ë¬´", "ì›ë˜ëŒ€ë¡œí•´ë†“ê¸°": "ì›ìƒíšŒë³µ",
    "ì²­ì†Œë¹„": "ì›ìƒíšŒë³µë¹„ìš©", "ì²­ì†Œ": "ì›ìƒíšŒë³µ",
    "ì¸µê°„ì†ŒìŒ": "ê³µë™ìƒí™œí‰ì˜¨", "ì˜†ì§‘ì†ŒìŒ": "ë°©ìŒ", "ê°œí‚¤ìš°ê¸°": "ë°˜ë ¤ë™ë¬¼íŠ¹ì•½",

    # 5. ê¶Œë¦¬/ëŒ€í•­ë ¥/í™•ì •ì¼ì
    "í™•ì •ì¼ì": "í™•ì •ì¼ì", "ì „ì…": "ì£¼ë¯¼ë“±ë¡", "ëŒ€í•­ë ¥": "ëŒ€í•­ë ¥",
    "ìš°ì„ ë³€ì œ": "ìš°ì„ ë³€ì œê¶Œ", "ìµœìš°ì„ ": "ìµœìš°ì„ ë³€ì œê¶Œ",
    "ê²½ë§¤": "ê²½ë§¤ì ˆì°¨", "ê³µë§¤": "ê³µë§¤ì ˆì°¨",
    "ë“±ê¸°": "ë“±ê¸°ë¶€ë“±ë³¸", "ë“±ë³¸": "ë“±ê¸°ë¶€ë“±ë³¸",
    "ê·¼ì €ë‹¹": "ê·¼ì €ë‹¹ê¶Œ", "ê°€ì••ë¥˜": "ê°€ì••ë¥˜", "ê°€ì²˜ë¶„": "ê°€ì²˜ë¶„",

    # 6. ë¶„ìŸ í•´ê²°
    "ë‚´ìš©ì¦ëª…": "ë‚´ìš©ì¦ëª…", "ì†Œì†¡": "ì†Œì†¡", "ë¯¼ì‚¬": "ë¯¼ì‚¬ì†Œì†¡",
    "ì¡°ì •ìœ„": "ì£¼íƒì„ëŒ€ì°¨ë¶„ìŸì¡°ì •ìœ„ì›íšŒ", "ì†Œì†¡ë§ê³ ": "ë¶„ìŸì¡°ì •",
    "ë²•ì›ê°€ê¸°ì‹«ìŒ": "ë¶„ìŸì¡°ì •",
    "ì§‘ì£¼ì¸ì‚¬ë§": "ì„ì°¨ê¶ŒìŠ¹ê³„", "ìì‹ìƒì†": "ì„ì°¨ê¶ŒìŠ¹ê³„",
}

# --------------------------------------------------------------------------------------
# Prompts
# --------------------------------------------------------------------------------------
NORMALIZATION_PROMPT: str = """
ë‹¹ì‹ ì€ ë²•ë¥  AI ì±—ë´‡ì˜ ì „ì²˜ë¦¬ ë‹´ë‹¹ìì…ë‹ˆë‹¤.
ì•„ë˜ [ìš©ì–´ ì‚¬ì „]ì„ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ 'ë²•ë¥  í‘œì¤€ì–´'ë¡œ ë³€í™˜í•´ ì£¼ì„¸ìš”.

[ìˆ˜í–‰ ì§€ì¹¨]
1. ì‚¬ì „ì— ìˆëŠ” ë‹¨ì–´ëŠ” ë°˜ë“œì‹œ ë§¤í•‘ëœ ë²•ë¥  ìš©ì–´ë¡œ ë³€ê²½í•˜ì„¸ìš”.
2. ë‹¨ì–´ë¥¼ ë³€ê²½í•  ë•Œ ë¬¸ë§¥ì— ë§ê²Œ ì¡°ì‚¬(ì´/ê°€, ì„/ë¥¼ ë“±)ë‚˜ ì„œìˆ ì–´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ìˆ˜ì •í•˜ì„¸ìš”.
3. ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ì™œê³¡í•˜ê±°ë‚˜ ì¶”ê°€ì ì¸ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.
4. ì˜¤ì§ 'ë³€ê²½ëœ ì§ˆë¬¸' í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”. (ì„¤ëª… ê¸ˆì§€)

[ìš©ì–´ ì‚¬ì „]
{dictionary}

ì‚¬ìš©ì ì§ˆë¬¸: {question}
ë³€ê²½ëœ ì§ˆë¬¸:
"""

SYSTEM_PROMPT: str = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ 'ì£¼íƒ ì „ì›”ì„¸ ì‚¬ê¸° ì˜ˆë°© ë° ì„ëŒ€ì°¨ ë²•ë¥  ì „ë¬¸ê°€ AI'ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ [ë²•ì  ìœ„ê³„ê°€ ì •ë¦¬ëœ ì°¸ê³  ë¬¸ì„œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

[ë‹µë³€ ìƒì„± ì›ì¹™]
1. **ë²•ì  ìœ„ê³„ ì¤€ìˆ˜**:
   - ë°˜ë“œì‹œ [SECTION 1: í•µì‹¬ ë²•ë ¹]ì˜ ë‚´ìš©ì„ ìµœìš°ì„  íŒë‹¨ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ìœ¼ì„¸ìš”.
   - [SECTION 1]ì˜ ë‚´ìš©ì´ ëª¨í˜¸í•  ë•Œë§Œ [SECTION 2]ì™€ [SECTION 3]ë¥¼ ë³´ì¶© ê·¼ê±°ë¡œ í™œìš©í•˜ì„¸ìš”.
   - ë§Œì•½ [SECTION 3: íŒë¡€]ê°€ [SECTION 1: ë²•ë ¹]ê³¼ ë‹¤ë¥´ê²Œ í•´ì„ë˜ëŠ” íŠ¹ìˆ˜í•œ ê²½ìš°ë¼ë©´,
     "ì›ì¹™ì€ ë²•ë ¹ì— ë”°ë¥´ë‚˜, íŒë¡€ëŠ” ì˜ˆì™¸ì ìœ¼ë¡œ..."ë¼ê³  ì„¤ëª…í•˜ì„¸ìš”.

2. **ë‹µë³€ êµ¬ì¡°**:
   - **í•µì‹¬ ê²°ë¡ **: ì§ˆë¬¸ì— ëŒ€í•œ ê²°ë¡ (ê°€ëŠ¥/ë¶ˆê°€ëŠ¥/ìœ íš¨/ë¬´íš¨)ì„ ë‘ê´„ì‹ìœ¼ë¡œ ìš”ì•½.
   - **ë²•ì  ê·¼ê±°**: "ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì œOì¡°ì— ë”°ë¥´ë©´..." (SECTION 1 ì¸ìš©)
   - **ì‹¤ë¬´ ì ˆì°¨**: í•„ìš”ì‹œ ì‹ ê³  ë°©ë²•, ì„œë¥˜ ë“± ì•ˆë‚´ (SECTION 2 ì¸ìš©)
   - **ì°¸ê³  ì‚¬ë¡€**: ìœ ì‚¬í•œ ìƒí™©ì—ì„œì˜ íŒê²°ì´ë‚˜ í•´ì„ (SECTION 3 ì¸ìš©)

3. **ì£¼ì˜ì‚¬í•­**:
   - ì‚¬ìš©ìì˜ ê³„ì•½ì„œ ë‚´ìš©ì´ ë²•ë ¹(ê°•í–‰ê·œì •)ì— ìœ„ë°˜ë˜ë©´ "íš¨ë ¥ì´ ì—†ë‹¤(ë¬´íš¨)"ê³  ëª…í™•íˆ ê²½ê³ í•˜ì„¸ìš”.
   - ë²•ë¥ ì  ì¡°ì–¸ì¼ ë¿ì´ë¯€ë¡œ, ìµœì¢…ì ìœ¼ë¡œëŠ” ë³€í˜¸ì‚¬ ë“±ì˜ ì „ë¬¸ê°€ í™•ì¸ì´ í•„ìš”í•¨ì„ ê³ ì§€í•˜ì„¸ìš”.

[ë²•ì  ìœ„ê³„ê°€ ì •ë¦¬ëœ ì°¸ê³  ë¬¸ì„œ]
{context}
"""

# --------------------------------------------------------------------------------------
# Utilities
# --------------------------------------------------------------------------------------
def _safe_int(x: object, default: int = 99) -> int:
    try:
        return int(x)  # type: ignore[arg-type]
    except Exception:
        return default


def _truncate(text: str, max_chars: int) -> str:
    if text is None:
        return ""
    if len(text) <= max_chars:
        return text
    return text[: max_chars - 1] + "â€¦"


def _dedupe_docs(
    docs: Iterable[Document],
    key_fields: Sequence[str] = ("chunk_id", "id"),
) -> List[Document]:
    """
    ë©”íƒ€ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (chunk_id ìš°ì„ )
    - key_fields ì¤‘ ì²« ë²ˆì§¸ë¡œ ë°œê²¬ë˜ëŠ” ê°’ì„ í‚¤ë¡œ ì‚¬ìš©
    - í‚¤ê°€ ì—†ìœ¼ë©´ page_contentì˜ í•´ì‹œ(ì§§ê²Œ)ë¡œ í´ë°±
    """
    seen = set()
    out: List[Document] = []
    for d in docs:
        md = d.metadata or {}
        key = None
        for f in key_fields:
            v = md.get(f)
            if v:
                key = f"{f}:{v}"
                break
        if key is None:
            key = f"content:{hash(d.page_content)}"
        if key in seen:
            continue
        seen.add(key)
        out.append(d)
    return out


# --------------------------------------------------------------------------------------
# Sparse scoring (BM25) + fusion utilities
# --------------------------------------------------------------------------------------

_TOKEN_RE = re.compile(r"[0-9A-Za-zê°€-í£]+")


def _default_tokenize(text: str) -> List[str]:
    # í•œêµ­ì–´/ì˜ë¬¸/ìˆ«ì ì¤‘ì‹¬ì˜ ê°€ë²¼ìš´ í† í¬ë‚˜ì´ì € (í˜•íƒœì†Œ ë¶„ì„ê¸° ì—†ì´ë„ ë™ì‘)
    return _TOKEN_RE.findall((text or "").lower())


def _bm25_scores(
    query_tokens: List[str],
    docs_tokens: List[List[str]],
    *,
    k1: float = 1.5,
    b: float = 0.75,
) -> List[float]:
    '''
    BM25Okapi-lite (candidate-level).
    - docs_tokensëŠ” í›„ë³´ ë¬¸ì„œë“¤(ë³´í†µ 20~80ê°œ)ì— ëŒ€í•´ì„œë§Œ ê³„ì‚°í•˜ë¯€ë¡œ O(N*V)ë¡œë„ ì¶©ë¶„.
    '''
    N = len(docs_tokens)
    if N == 0:
        return []
    if not query_tokens:
        return [0.0] * N

    # document lengths
    doc_lens = [len(toks) for toks in docs_tokens]
    avgdl = (sum(doc_lens) / N) if N else 1.0
    if avgdl <= 0:
        avgdl = 1.0

    # df: term -> number of docs containing term
    df: Dict[str, int] = defaultdict(int)
    for toks in docs_tokens:
        seen = set(toks)
        for t in seen:
            df[t] += 1

    # idf
    idf: Dict[str, float] = {}
    for t, dfi in df.items():
        # standard BM25 idf variant
        idf[t] = math.log(1.0 + (N - dfi + 0.5) / (dfi + 0.5))

    # query term frequency (optional weighting)
    qtf = Counter(query_tokens)

    scores: List[float] = []
    for toks, dl in zip(docs_tokens, doc_lens):
        tf = Counter(toks)
        score = 0.0
        norm = (1.0 - b) + b * (dl / avgdl)
        for term, qf in qtf.items():
            if term not in tf:
                continue
            f = tf[term]
            denom = f + k1 * norm
            if denom == 0:
                continue
            score += (idf.get(term, 0.0) * (f * (k1 + 1.0) / denom)) * (1.0 + 0.1 * (qf - 1))
        scores.append(float(score))
    return scores


def _rank_fusion(
    dense_ranks: List[int],
    sparse_ranks: List[int],
    *,
    mode: str = "rrf",
    w_dense: float = 0.6,
    w_sparse: float = 0.4,
    rrf_k: int = 60,
) -> List[float]:
    '''
    Rank ê¸°ë°˜ fusion (ì ìˆ˜ ìŠ¤ì¼€ì¼/ê±°ë¦¬/ìœ ì‚¬ë„ ì •ì˜ì— ëœ ë¯¼ê°).
    Returns:
        fused_scores (higher is better)
    '''
    n = len(dense_ranks)
    if n == 0:
        return []
    if mode == "rrf":
        k = max(1, int(rrf_k))
        return [
            (w_dense / (k + dense_ranks[i])) + (w_sparse / (k + sparse_ranks[i]))
            for i in range(n)
        ]
    # mode == "rank_sum": ranks -> [0,1]ë¡œ ë³€í™˜ í›„ ê°€ì¤‘í•©
    if n == 1:
        return [w_dense + w_sparse]
    def to_unit(r: int) -> float:
        return 1.0 - (r - 1) / (n - 1)
    return [
        (w_dense * to_unit(dense_ranks[i])) + (w_sparse * to_unit(sparse_ranks[i]))
        for i in range(n)
    ]



# --------------------------------------------------------------------------------------
# Config
# --------------------------------------------------------------------------------------
@dataclass
class RAGConfig:
    # LLM
    llm_model: str = "exaone3.5:2.4b"
    temperature: float = 0.1
    normalize_temperature: float = 0.0

    # Embedding
    # - "auto": UPSTAGE_AVAILABLEë©´ Upstage(SOLAR) ìš°ì„ , ì•„ë‹ˆë©´ Cohere
    # - "upstage": UpstageEmbeddings ê°•ì œ
    # - "cohere": CohereEmbeddings ê°•ì œ
    embedding_backend: str = "auto"
    embedding_model: str = "solar-embedding-1-large-passage"

    # Retrieval sizes (final target)
    k_law: int = 5
    k_rule: int = 5
    k_case: int = 3

    # Oversampling before rerank
    search_multiplier: int = 2

    # Rerank
    enable_rerank: bool = True
    rerank_threshold: float = 0.2
    rerank_model: str = "rerank-multilingual-v3.0"
    rerank_max_documents: int = 80              # cohere rerank ì…ë ¥ ë¬¸ì„œ ìµœëŒ€ ê°œìˆ˜
    rerank_doc_max_chars: int = 2000            # rerank ì…ë ¥ ë¬¸ì„œ truncation


    # Dense + Sparse hybrid (BM25)
    # - Pinecone ì¸ë±ìŠ¤ ë³€ê²½ ì—†ì´ë„, Dense ìƒìœ„ í›„ë³´ì— ëŒ€í•´ BM25 ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ê²°í•©(=candidate-level hybrid)í•©ë‹ˆë‹¤.
    enable_bm25: bool = True
    bm25_k1: float = 1.5
    bm25_b: float = 0.75
    bm25_max_doc_chars: int = 4000  # BM25 í† í¬ë‚˜ì´ì§•/ìŠ¤ì½”ì–´ë§ ì‹œ ë¬¸ì„œ í…ìŠ¤íŠ¸ ìµœëŒ€ ê¸¸ì´

    # Fusion strategy: "rrf" (ê¶Œì¥, ì ìˆ˜ ìŠ¤ì¼€ì¼ì— ê°•ê±´) | "rank_sum"
    hybrid_fusion: str = "rrf"
    hybrid_dense_weight: float = 0.6
    hybrid_sparse_weight: float = 0.4
    rrf_k: int = 60

    # 2-stage case expansion
    case_candidate_k: int = 40                  # case-indexì—ì„œ 'ì²­í¬'ë¡œ ê°€ì ¸ì˜¬ í›„ë³´ ìˆ˜
    case_expand_top_n: Optional[int] = None     # Noneì´ë©´ k_case ì‚¬ìš©
    case_context_top_k: int = 50                # ì„ íƒëœ ì‚¬ê±´ë²ˆí˜¸ì˜ ì „ë¬¸ í™•ì¥ ì‹œ ìµœëŒ€ chunk ìˆ˜

    # Deduping
    dedupe_key_fields: Tuple[str, ...] = ("chunk_id", "id")

    def __post_init__(self) -> None:
        if not (0 <= self.temperature <= 2):
            raise ValueError("temperatureëŠ” 0~2 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        if not (0 <= self.rerank_threshold <= 1):
            raise ValueError("rerank_thresholdëŠ” 0~1 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        if self.search_multiplier < 1:
            raise ValueError("search_multiplierëŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        if self.rerank_max_documents < 1:
            raise ValueError("rerank_max_documentsëŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        if self.case_candidate_k < 1:
            raise ValueError("case_candidate_këŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        if self.case_context_top_k < 1:
            raise ValueError("case_context_top_këŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

        # BM25 / hybrid fusion validation
        if self.bm25_k1 <= 0:
            raise ValueError("bm25_k1ì€ 0ë³´ë‹¤ ì»¤ì•¼ í•©ë‹ˆë‹¤.")
        if not (0 <= self.bm25_b <= 1):
            raise ValueError("bm25_bëŠ” 0~1 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        if self.bm25_max_doc_chars < 200:
            raise ValueError("bm25_max_doc_charsëŠ” 200 ì´ìƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        if self.hybrid_fusion not in ("rrf", "rank_sum"):
            raise ValueError('hybrid_fusionì€ "rrf" ë˜ëŠ” "rank_sum" ì´ì–´ì•¼ í•©ë‹ˆë‹¤.')
        if self.rrf_k < 1:
            raise ValueError("rrf_këŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        if self.hybrid_dense_weight < 0 or self.hybrid_sparse_weight < 0:
            raise ValueError("hybrid_*_weightëŠ” 0 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        if self.hybrid_dense_weight == 0 and self.hybrid_sparse_weight == 0:
            raise ValueError("hybrid_dense_weightì™€ hybrid_sparse_weightê°€ ëª¨ë‘ 0ì¼ ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤.")



# --------------------------------------------------------------------------------------
# Pipeline
# --------------------------------------------------------------------------------------
class RAGPipeline:
    """
    Unified RAG pipeline (no web framework integration).

    Usage:
        pipeline = RAGPipeline()
        answer = pipeline.generate_answer("ë³´ì¦ê¸ˆì„ ëª» ëŒë ¤ë°›ì•˜ì–´ìš”. ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?")
    """

    def __init__(
        self,
        config: Optional[RAGConfig] = None,
        *,
        pc_api_key: Optional[str] = None,
        upstage_api_key: Optional[str] = None,
        cohere_api_key: Optional[str] = None,
        embedding: Optional[object] = None,
        cohere_client: Optional[object] = None,
    ) -> None:
        self.config = config or RAGConfig()

        self._pc_api_key = pc_api_key or os.getenv("PINECONE_API_KEY")
        self._upstage_api_key = upstage_api_key or os.getenv("UPSTAGE_API_KEY")
        self._cohere_api_key = cohere_api_key or os.getenv("COHERE_API_KEY")

        if not self._pc_api_key:
            raise ValueError("Pinecone API keyê°€ í•„ìš”í•©ë‹ˆë‹¤. pc_api_key ì¸ìë¡œ ì£¼ê±°ë‚˜ PINECONE_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")        # Embeddings
        # - ê¸°ë³¸ì€ config.embedding_backendì— ë”°ë¼ ìë™ ì„ íƒí•©ë‹ˆë‹¤.
        # - Solar(Upstage) + BM25 Dense/Sparse ì¡°í•©ì„ ì›í•˜ë©´ embedding_backend="upstage" ê¶Œì¥
        if embedding is not None:
            self._embedding = embedding
        else:
            backend = (self.config.embedding_backend or "auto").lower()

            if backend in ("auto", "upstage") and UPSTAGE_AVAILABLE:
                if not self._upstage_api_key:
                    raise ValueError(
                        "embedding_backend=upstage(ë˜ëŠ” autoì—ì„œ upstage ì‚¬ìš©)ë¥¼ ìœ„í•´ì„œëŠ” UPSTAGE_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤. "
                        "upstage_api_key ì¸ìë¡œ ì£¼ê±°ë‚˜ UPSTAGE_API_KEYë¥¼ ì„¤ì •í•˜ê±°ë‚˜, embedding ê°ì²´ë¥¼ ì£¼ì…í•˜ì„¸ìš”."
                    )
                # langchain_upstageëŠ” ë³´í†µ í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ë¥¼ ì½ìŠµë‹ˆë‹¤.
                os.environ.setdefault("UPSTAGE_API_KEY", self._upstage_api_key)
                self._embedding = UpstageEmbeddings(model=self.config.embedding_model)  # type: ignore[call-arg]

            elif backend in ("auto", "cohere") and COHERE_EMBED_AVAILABLE:
                if not self._cohere_api_key:
                    raise ValueError(
                        "embedding_backend=cohere(ë˜ëŠ” autoì—ì„œ cohere ì‚¬ìš©)ë¥¼ ìœ„í•´ì„œëŠ” COHERE_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤. "
                        "cohere_api_key ì¸ìë¡œ ì£¼ê±°ë‚˜ COHERE_API_KEYë¥¼ ì„¤ì •í•˜ê±°ë‚˜, embedding ê°ì²´ë¥¼ ì£¼ì…í•˜ì„¸ìš”."
                    )
                self._embedding = CohereEmbeddings(  # type: ignore[call-arg]
                    model=self.config.embedding_model,
                    cohere_api_key=self._cohere_api_key,
                )

            else:
                raise ImportError(
                    "ì‚¬ìš© ê°€ëŠ¥í•œ embedding backendê°€ ì—†ìŠµë‹ˆë‹¤. "
                    "langchain_upstage(UpstageEmbeddings) ë˜ëŠ” langchain_community(CohereEmbeddings) ì„¤ì¹˜/ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”."
                )

        # Vector stores
        logger.info("ğŸ”— Pinecone 3ì¤‘ ì¸ë±ìŠ¤ ì—°ê²° ì¤‘...")
        self._law_store = PineconeVectorStore(
            index_name=INDEX_NAMES["law"],
            embedding=self._embedding,
            pinecone_api_key=self._pc_api_key,
        )
        self._rule_store = PineconeVectorStore(
            index_name=INDEX_NAMES["rule"],
            embedding=self._embedding,
            pinecone_api_key=self._pc_api_key,
        )
        self._case_store = PineconeVectorStore(
            index_name=INDEX_NAMES["case"],
            embedding=self._embedding,
            pinecone_api_key=self._pc_api_key,
        )
        logger.info("âœ… [Law / Rule / Case] 3ê°œ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ!")

        # LLM instances (reused)
        self._normalize_llm = ChatOllama(
            model=self.config.llm_model,
            temperature=self.config.normalize_temperature,
        )
        self._generation_llm = ChatOllama(
            model=self.config.llm_model,
            temperature=self.config.temperature,
        )

        # Cohere rerank client (optional)
        self._cohere_client = None
        if self.config.enable_rerank:
            if not COHERE_AVAILABLE:
                logger.warning("âš ï¸ cohere íŒ¨í‚¤ì§€ê°€ ì—†ì–´ rerankë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
            elif not self._cohere_api_key:
                logger.warning("âš ï¸ COHERE_API_KEYê°€ ì—†ì–´ rerankë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
            else:
                self._cohere_client = cohere_client or cohere.Client(self._cohere_api_key)  # type: ignore[attr-defined]

    # ----------------------------
    # Properties
    # ----------------------------
    @property
    def law_store(self) -> PineconeVectorStore:
        return self._law_store

    @property
    def rule_store(self) -> PineconeVectorStore:
        return self._rule_store

    @property
    def case_store(self) -> PineconeVectorStore:
        return self._case_store

    # ----------------------------
    # Core steps
    # ----------------------------
    def normalize_query(self, user_query: str) -> str:
        """ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²•ë¥  ìš©ì–´ë¡œ í‘œì¤€í™”"""
        prompt = ChatPromptTemplate.from_template(NORMALIZATION_PROMPT)
        chain = prompt | self._normalize_llm | StrOutputParser()

        try:
            normalized = chain.invoke({"dictionary": KEYWORD_DICT, "question": user_query})
            return str(normalized).strip()
        except Exception as e:
            logger.warning(f"âš ï¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨ (ì›ë³¸ ì‚¬ìš©): {e}")
            return user_query

    def get_full_case_context(self, case_no: str) -> str:
        """íŠ¹ì • ì‚¬ê±´ë²ˆí˜¸(case_no)ì˜ íŒë¡€ ì „ë¬¸(ì²­í¬ë“¤ì„ ì—°ê²°)ì„ ê°€ì ¸ì˜´"""
        try:
            results = self.case_store.similarity_search(
                query="íŒë¡€ ì „ë¬¸ ê²€ìƒ‰",  # API ìš”êµ¬ì‚¬í•­ìš© ë”ë¯¸ ì¿¼ë¦¬
                k=self.config.case_context_top_k,
                filter={"case_no": {"$eq": case_no}},
            )
            # chunk_id ìˆœ ì •ë ¬ í›„ ì¤‘ë³µ ì œê±°
            sorted_docs = sorted(results, key=lambda x: str(x.metadata.get("chunk_id", "")))
            unique_docs = _dedupe_docs(sorted_docs, self.config.dedupe_key_fields)
            return "\n".join([d.page_content for d in unique_docs]).strip()
        except Exception as e:
            logger.warning(f"âš ï¸ íŒë¡€ ì „ë¬¸ ë¡œë”© ì‹¤íŒ¨ ({case_no}): {e}")
            return ""

    def _attach_source(self, docs: List[Document], source: str) -> List[Document]:
        """ê²€ìƒ‰ ì¶œì²˜(law/rule/case)ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì£¼ì…"""
        for d in docs:
            if d.metadata is None:
                d.metadata = {}
            d.metadata["__source_index"] = source
        return docs

    def _rerank(self, query: str, docs: List[Document]) -> Optional[List[Tuple[int, float]]]:
        """
        Cohere rerank ì‹¤í–‰
        Returns:
            [(doc_index, score), ...] (score desc)
            ì‹¤íŒ¨/ë¹„í™œì„± ì‹œ None
        """
        if not self._cohere_client:
            return None

        cfg = self.config
        # cohere ë¬¸ì„œ ì…ë ¥ ì¤€ë¹„ (ë„ˆë¬´ ê¸¸ë©´ truncation)
        texts = [_truncate(d.page_content or "", cfg.rerank_doc_max_chars) for d in docs]

        try:
            rerank_results = self._cohere_client.rerank(
                model=cfg.rerank_model,
                query=query,
                documents=texts,
                top_n=len(texts),
            )
            # ê²°ê³¼ëŠ” relevance_score ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì œê³µë˜ëŠ” ê²ƒì´ ì¼ë°˜ì 
            ranked = [(r.index, float(r.relevance_score)) for r in rerank_results.results]
            return ranked
        except Exception as e:
            logger.warning(f"âš ï¸ Rerank ì‹¤íŒ¨ (skip): {e}")
            return None

    def _cap_for_rerank(self, law: List[Document], rule: List[Document], case: List[Document]) -> List[Document]:
        """
        rerank ì…ë ¥ ë¬¸ì„œ ìˆ˜ë¥¼ ì œí•œ.
        - law/ruleì€ ê°€ëŠ¥í•œ í•œ ìœ ì§€
        - caseëŠ” ë‚¨ëŠ” ìŠ¬ë¡¯ë§Œí¼ ì±„ì›€
        """
        cfg = self.config
        law = _dedupe_docs(law, cfg.dedupe_key_fields)
        rule = _dedupe_docs(rule, cfg.dedupe_key_fields)
        case = _dedupe_docs(case, cfg.dedupe_key_fields)

        base = law + rule
        if len(base) >= cfg.rerank_max_documents:
            return base[: cfg.rerank_max_documents]

        remaining = cfg.rerank_max_documents - len(base)
        return base + case[:remaining]


    # ----------------------------
    # Dense + Sparse (BM25) candidate-level hybrid
    # ----------------------------
    def _search_dense_candidates(self, store: PineconeVectorStore, query: str, k: int) -> List[Document]:
        '''
        PineconeVectorStoreì—ì„œ dense ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³ , ê°€ëŠ¥í•œ ê²½ìš° scoreë¥¼ ë©”íƒ€ë°ì´í„°ì— ë‚¨ê¹ë‹ˆë‹¤.
        - score ìŠ¤ì¼€ì¼/ì˜ë¯¸(ê±°ë¦¬/ìœ ì‚¬ë„)ëŠ” êµ¬í˜„/ì¸ë±ìŠ¤ ì„¤ì •ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
          í›„ì† ê²°í•©ì€ 'ë­í¬ ê¸°ë°˜'(RRF/RankSum)ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        '''
        try:
            pairs = store.similarity_search_with_score(query, k=k)  # type: ignore[attr-defined]
            docs: List[Document] = []
            for rank, (doc, score) in enumerate(pairs, start=1):
                if doc.metadata is None:
                    doc.metadata = {}
                doc.metadata["__dense_score"] = float(score)
                doc.metadata["__dense_rank"] = int(rank)
                docs.append(doc)
            return docs
        except Exception:
            docs = store.similarity_search(query, k=k)
            for rank, doc in enumerate(docs, start=1):
                if doc.metadata is None:
                    doc.metadata = {}
                doc.metadata["__dense_rank"] = int(rank)
            return docs

    def _dense_sparse_fuse(self, query: str, docs: List[Document]) -> List[Document]:
        '''
        Dense ê²°ê³¼(ë­í¬) + BM25(sparse) ë­í¬ë¥¼ ê²°í•©í•˜ì—¬ í›„ë³´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¬ì •ë ¬í•©ë‹ˆë‹¤.
        - í›„ë³´ ìˆ˜ê°€ ë§ì§€ ì•Šì€ ìƒí™©(ë³´í†µ 10~80ê°œ)ì—ì„œ ë¹ ë¥´ê²Œ ë™ì‘í•©ë‹ˆë‹¤.
        - Pinecone ì¸ë±ìŠ¤ì— sparse vectorë¥¼ ë³„ë„ë¡œ ì €ì¥í•˜ì§€ ì•Šì•„ë„ ì ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
        '''
        cfg = self.config
        if not cfg.enable_bm25:
            return docs
        docs = _dedupe_docs(docs, cfg.dedupe_key_fields)
        if len(docs) <= 1:
            return docs

        # dense ranks: ë©”íƒ€ë°ì´í„°ì— ì—†ìœ¼ë©´ í˜„ì¬ ìˆœì„œë¥¼ ì‚¬ìš©
        dense_ranks: List[int] = []
        for i, d in enumerate(docs, start=1):
            if d.metadata is None:
                d.metadata = {}
            dense_ranks.append(int(d.metadata.get("__dense_rank", i)))

        # BM25 scoring on truncated doc text
        query_tokens = _default_tokenize(query)
        doc_texts = [_truncate(d.page_content or "", cfg.bm25_max_doc_chars) for d in docs]
        docs_tokens = [_default_tokenize(t) for t in doc_texts]
        bm25 = _bm25_scores(query_tokens, docs_tokens, k1=cfg.bm25_k1, b=cfg.bm25_b)

        # sparse ranks
        order_sparse = sorted(range(len(docs)), key=lambda i: bm25[i], reverse=True)
        sparse_ranks = [0] * len(docs)
        for r, i in enumerate(order_sparse, start=1):
            sparse_ranks[i] = r

        # attach sparse metadata
        for i, d in enumerate(docs):
            d.metadata["__bm25_score"] = float(bm25[i])
            d.metadata["__bm25_rank"] = int(sparse_ranks[i])

        fused = _rank_fusion(
            dense_ranks,
            sparse_ranks,
            mode=cfg.hybrid_fusion,
            w_dense=cfg.hybrid_dense_weight,
            w_sparse=cfg.hybrid_sparse_weight,
            rrf_k=cfg.rrf_k,
        )

        order = sorted(range(len(docs)), key=lambda i: fused[i], reverse=True)
        out: List[Document] = []
        for rank, i in enumerate(order, start=1):
            d = docs[i]
            d.metadata["__hybrid_score"] = float(fused[i])
            d.metadata["__hybrid_rank"] = int(rank)
            out.append(d)
        return out


    def triple_hybrid_retrieval(self, query: str) -> List[Document]:
        """
        Law/Rule/Case 3ì¤‘ ì¸ë±ìŠ¤ ê²€ìƒ‰ + ì„ íƒì  Rerank + 2-stage case í™•ì¥.
        Returns:
            ìµœì¢… Document ë¦¬ìŠ¤íŠ¸ (ë²•ì  ìœ„ê³„ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ì— ë°”ë¡œ ë„£ì„ ìˆ˜ ìˆëŠ” í˜•íƒœ)
        """
        cfg = self.config
        mult = cfg.search_multiplier

        logger.info(f"ğŸ” [í†µí•© ê²€ìƒ‰] query='{query}'")

        # 1) Retrieve (oversampling)
        docs_law = self._attach_source(
            self._search_dense_candidates(self.law_store, query, k=cfg.k_law * mult),
            "law",
        )
        docs_rule = self._attach_source(
            self._search_dense_candidates(self.rule_store, query, k=cfg.k_rule * mult),
            "rule",
        )
        # 2-stage: caseëŠ” ì²­í¬ í›„ë³´ë¥¼ ë„‰ë„‰íˆ í™•ë³´
        docs_case_chunks = self._attach_source(
            self._search_dense_candidates(self.case_store, query, k=cfg.case_candidate_k),
            "case",
        )

        # 1.5) Dense + Sparse(BM25) candidate-level hybrid re-ordering (per index)
        docs_law = self._dense_sparse_fuse(query, docs_law)
        docs_rule = self._dense_sparse_fuse(query, docs_rule)
        docs_case_chunks = self._dense_sparse_fuse(query, docs_case_chunks)

        # 2) Prepare rerank input (cap)
        combined_for_rerank = self._cap_for_rerank(docs_law, docs_rule, docs_case_chunks)

        # 3) Rerank (optional)
        selected_docs: List[Document]
        ranked = self._rerank(query, combined_for_rerank) if cfg.enable_rerank else None

        if ranked:
            # threshold filtering
            filtered = [(i, s) for (i, s) in ranked if s >= cfg.rerank_threshold]
            if not filtered:
                # fallback: take top few if threshold too strict
                desired = min(cfg.k_law + cfg.k_rule + cfg.k_case, len(ranked))
                filtered = ranked[:desired]

            # rerank order ìœ ì§€
            selected_docs = [combined_for_rerank[i] for (i, _s) in filtered]

            logger.info(f"ğŸ“Œ Rerank selected={len(selected_docs)} (threshold={cfg.rerank_threshold})")
        else:
            # no rerank: keep retrieval order
            selected_docs = combined_for_rerank

        # 4) Deduplicate (again)
        selected_docs = _dedupe_docs(selected_docs, cfg.dedupe_key_fields)

        # 5) Select top docs per source (law/rule) and top cases per case_no (2-stage expansion)
        law_ranked = [d for d in selected_docs if d.metadata.get("__source_index") == "law"]
        rule_ranked = [d for d in selected_docs if d.metadata.get("__source_index") == "rule"]
        case_ranked_chunks = [d for d in selected_docs if d.metadata.get("__source_index") == "case"]

        final_law = law_ranked[: cfg.k_law]
        final_rule = rule_ranked[: cfg.k_rule]

        # case: choose unique case_no in order, then expand only for top N
        top_n = cfg.case_expand_top_n if cfg.case_expand_top_n is not None else cfg.k_case
        seen_case_no = set()
        chosen_case_docs: List[Document] = []
        for d in case_ranked_chunks:
            case_no = d.metadata.get("case_no")
            if not case_no or case_no in seen_case_no:
                continue
            seen_case_no.add(case_no)
            chosen_case_docs.append(d)
            if len(chosen_case_docs) >= top_n:
                break

        expanded_cases: List[Document] = []
        for d in chosen_case_docs:
            case_no = d.metadata.get("case_no")
            if not case_no:
                continue
            full_text = self.get_full_case_context(str(case_no))
            if not full_text:
                # ì „ë¬¸ í™•ì¥ ì‹¤íŒ¨ ì‹œ ì²­í¬ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                expanded_cases.append(d)
                continue

            title = d.metadata.get("title") or d.metadata.get("case_name") or str(case_no)
            md = dict(d.metadata)
            md["__expanded"] = True
            # ë³´í†µ case priorityëŠ” 9ê°€ ê¸°ëŒ€ë˜ì§€ë§Œ, ì›ë³¸ ìœ ì§€
            expanded_cases.append(
                Document(
                    page_content=f"[íŒë¡€ ì „ë¬¸: {title}]\n{full_text}",
                    metadata=md,
                )
            )

        # k_case ì œí•œ(ì•ˆì „)
        final_case = expanded_cases[: cfg.k_case]

        # 6) Priority sort (ë²•ì  ìœ„ê³„)
        final_docs = final_law + final_rule + final_case
        final_docs = sorted(final_docs, key=lambda x: _safe_int((x.metadata or {}).get("priority", 99), 99))

        return final_docs

    # ----------------------------
    # Context formatting
    # ----------------------------
    @staticmethod
    def format_context_with_hierarchy(docs: List[Document]) -> str:
        """
        ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë²•ì  ìœ„ê³„(Priority)ì— ë”°ë¼ ì„¹ì…˜ë³„ë¡œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.
        """
        section_1_law: List[str] = []   # Priority 1, 2, 4, 5
        section_2_rule: List[str] = []  # Priority 3, 6, 7, 8, 11
        section_3_case: List[str] = []  # ê¸°íƒ€ (ì£¼ë¡œ íŒë¡€)

        for doc in docs:
            md = doc.metadata or {}
            p = _safe_int(md.get("priority", 99), 99)
            src = md.get("src_title", md.get("__source_index", "ìë£Œ"))
            title = md.get("title", "")
            content = doc.page_content or ""

            entry = f"[{src}] {title}\n{content}".strip()

            if p in (1, 2, 4, 5):
                section_1_law.append(entry)
            elif p in (3, 6, 7, 8, 11):
                section_2_rule.append(entry)
            else:
                section_3_case.append(entry)

        parts: List[str] = []
        if section_1_law:
            parts.append("## [SECTION 1: í•µì‹¬ ë²•ë ¹ (ìµœìš°ì„  ë²•ì  ê·¼ê±°)]\n" + "\n\n".join(section_1_law))
        if section_2_rule:
            parts.append("## [SECTION 2: ê´€ë ¨ ê·œì • ë° ì ˆì°¨ (ì„¸ë¶€ ê¸°ì¤€)]\n" + "\n\n".join(section_2_rule))
        if section_3_case:
            parts.append("## [SECTION 3: íŒë¡€ ë° í•´ì„ ì‚¬ë¡€ (ì ìš© ì˜ˆì‹œ)]\n" + "\n\n".join(section_3_case))

        return "\n\n".join(parts).strip()

    # ----------------------------
    # Answer generation
    # ----------------------------
    def generate_answer(self, user_input: str, *, skip_normalization: bool = False) -> str:
        """
        ìµœì¢… ë‹µë³€ ìƒì„±:
        (1) ì§ˆë¬¸ í‘œì¤€í™” (optional)
        (2) 3ì¤‘ ê²€ìƒ‰ + rerank + 2-stage íŒë¡€ í™•ì¥
        (3) ìœ„ê³„ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        (4) LLM ë‹µë³€ ìƒì„±
        """
        # 1) Normalize
        normalized_query = user_input if skip_normalization else self.normalize_query(user_input)
        if not skip_normalization:
            logger.info(f"ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: {normalized_query}")

        # 2) Retrieve
        retrieved_docs = self.triple_hybrid_retrieval(normalized_query)
        if not retrieved_docs:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë²•ë ¹ì´ë‚˜ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # 3) Context
        hierarchical_context = self.format_context_with_hierarchy(retrieved_docs)

        # 4) Generate
        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", SYSTEM_PROMPT),
                ("human", "{question}"),
            ]
        )
        chain = prompt | self._generation_llm | StrOutputParser()

        logger.info("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...")
        try:
            return str(chain.invoke({"context": hierarchical_context, "question": normalized_query})).strip()
        except Exception as e:
            logger.warning(f"âš ï¸ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


__all__ = [
    "RAGConfig",
    "RAGPipeline",
    "INDEX_NAMES",
    "KEYWORD_DICT",
    "NORMALIZATION_PROMPT",
    "SYSTEM_PROMPT",
]
