"""
Hybrid RAG Module - Dense (Solar) + Sparse (BM25) í†µí•© ê²€ìƒ‰

ì£¼íƒì„ëŒ€ì°¨ RAG ì‹œìŠ¤í…œ - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ëª¨ë“ˆ

[í•µì‹¬ ê°œì„ ì‚¬í•­]
- Dense ê²€ìƒ‰ (Solar/Cohere Embedding via Pinecone)
- Sparse ê²€ìƒ‰ (BM25 with Korean tokenization)
- Reciprocal Rank Fusion (RRF) ë˜ëŠ” Weighted Score ê²°í•©
- 2-stage case expansion ìœ ì§€
- Cohere Rerank ì„ íƒì  ì ìš©

[í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì „ëµ]
1. Dense: Pinecone VectorStoreì—ì„œ ì‹œë§¨í‹± ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰
2. Sparse: BM25ë¡œ í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ ê²€ìƒ‰ (ê²€ìƒ‰ëœ ë¬¸ì„œ í’€ì—ì„œ ì¬ìˆœìœ„í™”)
3. Fusion: RRF ë˜ëŠ” ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ë‘ ê²°ê³¼ ê²°í•©
4. Rerank: (ì„ íƒ) Cohereë¡œ ìµœì¢… ê´€ë ¨ë„ ê¸°ë°˜ ì¬ìˆœìœ„í™”

[ì˜ì¡´ì„±]
- rank_bm25: BM25 ìŠ¤ì½”ì–´ë§
- kiwipiepy (ì„ íƒ): í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ (ì—†ìœ¼ë©´ ê³µë°± í† í°í™”)
"""

from __future__ import annotations

import logging
import os
import re
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import (
    Dict, List, Optional, Sequence, Tuple, Iterable, 
    Callable, Protocol, Union, Any
)

from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.chat_models import ChatOllama
from langchain_upstage import UpstageEmbeddings
from langchain_pinecone import PineconeVectorStore

# BM25
try:
    from rank_bm25 import BM25Okapi, BM25Plus
    BM25_AVAILABLE = True
except ImportError:
    BM25_AVAILABLE = False
    BM25Okapi = None
    BM25Plus = None

# Korean tokenizer (optional)
try:
    from kiwipiepy import Kiwi
    KIWI_AVAILABLE = True
except ImportError:
    KIWI_AVAILABLE = False
    Kiwi = None

# Cohere Rerank (optional)
try:
    import cohere
    COHERE_AVAILABLE = True
except ImportError:
    cohere = None
    COHERE_AVAILABLE = False

# --------------------------------------------------------------------------------------
# Logging
# --------------------------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# --------------------------------------------------------------------------------------
# Index names
# --------------------------------------------------------------------------------------
INDEX_NAMES: Dict[str, str] = {
    "law": "law-index-final",
    "rule": "rule-index-final",
    "case": "case-index-final",
}

# --------------------------------------------------------------------------------------
# Keyword dictionary (query normalization)
# --------------------------------------------------------------------------------------
KEYWORD_DICT: Dict[str, str] = {
    # 1. ê³„ì•½ ì£¼ì²´ ë° ëŒ€ìƒ
    "ì§‘ì£¼ì¸": "ì„ëŒ€ì¸", "ê±´ë¬¼ì£¼": "ì„ëŒ€ì¸", "ì£¼ì¸ì§‘": "ì„ëŒ€ì¸",
    "ì„ëŒ€ì—…ì": "ì„ëŒ€ì¸", "ìƒˆì£¼ì¸": "ì„ëŒ€ì¸",
    "ì„¸ì…ì": "ì„ì°¨ì¸", "ì›”ì„¸ì…ì": "ì„ì°¨ì¸", "ì„¸ë“¤ì–´ì‚¬ëŠ”ì‚¬ëŒ": "ì„ì°¨ì¸",
    "ì„ì°¨ì": "ì„ì°¨ì¸", "ì…ì£¼ì": "ì„ì°¨ì¸",
    "ë¶€ë™ì‚°": "ê³µì¸ì¤‘ê°œì‚¬", "ì¤‘ê°œì¸": "ê³µì¸ì¤‘ê°œì‚¬", "ì¤‘ê°œì†Œ": "ê³µì¸ì¤‘ê°œì‚¬",
    "ë¹Œë¼": "ì„ì°¨ì£¼íƒ", "ì•„íŒŒíŠ¸": "ì„ì°¨ì£¼íƒ", "ì˜¤í”¼ìŠ¤í…”": "ì„ì°¨ì£¼íƒ",
    "ìš°ë¦¬ì§‘": "ì„ì°¨ì£¼íƒ", "ê±°ì£¼ì§€": "ì„ì°¨ì£¼íƒ",
    "ê³„ì•½ì„œ": "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ", "ì§‘ë¬¸ì„œ": "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ", "ì¢…ì´": "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ",

    # 2. ë³´ì¦ê¸ˆ ë° ê¸ˆì „
    "ë³´ì¦ê¸ˆ": "ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ", "ì „ì„¸ê¸ˆ": "ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ", "ë³´ì¦ë³´í—˜": "ë³´ì¦ê¸ˆë°˜í™˜ë³´ì¦",
    "ëˆëª»ë°›ìŒ": "ë³´ì¦ê¸ˆë¯¸ë°˜í™˜", "ì•ˆëŒë ¤ì¤Œ": "ë³´ì¦ê¸ˆë¯¸ë°˜í™˜", "ëª»ëŒë ¤ë°›ìŒ": "ë³´ì¦ê¸ˆë¯¸ë°˜í™˜",
    "ì›”ì„¸": "ì°¨ì„", "ê´€ë¦¬ë¹„": "ê´€ë¦¬ë¹„", "ì—°ì²´": "ì°¨ì„ì—°ì²´", "ë°€ë¦¼": "ì°¨ì„ì—°ì²´",
    "ë³µë¹„": "ì¤‘ê°œë³´ìˆ˜", "ìˆ˜ìˆ˜ë£Œ": "ì¤‘ê°œë³´ìˆ˜", "ì¤‘ê°œë¹„": "ì¤‘ê°œë³´ìˆ˜",
    "ì›”ì„¸ì˜¬ë¦¬ê¸°": "ì°¨ì„ì¦ì•¡", "ì¸ìƒ": "ì¦ì•¡", "ë”ë‹¬ë¼ê³ í•¨": "ì¦ì•¡",
    "ì›”ì„¸ê¹ê¸°": "ì°¨ì„ê°ì•¡", "í• ì¸": "ê°ì•¡", "ë‚´ë¦¬ê¸°": "ê°ì•¡",
    "ëˆë¨¼ì €ë°›ê¸°": "ìš°ì„ ë³€ì œê¶Œ", "ìˆœìœ„": "ìš°ì„ ë³€ì œê¶Œ", "ì•ˆì „ì¥ì¹˜": "ëŒ€í•­ë ¥",
    "ëŒë ¤ë°›ê¸°": "ë³´ì¦ê¸ˆë°˜í™˜",

    # 3. ê¸°ê°„ ë° ì¢…ë£Œ/ê°±ì‹ 
    "ì¬ê³„ì•½": "ê³„ì•½ê°±ì‹ ", "ì—°ì¥": "ê³„ì•½ê°±ì‹ ", "ê°±ì‹ ": "ê³„ì•½ê°±ì‹ ",
    "ê°±ì‹ ì²­êµ¬": "ê³„ì•½ê°±ì‹ ìš”êµ¬ê¶Œ", "2ë…„ë”": "ê³„ì•½ê°±ì‹ ìš”êµ¬ê¶Œ", "2í”ŒëŸ¬ìŠ¤2": "ê³„ì•½ê°±ì‹ ìš”êµ¬ê¶Œ",
    "ìë™ì—°ì¥": "ë¬µì‹œì ê°±ì‹ ", "ë¬µì‹œ": "ë¬µì‹œì ê°±ì‹ ", "ì—°ë½ì—†ìŒ": "ë¬µì‹œì ê°±ì‹ ",
    "ì´ì‚¬": "ì£¼íƒì˜ì¸ë„", "ì§ë¹¼ê¸°": "ì£¼íƒì˜ì¸ë„", "í‡´ê±°": "ì£¼íƒì˜ì¸ë„",
    "ë°©ë¹¼": "ê³„ì•½í•´ì§€", "ì¤‘ë„í•´ì§€": "ê³„ì•½í•´ì§€",
    "ì£¼ì†Œì˜®ê¸°ê¸°": "ì£¼ë¯¼ë“±ë¡", "ì „ì…ì‹ ê³ ": "ì£¼ë¯¼ë“±ë¡", "ì£¼ì†Œì§€ì´ì „": "ì£¼ë¯¼ë“±ë¡",
    "ì§‘ì£¼ì¸ë°”ë€œ": "ì„ëŒ€ì¸ì§€ìœ„ìŠ¹ê³„", "ì£¼ì¸ë°”ë€œ": "ì„ëŒ€ì¸ì§€ìœ„ìŠ¹ê³„",
    "ë§¤ë§¤": "ì„ëŒ€ì¸ì§€ìœ„ìŠ¹ê³„",
    "ë‚˜ê°€ë¼ê³ í•¨": "ê³„ì•½ê°±ì‹ ê±°ì ˆ", "ì«“ê²¨ë‚¨": "ëª…ë„", "ë¹„ì›Œë‹¬ë¼": "ëª…ë„",

    # 4. ìˆ˜ë¦¬ ë° ìƒí™œí™˜ê²½
    "ì§‘ê³ ì¹˜ê¸°": "ìˆ˜ì„ ì˜ë¬´", "ìˆ˜ë¦¬": "ìˆ˜ì„ ì˜ë¬´", "ê³ ì³ì¤˜": "ìˆ˜ì„ ì˜ë¬´",
    "ì•ˆê³ ì³ì¤Œ": "ìˆ˜ì„ ì˜ë¬´ìœ„ë°˜",
    "ê³°íŒ¡ì´": "í•˜ì", "ë¬¼ìƒ˜": "ëˆ„ìˆ˜", "ë³´ì¼ëŸ¬ê³ ì¥": "í•˜ì", "íŒŒì†": "í›¼ì†",
    "ê¹¨ë—ì´ì¹˜ìš°ê¸°": "ì›ìƒíšŒë³µì˜ë¬´", "ì›ë˜ëŒ€ë¡œí•´ë†“ê¸°": "ì›ìƒíšŒë³µ",
    "ì²­ì†Œë¹„": "ì›ìƒíšŒë³µë¹„ìš©", "ì²­ì†Œ": "ì›ìƒíšŒë³µ",
    "ì¸µê°„ì†ŒìŒ": "ê³µë™ìƒí™œí‰ì˜¨", "ì˜†ì§‘ì†ŒìŒ": "ë°©ìŒ", "ê°œí‚¤ìš°ê¸°": "ë°˜ë ¤ë™ë¬¼íŠ¹ì•½",
    "ë‹´ë°°": "í¡ì—°ê¸ˆì§€íŠ¹ì•½",

    # 5. ê¶Œë¦¬/ëŒ€í•­ë ¥/í™•ì •ì¼ì
    "í™•ì •ì¼ì": "í™•ì •ì¼ì", "ì „ì…": "ì£¼ë¯¼ë“±ë¡", "ëŒ€í•­ë ¥": "ëŒ€í•­ë ¥",
    "ìš°ì„ ë³€ì œ": "ìš°ì„ ë³€ì œê¶Œ", "ìµœìš°ì„ ": "ìµœìš°ì„ ë³€ì œê¶Œ",
    "ê²½ë§¤": "ê²½ë§¤ì ˆì°¨", "ê³µë§¤": "ê³µë§¤ì ˆì°¨",
    "ë“±ê¸°": "ë“±ê¸°ë¶€ë“±ë³¸", "ë“±ë³¸": "ë“±ê¸°ë¶€ë“±ë³¸",
    "ê·¼ì €ë‹¹": "ê·¼ì €ë‹¹ê¶Œ", "ê°€ì••ë¥˜": "ê°€ì••ë¥˜", "ê°€ì²˜ë¶„": "ê°€ì²˜ë¶„",
    "ê¹¡í†µì „ì„¸": "ì „ì„¸í”¼í•´", "ì‚¬ê¸°": "ì „ì„¸ì‚¬ê¸°", "ê²½ë§¤ë„˜ì–´ê°": "ê¶Œë¦¬ë¦¬ìŠ¤í¬",

    # 6. ë¶„ìŸ í•´ê²°
    "ë‚´ìš©ì¦ëª…": "ë‚´ìš©ì¦ëª…", "ì†Œì†¡": "ì†Œì†¡", "ë¯¼ì‚¬": "ë¯¼ì‚¬ì†Œì†¡",
    "ì¡°ì •ìœ„": "ì£¼íƒì„ëŒ€ì°¨ë¶„ìŸì¡°ì •ìœ„ì›íšŒ", "ì†Œì†¡ë§ê³ ": "ë¶„ìŸì¡°ì •",
    "ë²•ì›ê°€ê¸°ì‹«ìŒ": "ë¶„ìŸì¡°ì •",
    "ì§‘ì£¼ì¸ì‚¬ë§": "ì„ì°¨ê¶ŒìŠ¹ê³„", "ìì‹ìƒì†": "ì„ì°¨ê¶ŒìŠ¹ê³„",
    "íŠ¹ì•½": "íŠ¹ì•½ì‚¬í•­", "ë¶ˆê³µì •": "ê°•í–‰ê·œì •ìœ„ë°˜", "ë…ì†Œì¡°í•­": "ë¶ˆë¦¬í•œì•½ì •",
    "íš¨ë ¥ìˆë‚˜": "ë¬´íš¨ì—¬ë¶€",
}

# --------------------------------------------------------------------------------------
# Prompts
# --------------------------------------------------------------------------------------
NORMALIZATION_PROMPT: str = """
ë‹¹ì‹ ì€ ë²•ë¥  AI ì±—ë´‡ì˜ ì „ì²˜ë¦¬ ë‹´ë‹¹ìì…ë‹ˆë‹¤.
ì•„ë˜ [ìš©ì–´ ì‚¬ì „]ì„ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ 'ë²•ë¥  í‘œì¤€ì–´'ë¡œ ë³€í™˜í•´ ì£¼ì„¸ìš”.

[ìˆ˜í–‰ ì§€ì¹¨]
1. ì‚¬ì „ì— ìˆëŠ” ë‹¨ì–´ëŠ” ë°˜ë“œì‹œ ë§¤í•‘ëœ ë²•ë¥  ìš©ì–´ë¡œ ë³€ê²½í•˜ì„¸ìš”.
2. ë‹¨ì–´ë¥¼ ë³€ê²½í•  ë•Œ ë¬¸ë§¥ì— ë§ê²Œ ì¡°ì‚¬(ì´/ê°€, ì„/ë¥¼ ë“±)ë‚˜ ì„œìˆ ì–´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ìˆ˜ì •í•˜ì„¸ìš”.
3. ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ì™œê³¡í•˜ê±°ë‚˜ ì¶”ê°€ì ì¸ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.
4. ì˜¤ì§ 'ë³€ê²½ëœ ì§ˆë¬¸' í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”. (ì„¤ëª… ê¸ˆì§€)

[ìš©ì–´ ì‚¬ì „]
{dictionary}

ì‚¬ìš©ì ì§ˆë¬¸: {question}
ë³€ê²½ëœ ì§ˆë¬¸:
"""

SYSTEM_PROMPT: str = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ 'ì£¼íƒ ì „ì›”ì„¸ ì‚¬ê¸° ì˜ˆë°© ë° ì„ëŒ€ì°¨ ë²•ë¥  ì „ë¬¸ê°€ AI'ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ [ë²•ì  ìœ„ê³„ê°€ ì •ë¦¬ëœ ì°¸ê³  ë¬¸ì„œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

[ë‹µë³€ ìƒì„± ì›ì¹™]
1. **ë²•ì  ìœ„ê³„ ì¤€ìˆ˜**:
   - ë°˜ë“œì‹œ [SECTION 1: í•µì‹¬ ë²•ë ¹]ì˜ ë‚´ìš©ì„ ìµœìš°ì„  íŒë‹¨ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ìœ¼ì„¸ìš”.
   - [SECTION 1]ì˜ ë‚´ìš©ì´ ëª¨í˜¸í•  ë•Œë§Œ [SECTION 2]ì™€ [SECTION 3]ë¥¼ ë³´ì¶© ê·¼ê±°ë¡œ í™œìš©í•˜ì„¸ìš”.
   - ë§Œì•½ [SECTION 3: íŒë¡€]ê°€ [SECTION 1: ë²•ë ¹]ê³¼ ë‹¤ë¥´ê²Œ í•´ì„ë˜ëŠ” íŠ¹ìˆ˜í•œ ê²½ìš°ë¼ë©´,
     "ì›ì¹™ì€ ë²•ë ¹ì— ë”°ë¥´ë‚˜, íŒë¡€ëŠ” ì˜ˆì™¸ì ìœ¼ë¡œ..."ë¼ê³  ì„¤ëª…í•˜ì„¸ìš”.

2. **ë‹µë³€ êµ¬ì¡°**:
   - **í•µì‹¬ ê²°ë¡ **: ì§ˆë¬¸ì— ëŒ€í•œ ê²°ë¡ (ê°€ëŠ¥/ë¶ˆê°€ëŠ¥/ìœ íš¨/ë¬´íš¨)ì„ ë‘ê´„ì‹ìœ¼ë¡œ ìš”ì•½.
   - **ë²•ì  ê·¼ê±°**: "ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì œOì¡°ì— ë”°ë¥´ë©´..." (SECTION 1 ì¸ìš©)
   - **ì‹¤ë¬´ ì ˆì°¨**: í•„ìš”ì‹œ ì‹ ê³  ë°©ë²•, ì„œë¥˜ ë“± ì•ˆë‚´ (SECTION 2 ì¸ìš©)
   - **ì°¸ê³  ì‚¬ë¡€**: ìœ ì‚¬í•œ ìƒí™©ì—ì„œì˜ íŒê²°ì´ë‚˜ í•´ì„ (SECTION 3 ì¸ìš©)

3. **ì£¼ì˜ì‚¬í•­**:
   - ì‚¬ìš©ìì˜ ê³„ì•½ì„œ ë‚´ìš©ì´ ë²•ë ¹(ê°•í–‰ê·œì •)ì— ìœ„ë°˜ë˜ë©´ "íš¨ë ¥ì´ ì—†ë‹¤(ë¬´íš¨)"ê³  ëª…í™•íˆ ê²½ê³ í•˜ì„¸ìš”.
   - ë²•ë¥ ì  ì¡°ì–¸ì¼ ë¿ì´ë¯€ë¡œ, ìµœì¢…ì ìœ¼ë¡œëŠ” ë³€í˜¸ì‚¬ ë“±ì˜ ì „ë¬¸ê°€ í™•ì¸ì´ í•„ìš”í•¨ì„ ê³ ì§€í•˜ì„¸ìš”.

[ë²•ì  ìœ„ê³„ê°€ ì •ë¦¬ëœ ì°¸ê³  ë¬¸ì„œ]
{context}
"""

# --------------------------------------------------------------------------------------
# Utilities
# --------------------------------------------------------------------------------------
def _safe_int(x: object, default: int = 99) -> int:
    try:
        return int(x)  # type: ignore[arg-type]
    except Exception:
        return default


def _truncate(text: str, max_chars: int) -> str:
    if text is None:
        return ""
    if len(text) <= max_chars:
        return text
    return text[: max_chars - 1] + "â€¦"


def _dedupe_docs(
    docs: Iterable[Document],
    key_fields: Sequence[str] = ("chunk_id", "id"),
) -> List[Document]:
    """ë©”íƒ€ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì¤‘ë³µ ì œê±°"""
    seen: set = set()
    out: List[Document] = []
    for d in docs:
        md = d.metadata or {}
        key = None
        for f in key_fields:
            v = md.get(f)
            if v:
                key = f"{f}:{v}"
                break
        if key is None:
            key = f"content:{hash(d.page_content)}"
        if key in seen:
            continue
        seen.add(key)
        out.append(d)
    return out


# --------------------------------------------------------------------------------------
# Korean Tokenizer
# --------------------------------------------------------------------------------------
class Tokenizer(ABC):
    """í† í¬ë‚˜ì´ì € ì¶”ìƒ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def tokenize(self, text: str) -> List[str]:
        pass


class SimpleTokenizer(Tokenizer):
    """ê³µë°± ê¸°ë°˜ ë‹¨ìˆœ í† í¬ë‚˜ì´ì € (fallback)"""
    
    def __init__(self, min_length: int = 1):
        self.min_length = min_length
        # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ì¶”ì¶œ
        self._pattern = re.compile(r'[ê°€-í£a-zA-Z0-9]+')
    
    def tokenize(self, text: str) -> List[str]:
        if not text:
            return []
        tokens = self._pattern.findall(text.lower())
        return [t for t in tokens if len(t) >= self.min_length]


class KiwiTokenizer(Tokenizer):
    """Kiwi ê¸°ë°˜ í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ í† í¬ë‚˜ì´ì €"""
    
    def __init__(
        self, 
        pos_tags: Optional[Tuple[str, ...]] = None,
        min_length: int = 1
    ):
        if not KIWI_AVAILABLE:
            raise ImportError("kiwipiepyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install kiwipiepy")
        
        self._kiwi = Kiwi()
        # ê¸°ë³¸: ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ì™¸ë˜ì–´/í•œì
        self.pos_tags = pos_tags or ('NNG', 'NNP', 'VV', 'VA', 'SL', 'SH')
        self.min_length = min_length
    
    def tokenize(self, text: str) -> List[str]:
        if not text:
            return []
        
        tokens = []
        result = self._kiwi.tokenize(text)
        for token in result:
            if token.tag in self.pos_tags and len(token.form) >= self.min_length:
                tokens.append(token.form.lower())
        return tokens


def get_default_tokenizer() -> Tokenizer:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ìµœì ì˜ í† í¬ë‚˜ì´ì € ë°˜í™˜"""
    if KIWI_AVAILABLE:
        logger.info("âœ… Kiwi í† í¬ë‚˜ì´ì € ì‚¬ìš© (í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„)")
        return KiwiTokenizer()
    else:
        logger.info("â„¹ï¸ SimpleTokenizer ì‚¬ìš© (ê³µë°± ê¸°ë°˜)")
        return SimpleTokenizer()


# --------------------------------------------------------------------------------------
# BM25 Scorer
# --------------------------------------------------------------------------------------
class BM25Scorer:
    """
    BM25 ê¸°ë°˜ ë¬¸ì„œ ìŠ¤ì½”ì–´ë§
    
    ê²€ìƒ‰ëœ ë¬¸ì„œ í’€ì—ì„œ ì¿¼ë¦¬ì™€ì˜ BM25 ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        tokenizer: Optional[Tokenizer] = None,
        algorithm: str = "okapi",  # "okapi" or "plus"
        k1: float = 1.5,
        b: float = 0.75,
    ):
        if not BM25_AVAILABLE:
            raise ImportError("rank_bm25ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install rank-bm25")
        
        self.tokenizer = tokenizer or get_default_tokenizer()
        self.algorithm = algorithm
        self.k1 = k1
        self.b = b
        
        self._bm25: Optional[Any] = None
        self._corpus_tokens: List[List[str]] = []
    
    def fit(self, documents: List[Document]) -> "BM25Scorer":
        """ë¬¸ì„œ ì½”í¼ìŠ¤ë¡œ BM25 ì¸ë±ìŠ¤ êµ¬ì¶•"""
        self._corpus_tokens = [
            self.tokenizer.tokenize(doc.page_content or "")
            for doc in documents
        ]
        
        BM25Class = BM25Plus if self.algorithm == "plus" else BM25Okapi
        self._bm25 = BM25Class(self._corpus_tokens, k1=self.k1, b=self.b)
        
        return self
    
    def score(self, query: str) -> List[float]:
        """ì¿¼ë¦¬ì— ëŒ€í•œ ê° ë¬¸ì„œì˜ BM25 ì ìˆ˜ ë°˜í™˜"""
        if self._bm25 is None:
            raise RuntimeError("fit()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”")
        
        query_tokens = self.tokenizer.tokenize(query)
        scores = self._bm25.get_scores(query_tokens)
        return scores.tolist()
    
    def get_top_k(
        self, 
        query: str, 
        documents: List[Document], 
        k: int
    ) -> List[Tuple[Document, float]]:
        """ìƒìœ„ kê°œ ë¬¸ì„œì™€ ì ìˆ˜ ë°˜í™˜"""
        self.fit(documents)
        scores = self.score(query)
        
        # (document, score) ìŒìœ¼ë¡œ ì •ë ¬
        doc_scores = list(zip(documents, scores))
        doc_scores.sort(key=lambda x: x[1], reverse=True)
        
        return doc_scores[:k]


# --------------------------------------------------------------------------------------
# Hybrid Score Fusion
# --------------------------------------------------------------------------------------
class ScoreFusion:
    """Denseì™€ Sparse ì ìˆ˜ë¥¼ ê²°í•©í•˜ëŠ” ì „ëµ"""
    
    @staticmethod
    def reciprocal_rank_fusion(
        dense_ranks: Dict[str, int],  # doc_id -> rank (1-indexed)
        sparse_ranks: Dict[str, int],
        k: int = 60,
    ) -> Dict[str, float]:
        """
        Reciprocal Rank Fusion (RRF)
        
        RRF Score = Î£ 1/(k + rank)
        
        Args:
            dense_ranks: Dense ê²€ìƒ‰ ê²°ê³¼ì˜ ìˆœìœ„
            sparse_ranks: Sparse ê²€ìƒ‰ ê²°ê³¼ì˜ ìˆœìœ„
            k: RRF ìƒìˆ˜ (ê¸°ë³¸ê°’ 60)
        
        Returns:
            doc_id -> RRF score
        """
        all_docs = set(dense_ranks.keys()) | set(sparse_ranks.keys())
        scores: Dict[str, float] = {}
        
        for doc_id in all_docs:
            score = 0.0
            if doc_id in dense_ranks:
                score += 1.0 / (k + dense_ranks[doc_id])
            if doc_id in sparse_ranks:
                score += 1.0 / (k + sparse_ranks[doc_id])
            scores[doc_id] = score
        
        return scores
    
    @staticmethod
    def weighted_sum(
        dense_scores: Dict[str, float],
        sparse_scores: Dict[str, float],
        alpha: float = 0.5,
        normalize: bool = True,
    ) -> Dict[str, float]:
        """
        ê°€ì¤‘ í•©ì‚°
        
        Final Score = alpha * dense_score + (1-alpha) * sparse_score
        
        Args:
            dense_scores: Dense ê²€ìƒ‰ ì ìˆ˜ (ì •ê·œí™” ê¶Œì¥)
            sparse_scores: Sparse ê²€ìƒ‰ ì ìˆ˜ (ì •ê·œí™” ê¶Œì¥)
            alpha: Dense ê°€ì¤‘ì¹˜ (0~1)
            normalize: ì ìˆ˜ ì •ê·œí™” ì—¬ë¶€
        """
        if normalize:
            dense_scores = ScoreFusion._normalize(dense_scores)
            sparse_scores = ScoreFusion._normalize(sparse_scores)
        
        all_docs = set(dense_scores.keys()) | set(sparse_scores.keys())
        scores: Dict[str, float] = {}
        
        for doc_id in all_docs:
            d_score = dense_scores.get(doc_id, 0.0)
            s_score = sparse_scores.get(doc_id, 0.0)
            scores[doc_id] = alpha * d_score + (1 - alpha) * s_score
        
        return scores
    
    @staticmethod
    def _normalize(scores: Dict[str, float]) -> Dict[str, float]:
        """Min-Max ì •ê·œí™”"""
        if not scores:
            return scores
        
        values = list(scores.values())
        min_val, max_val = min(values), max(values)
        
        if max_val == min_val:
            return {k: 1.0 for k in scores}
        
        return {
            k: (v - min_val) / (max_val - min_val)
            for k, v in scores.items()
        }


# --------------------------------------------------------------------------------------
# Config
# --------------------------------------------------------------------------------------
@dataclass
class RAGConfig:
    """RAG íŒŒì´í”„ë¼ì¸ ì„¤ì •"""
    
    # LLM
    llm_model: str = "exaone3.5:2.4b"
    temperature: float = 0.1
    normalize_temperature: float = 0.0

    # Embedding
    embedding_model: str = "solar-embedding-1-large-passage"

    # Retrieval sizes (final target)
    k_law: int = 5
    k_rule: int = 5
    k_case: int = 3

    # Oversampling before fusion/rerank
    search_multiplier: int = 2

    # ============ Hybrid Search Settings ============
    enable_hybrid: bool = True
    hybrid_method: str = "rrf"  # "rrf" or "weighted"
    hybrid_alpha: float = 0.5   # Dense ê°€ì¤‘ì¹˜ (weighted ë°©ì‹ì—ì„œ ì‚¬ìš©)
    rrf_k: int = 60             # RRF ìƒìˆ˜
    
    # BM25 Settings
    bm25_algorithm: str = "okapi"  # "okapi" or "plus"
    bm25_k1: float = 1.5
    bm25_b: float = 0.75
    use_kiwi_tokenizer: bool = True  # Falseë©´ SimpleTokenizer ì‚¬ìš©
    
    # ============ Rerank Settings ============
    enable_rerank: bool = True
    rerank_threshold: float = 0.2
    rerank_model: str = "rerank-multilingual-v3.0"
    rerank_max_documents: int = 80
    rerank_doc_max_chars: int = 2000

    # 2-stage case expansion
    case_candidate_k: int = 40
    case_expand_top_n: Optional[int] = None
    case_context_top_k: int = 50

    # Deduping
    dedupe_key_fields: Tuple[str, ...] = ("chunk_id", "id")

    def __post_init__(self) -> None:
        if not (0 <= self.temperature <= 2):
            raise ValueError("temperatureëŠ” 0~2 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        if not (0 <= self.rerank_threshold <= 1):
            raise ValueError("rerank_thresholdëŠ” 0~1 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        if not (0 <= self.hybrid_alpha <= 1):
            raise ValueError("hybrid_alphaëŠ” 0~1 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        if self.hybrid_method not in ("rrf", "weighted"):
            raise ValueError("hybrid_methodëŠ” 'rrf' ë˜ëŠ” 'weighted'ì—¬ì•¼ í•©ë‹ˆë‹¤.")


# --------------------------------------------------------------------------------------
# Pipeline
# --------------------------------------------------------------------------------------
class RAGPipeline:
    """
    Hybrid RAG Pipeline - Dense (Solar) + Sparse (BM25)
    
    Usage:
        # ê¸°ë³¸ (í•˜ì´ë¸Œë¦¬ë“œ í™œì„±í™”)
        pipeline = RAGPipeline()
        answer = pipeline.generate_answer("ë³´ì¦ê¸ˆì„ ëª» ëŒë ¤ë°›ì•˜ì–´ìš”")
        
        # Denseë§Œ ì‚¬ìš©
        config = RAGConfig(enable_hybrid=False)
        pipeline = RAGPipeline(config)
        
        # ê°€ì¤‘ í•©ì‚° ë°©ì‹
        config = RAGConfig(hybrid_method="weighted", hybrid_alpha=0.7)
        pipeline = RAGPipeline(config)
    """

    def __init__(
        self,
        config: Optional[RAGConfig] = None,
        *,
        pc_api_key: Optional[str] = None,
        cohere_api_key: Optional[str] = None,
        embedding: Optional[object] = None,
        tokenizer: Optional[Tokenizer] = None,
    ) -> None:
        self.config = config or RAGConfig()

        self._pc_api_key = pc_api_key or os.getenv("PINECONE_API_KEY")
        self._cohere_api_key = cohere_api_key or os.getenv("COHERE_API_KEY")

        if not self._pc_api_key:
            raise ValueError(
                "Pinecone API keyê°€ í•„ìš”í•©ë‹ˆë‹¤. "
                "pc_api_key ì¸ì ë˜ëŠ” PINECONE_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”."
            )

        # Embeddings
        if embedding is not None:
            self._embedding = embedding
        else:
            self._embedding = UpstageEmbeddings(
                model=self.config.embedding_model
            )

        # Vector stores
        logger.info("ğŸ”— Pinecone 3ì¤‘ ì¸ë±ìŠ¤ ì—°ê²° ì¤‘...")
        self._law_store = PineconeVectorStore(
            index_name=INDEX_NAMES["law"],
            embedding=self._embedding,
            pinecone_api_key=self._pc_api_key,
        )
        self._rule_store = PineconeVectorStore(
            index_name=INDEX_NAMES["rule"],
            embedding=self._embedding,
            pinecone_api_key=self._pc_api_key,
        )
        self._case_store = PineconeVectorStore(
            index_name=INDEX_NAMES["case"],
            embedding=self._embedding,
            pinecone_api_key=self._pc_api_key,
        )
        logger.info("âœ… [Law / Rule / Case] 3ê°œ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ!")

        # LLM instances
        self._normalize_llm = ChatOllama(
            model=self.config.llm_model,
            temperature=self.config.normalize_temperature,
        )
        self._generation_llm = ChatOllama(
            model=self.config.llm_model,
            temperature=self.config.temperature,
        )

        # Tokenizer for BM25
        self._tokenizer: Optional[Tokenizer] = None
        if self.config.enable_hybrid:
            if tokenizer is not None:
                self._tokenizer = tokenizer
            elif self.config.use_kiwi_tokenizer and KIWI_AVAILABLE:
                self._tokenizer = KiwiTokenizer()
                logger.info("âœ… Kiwi í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                self._tokenizer = SimpleTokenizer()
                logger.info("â„¹ï¸ SimpleTokenizer ì‚¬ìš© (ê³µë°± ê¸°ë°˜)")
            
            if not BM25_AVAILABLE:
                logger.warning(
                    "âš ï¸ rank_bm25ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤. "
                    "ì„¤ì¹˜: pip install rank-bm25"
                )
                self.config.enable_hybrid = False

        # Cohere client for rerank
        self._cohere_client: Optional[Any] = None
        if self.config.enable_rerank:
            if not COHERE_AVAILABLE:
                logger.warning("âš ï¸ cohere íŒ¨í‚¤ì§€ê°€ ì—†ì–´ rerankë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
            elif not self._cohere_api_key:
                logger.warning("âš ï¸ COHERE_API_KEYê°€ ì—†ì–´ rerankë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
            else:
                self._cohere_client = cohere.Client(self._cohere_api_key)
                logger.info("âœ… Cohere Reranking í™œì„±í™”")

    # ----------------------------
    # Properties
    # ----------------------------
    @property
    def law_store(self) -> PineconeVectorStore:
        return self._law_store

    @property
    def rule_store(self) -> PineconeVectorStore:
        return self._rule_store

    @property
    def case_store(self) -> PineconeVectorStore:
        return self._case_store

    # ----------------------------
    # Core methods
    # ----------------------------
    def normalize_query(self, user_query: str) -> str:
        """ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²•ë¥  ìš©ì–´ë¡œ í‘œì¤€í™”"""
        prompt = ChatPromptTemplate.from_template(NORMALIZATION_PROMPT)
        chain = prompt | self._normalize_llm | StrOutputParser()

        try:
            normalized = chain.invoke({
                "dictionary": KEYWORD_DICT, 
                "question": user_query
            })
            return str(normalized).strip()
        except Exception as e:
            logger.warning(f"âš ï¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨ (ì›ë³¸ ì‚¬ìš©): {e}")
            return user_query

    def get_full_case_context(self, case_no: str) -> str:
        """íŠ¹ì • ì‚¬ê±´ë²ˆí˜¸ì˜ íŒë¡€ ì „ë¬¸ì„ ê°€ì ¸ì˜´"""
        try:
            results = self.case_store.similarity_search(
                query="íŒë¡€ ì „ë¬¸ ê²€ìƒ‰",
                k=self.config.case_context_top_k,
                filter={"case_no": {"$eq": case_no}},
            )
            sorted_docs = sorted(
                results, 
                key=lambda x: str(x.metadata.get("chunk_id", ""))
            )
            unique_docs = _dedupe_docs(sorted_docs, self.config.dedupe_key_fields)
            return "\n".join([d.page_content for d in unique_docs]).strip()
        except Exception as e:
            logger.warning(f"âš ï¸ íŒë¡€ ì „ë¬¸ ë¡œë”© ì‹¤íŒ¨ ({case_no}): {e}")
            return ""

    def _attach_source(self, docs: List[Document], source: str) -> List[Document]:
        """ê²€ìƒ‰ ì¶œì²˜ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì£¼ì…"""
        for d in docs:
            if d.metadata is None:
                d.metadata = {}
            d.metadata["__source_index"] = source
        return docs

    def _get_doc_id(self, doc: Document) -> str:
        """ë¬¸ì„œì˜ ê³ ìœ  ID ìƒì„±"""
        md = doc.metadata or {}
        for field in self.config.dedupe_key_fields:
            if md.get(field):
                return f"{field}:{md[field]}"
        return f"hash:{hash(doc.page_content)}"

    def _compute_bm25_scores(
        self, 
        query: str, 
        documents: List[Document]
    ) -> Dict[str, float]:
        """BM25 ì ìˆ˜ ê³„ì‚°"""
        if not documents or not self._tokenizer:
            return {}
        
        scorer = BM25Scorer(
            tokenizer=self._tokenizer,
            algorithm=self.config.bm25_algorithm,
            k1=self.config.bm25_k1,
            b=self.config.bm25_b,
        )
        scorer.fit(documents)
        scores = scorer.score(query)
        
        return {
            self._get_doc_id(doc): score
            for doc, score in zip(documents, scores)
        }

    def _hybrid_fusion(
        self,
        query: str,
        dense_docs: List[Document],
    ) -> List[Document]:
        """
        Dense ê²€ìƒ‰ ê²°ê³¼ì— BM25ë¥¼ ê²°í•©í•˜ì—¬ í•˜ì´ë¸Œë¦¬ë“œ ìˆœìœ„ ìƒì„±
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            dense_docs: Dense ê²€ìƒ‰ìœ¼ë¡œ ê°€ì ¸ì˜¨ ë¬¸ì„œë“¤
        
        Returns:
            í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¡œ ì¬ìˆœìœ„í™”ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.config.enable_hybrid or not dense_docs:
            return dense_docs
        
        cfg = self.config
        
        # Dense ranks (ìˆœìœ„ ê¸°ë°˜)
        dense_ranks: Dict[str, int] = {}
        dense_scores: Dict[str, float] = {}
        for rank, doc in enumerate(dense_docs, start=1):
            doc_id = self._get_doc_id(doc)
            dense_ranks[doc_id] = rank
            # Dense scoreëŠ” ìˆœìœ„ì˜ ì—­ìˆ˜ë¡œ ê·¼ì‚¬
            dense_scores[doc_id] = 1.0 / rank
        
        # BM25 scores
        bm25_scores = self._compute_bm25_scores(query, dense_docs)
        
        # BM25 ranks
        sorted_bm25 = sorted(bm25_scores.items(), key=lambda x: x[1], reverse=True)
        bm25_ranks = {doc_id: rank for rank, (doc_id, _) in enumerate(sorted_bm25, start=1)}
        
        # Fusion
        if cfg.hybrid_method == "rrf":
            fused_scores = ScoreFusion.reciprocal_rank_fusion(
                dense_ranks, bm25_ranks, k=cfg.rrf_k
            )
        else:  # weighted
            fused_scores = ScoreFusion.weighted_sum(
                dense_scores, bm25_scores, alpha=cfg.hybrid_alpha
            )
        
        # ë¬¸ì„œë¥¼ fused_scoreë¡œ ì¬ì •ë ¬
        doc_map = {self._get_doc_id(d): d for d in dense_docs}
        sorted_ids = sorted(fused_scores.keys(), key=lambda x: fused_scores[x], reverse=True)
        
        reordered = [doc_map[doc_id] for doc_id in sorted_ids if doc_id in doc_map]
        
        logger.info(
            f"ğŸ”€ Hybrid Fusion ì™„ë£Œ ({cfg.hybrid_method}): "
            f"{len(dense_docs)}ê°œ â†’ {len(reordered)}ê°œ"
        )
        
        return reordered

    def _rerank(
        self, 
        query: str, 
        docs: List[Document]
    ) -> Optional[List[Tuple[int, float]]]:
        """Cohere rerank ì‹¤í–‰"""
        if not self._cohere_client:
            return None

        cfg = self.config
        texts = [_truncate(d.page_content or "", cfg.rerank_doc_max_chars) for d in docs]

        try:
            rerank_results = self._cohere_client.rerank(
                model=cfg.rerank_model,
                query=query,
                documents=texts,
                top_n=len(texts),
            )
            ranked = [
                (r.index, float(r.relevance_score)) 
                for r in rerank_results.results
            ]
            return ranked
        except Exception as e:
            logger.warning(f"âš ï¸ Rerank ì‹¤íŒ¨ (skip): {e}")
            return None

    def _cap_for_rerank(
        self, 
        law: List[Document], 
        rule: List[Document], 
        case: List[Document]
    ) -> List[Document]:
        """rerank ì…ë ¥ ë¬¸ì„œ ìˆ˜ ì œí•œ"""
        cfg = self.config
        law = _dedupe_docs(law, cfg.dedupe_key_fields)
        rule = _dedupe_docs(rule, cfg.dedupe_key_fields)
        case = _dedupe_docs(case, cfg.dedupe_key_fields)

        base = law + rule
        if len(base) >= cfg.rerank_max_documents:
            return base[: cfg.rerank_max_documents]

        remaining = cfg.rerank_max_documents - len(base)
        return base + case[:remaining]

    def triple_hybrid_retrieval(self, query: str) -> List[Document]:
        """
        3ì¤‘ ì¸ë±ìŠ¤ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Dense + Sparse + Rerank)
        
        ê²€ìƒ‰ íë¦„:
        1. Dense ê²€ìƒ‰ (Pinecone - Solar Embedding)
        2. Hybrid Fusion (Dense + BM25)
        3. Rerank (Cohere, ì„ íƒì )
        4. 2-stage Case Expansion
        5. Priority ì •ë ¬
        """
        cfg = self.config
        mult = cfg.search_multiplier

        logger.info(f"ğŸ” [Hybrid ê²€ìƒ‰] query='{query}'")

        # 1) Dense Retrieval (oversampling)
        docs_law = self._attach_source(
            self.law_store.similarity_search(query, k=cfg.k_law * mult),
            "law",
        )
        docs_rule = self._attach_source(
            self.rule_store.similarity_search(query, k=cfg.k_rule * mult),
            "rule",
        )
        docs_case_chunks = self._attach_source(
            self.case_store.similarity_search(query, k=cfg.case_candidate_k),
            "case",
        )

        # 2) Hybrid Fusion (Dense + BM25) - ê° ì¸ë±ìŠ¤ë³„ë¡œ ì ìš©
        if cfg.enable_hybrid:
            docs_law = self._hybrid_fusion(query, docs_law)
            docs_rule = self._hybrid_fusion(query, docs_rule)
            docs_case_chunks = self._hybrid_fusion(query, docs_case_chunks)

        # 3) Prepare for rerank
        combined_for_rerank = self._cap_for_rerank(docs_law, docs_rule, docs_case_chunks)

        # 4) Rerank (optional)
        selected_docs: List[Document]
        ranked = self._rerank(query, combined_for_rerank) if cfg.enable_rerank else None

        if ranked:
            filtered = [(i, s) for (i, s) in ranked if s >= cfg.rerank_threshold]
            if not filtered:
                desired = min(cfg.k_law + cfg.k_rule + cfg.k_case, len(ranked))
                filtered = ranked[:desired]
            selected_docs = [combined_for_rerank[i] for (i, _s) in filtered]
            logger.info(f"ğŸ“Œ Rerank ì™„ë£Œ: {len(selected_docs)}ê°œ ì„ íƒ (threshold={cfg.rerank_threshold})")
        else:
            selected_docs = combined_for_rerank

        # 5) Deduplicate
        selected_docs = _dedupe_docs(selected_docs, cfg.dedupe_key_fields)

        # 6) Select top docs per source + 2-stage case expansion
        law_ranked = [d for d in selected_docs if d.metadata.get("__source_index") == "law"]
        rule_ranked = [d for d in selected_docs if d.metadata.get("__source_index") == "rule"]
        case_ranked_chunks = [d for d in selected_docs if d.metadata.get("__source_index") == "case"]

        final_law = law_ranked[: cfg.k_law]
        final_rule = rule_ranked[: cfg.k_rule]

        # Case expansion
        top_n = cfg.case_expand_top_n if cfg.case_expand_top_n is not None else cfg.k_case
        seen_case_no: set = set()
        chosen_case_docs: List[Document] = []
        
        for d in case_ranked_chunks:
            case_no = d.metadata.get("case_no")
            if not case_no or case_no in seen_case_no:
                continue
            seen_case_no.add(case_no)
            chosen_case_docs.append(d)
            if len(chosen_case_docs) >= top_n:
                break

        expanded_cases: List[Document] = []
        for d in chosen_case_docs:
            case_no = d.metadata.get("case_no")
            if not case_no:
                continue
            full_text = self.get_full_case_context(str(case_no))
            if not full_text:
                expanded_cases.append(d)
                continue

            title = d.metadata.get("title") or d.metadata.get("case_name") or str(case_no)
            md = dict(d.metadata)
            md["__expanded"] = True
            expanded_cases.append(
                Document(
                    page_content=f"[íŒë¡€ ì „ë¬¸: {title}]\n{full_text}",
                    metadata=md,
                )
            )

        final_case = expanded_cases[: cfg.k_case]

        # 7) Priority sort
        final_docs = final_law + final_rule + final_case
        final_docs = sorted(
            final_docs, 
            key=lambda x: _safe_int((x.metadata or {}).get("priority", 99), 99)
        )

        logger.info(
            f"ğŸ“Š ìµœì¢… ê²€ìƒ‰ ê²°ê³¼: Law={len(final_law)}, "
            f"Rule={len(final_rule)}, Case={len(final_case)}"
        )

        return final_docs

    # ----------------------------
    # Context formatting
    # ----------------------------
    @staticmethod
    def format_context_with_hierarchy(docs: List[Document]) -> str:
        """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë²•ì  ìœ„ê³„ì— ë”°ë¼ ì„¹ì…˜ë³„ë¡œ ì¬êµ¬ì„±"""
        section_1_law: List[str] = []
        section_2_rule: List[str] = []
        section_3_case: List[str] = []

        for doc in docs:
            md = doc.metadata or {}
            p = _safe_int(md.get("priority", 99), 99)
            src = md.get("src_title", md.get("__source_index", "ìë£Œ"))
            title = md.get("title", "")
            content = doc.page_content or ""

            entry = f"[{src}] {title}\n{content}".strip()

            if p in (1, 2, 4, 5):
                section_1_law.append(entry)
            elif p in (3, 6, 7, 8, 11):
                section_2_rule.append(entry)
            else:
                section_3_case.append(entry)

        parts: List[str] = []
        if section_1_law:
            parts.append(
                "## [SECTION 1: í•µì‹¬ ë²•ë ¹ (ìµœìš°ì„  ë²•ì  ê·¼ê±°)]\n" 
                + "\n\n".join(section_1_law)
            )
        if section_2_rule:
            parts.append(
                "## [SECTION 2: ê´€ë ¨ ê·œì • ë° ì ˆì°¨ (ì„¸ë¶€ ê¸°ì¤€)]\n" 
                + "\n\n".join(section_2_rule)
            )
        if section_3_case:
            parts.append(
                "## [SECTION 3: íŒë¡€ ë° í•´ì„ ì‚¬ë¡€ (ì ìš© ì˜ˆì‹œ)]\n" 
                + "\n\n".join(section_3_case)
            )

        return "\n\n".join(parts).strip()

    # ----------------------------
    # Answer generation
    # ----------------------------
    def generate_answer(
        self, 
        user_input: str, 
        *, 
        skip_normalization: bool = False
    ) -> str:
        """
        ìµœì¢… ë‹µë³€ ìƒì„±
        
        Args:
            user_input: ì‚¬ìš©ì ì§ˆë¬¸
            skip_normalization: ì§ˆë¬¸ í‘œì¤€í™” ê±´ë„ˆë›°ê¸°
        
        Returns:
            ìƒì„±ëœ ë‹µë³€
        """
        # 1) Normalize
        normalized_query = (
            user_input if skip_normalization 
            else self.normalize_query(user_input)
        )
        if not skip_normalization:
            logger.info(f"ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: {normalized_query}")

        # 2) Retrieve (Hybrid)
        retrieved_docs = self.triple_hybrid_retrieval(normalized_query)
        if not retrieved_docs:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë²•ë ¹ì´ë‚˜ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # 3) Context
        hierarchical_context = self.format_context_with_hierarchy(retrieved_docs)

        # 4) Generate
        prompt = ChatPromptTemplate.from_messages([
            ("system", SYSTEM_PROMPT),
            ("human", "{question}"),
        ])
        chain = prompt | self._generation_llm | StrOutputParser()

        logger.info("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...")
        try:
            return str(chain.invoke({
                "context": hierarchical_context, 
                "question": normalized_query
            })).strip()
        except Exception as e:
            logger.warning(f"âš ï¸ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


# --------------------------------------------------------------------------------------
# Convenience functions
# --------------------------------------------------------------------------------------
def create_pipeline(
    enable_hybrid: bool = True,
    hybrid_method: str = "rrf",
    hybrid_alpha: float = 0.5,
    enable_rerank: bool = True,
    **kwargs
) -> RAGPipeline:
    """
    íŒŒì´í”„ë¼ì¸ ìƒì„± í—¬í¼ í•¨ìˆ˜
    
    Examples:
        # ê¸°ë³¸ í•˜ì´ë¸Œë¦¬ë“œ (RRF)
        pipeline = create_pipeline()
        
        # Denseë§Œ ì‚¬ìš©
        pipeline = create_pipeline(enable_hybrid=False)
        
        # ê°€ì¤‘ í•©ì‚° (Dense 70%)
        pipeline = create_pipeline(hybrid_method="weighted", hybrid_alpha=0.7)
        
        # Rerank ë¹„í™œì„±í™”
        pipeline = create_pipeline(enable_rerank=False)
    """
    config = RAGConfig(
        enable_hybrid=enable_hybrid,
        hybrid_method=hybrid_method,
        hybrid_alpha=hybrid_alpha,
        enable_rerank=enable_rerank,
        **kwargs
    )
    return RAGPipeline(config)


# --------------------------------------------------------------------------------------
# Exports
# --------------------------------------------------------------------------------------
__all__ = [
    # Config
    "RAGConfig",
    # Pipeline
    "RAGPipeline",
    "create_pipeline",
    # Tokenizers
    "Tokenizer",
    "SimpleTokenizer",
    "KiwiTokenizer",
    "get_default_tokenizer",
    # BM25
    "BM25Scorer",
    # Fusion
    "ScoreFusion",
    # Constants
    "INDEX_NAMES",
    "KEYWORD_DICT",
    "NORMALIZATION_PROMPT",
    "SYSTEM_PROMPT",
    # Availability flags
    "BM25_AVAILABLE",
    "KIWI_AVAILABLE",
    "COHERE_AVAILABLE",
]


# --------------------------------------------------------------------------------------
# Main (Test)
# --------------------------------------------------------------------------------------
if __name__ == "__main__":
    print("=" * 70)
    print("ğŸš€ Hybrid RAG Pipeline (Dense + Sparse) í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    # ì˜ì¡´ì„± ì²´í¬
    print("\nğŸ“¦ ì˜ì¡´ì„± ìƒíƒœ:")
    print(f"  - rank_bm25: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if BM25_AVAILABLE else 'âŒ ë¯¸ì„¤ì¹˜'}")
    print(f"  - kiwipiepy: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if KIWI_AVAILABLE else 'âŒ ë¯¸ì„¤ì¹˜ (SimpleTokenizer ì‚¬ìš©)'}")
    print(f"  - cohere: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if COHERE_AVAILABLE else 'âŒ ë¯¸ì„¤ì¹˜'}")
    
    try:
        # íŒŒì´í”„ë¼ì¸ ìƒì„±
        print("\nğŸ”§ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
        pipeline = create_pipeline(
            enable_hybrid=True,
            hybrid_method="rrf",
            enable_rerank=True,
        )
        
        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
        test_queries = [
            "ì§‘ì£¼ì¸ì´ ë³´ì¦ê¸ˆì„ ì•ˆ ëŒë ¤ì¤˜ìš”. ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
            "ì „ì…ì‹ ê³ ëŠ” ì–¸ì œê¹Œì§€ í•´ì•¼ ëŒ€í•­ë ¥ì´ ìƒê¸°ë‚˜ìš”?",
            "ê³„ì•½ ê°±ì‹ ì„ ìš”êµ¬í–ˆëŠ”ë° ì§‘ì£¼ì¸ì´ ê±°ì ˆí–ˆì–´ìš”.",
        ]
        
        for i, query in enumerate(test_queries, 1):
            print(f"\n{'='*70}")
            print(f"ğŸ“ í…ŒìŠ¤íŠ¸ {i}: {query}")
            print("=" * 70)
            
            answer = pipeline.generate_answer(query)
            print(f"\nğŸ’¬ ë‹µë³€:\n{answer}")
        
    except Exception as e:
        logger.error(f"ğŸ”¥ ì—ëŸ¬ ë°œìƒ: {e}")
        raise