"""
Unified RAG module with Hybrid Search (Dense + BM25)

ì£¼íƒì„ëŒ€ì°¨ RAG ì‹œìŠ¤í…œ - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ëª¨ë“ˆ

[ì£¼ìš” ê¸°ëŠ¥]
1. ì‚¬ìš©ì ì§ˆë¬¸ í‘œì¤€í™” (normalize_query)
2. Hybrid Retrieval: Dense(Solar) + Sparse(BM25)
3. 2-Stage Case Expansion: íŒë¡€ ê²€ìƒ‰ íš¨ìœ¨í™” ë° ì „ë¬¸ í™•ì¥
4. Rerank: Cohere ê¸°ë°˜ ì •ë°€ ì¬ì •ë ¬
5. ìµœì¢… ë‹µë³€ ìƒì„±: ë²•ì  ìœ„ê³„(Priority) ë°˜ì˜

[í•„ìˆ˜ ì˜ì¡´ì„±]
pip install rank_bm25 kiwipiepy langchain-upstage langchain-pinecone cohere
"""

from __future__ import annotations

import logging
import os
import time
from dataclasses import dataclass
from typing import List, Dict, Any, Optional

# LangChain Core
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Models & Vector Stores
from langchain_ollama import ChatOllama
from langchain_upstage import UpstageEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_community.retrievers import BM25Retriever
from pinecone import Pinecone
import cohere

# í˜•íƒœì†Œ ë¶„ì„ê¸° (BM25ìš©)
try:
    from kiwipiepy import Kiwi
    KIWI_AVAILABLE = True
except ImportError:
    KIWI_AVAILABLE = False
    print("âš ï¸ Warning: 'kiwipiepy' not installed. BM25 will use simple whitespace tokenizer.")

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)


# ==========================================
# 0. Constants & Config
# ==========================================

# ë²•ë¥  ìš©ì–´ ì‚¬ì „ (ì§ˆë¬¸ í‘œì¤€í™”ìš©)
LEGAL_KEYWORD_MAP = {
    "ì§‘ì£¼ì¸": "ì„ëŒ€ì¸", "ê±´ë¬¼ì£¼": "ì„ëŒ€ì¸", "ì„¸ì…ì": "ì„ì°¨ì¸", "ì›”ì„¸ì…ì": "ì„ì°¨ì¸",
    "ë¶€ë™ì‚°": "ê³µì¸ì¤‘ê°œì‚¬", "ë³µë¹„": "ì¤‘ê°œë³´ìˆ˜", "ê³„ì•½ì„œ": "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ",
    "ì „ì„¸ê¸ˆ": "ì„ì°¨ë³´ì¦ê¸ˆ", "ë³´ì¦ê¸ˆ": "ì„ì°¨ë³´ì¦ê¸ˆ", "ì›”ì„¸": "ì°¨ì„", "ë°©ì„¸": "ì°¨ì„",
    "ì›”ì„¸ì˜¬ë¦¬ê¸°": "ì°¨ì„ì¦ì•¡", "ì¸ìƒ": "ì¦ì•¡", "ì›”ì„¸ê¹ê¸°": "ì°¨ì„ê°ì•¡", "í• ì¸": "ê°ì•¡",
    "ëˆë¨¼ì €ë°›ê¸°": "ìš°ì„ ë³€ì œê¶Œ", "ìˆœìœ„": "ìš°ì„ ë³€ì œê¶Œ", "ì•ˆì „ì¥ì¹˜": "ëŒ€í•­ë ¥",
    "ì—°ì¥í•˜ê¸°": "ê³„ì•½ê°±ì‹ ìš”êµ¬ê¶Œ", "ì¬ê³„ì•½": "ê³„ì•½ê°±ì‹ ", "ìë™ì—°ì¥": "ë¬µì‹œì ê°±ì‹ ",
    "ë°©ë¹¼": "ê³„ì•½í•´ì§€", "ë‚˜ê°€ë¼ê³ ": "ê³„ì•½ê°±ì‹ ê±°ì ˆ", "ë¹„ì›Œë‹¬ë¼": "ëª…ë„", "ì´ì‚¬": "ì£¼íƒì˜ì¸ë„",
    "ì „ì…ì‹ ê³ ": "ì£¼ë¯¼ë“±ë¡", "ì§‘ê³ ì¹˜ê¸°": "ìˆ˜ì„ ì˜ë¬´", "ë¬¼ìƒ˜": "ëˆ„ìˆ˜", "ì²­ì†Œë¹„": "ì›ìƒíšŒë³µë¹„ìš©",
    "ê¹¡í†µì „ì„¸": "ì „ì„¸í”¼í•´", "ì‚¬ê¸°": "ì „ì„¸ì‚¬ê¸°", "ì¡°ì •ìœ„": "ì£¼íƒì„ëŒ€ì°¨ë¶„ìŸì¡°ì •ìœ„ì›íšŒ"
}

# LLM ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ 'ì£¼íƒ ì „ì›”ì„¸ ì‚¬ê¸° ì˜ˆë°© ë° ì„ëŒ€ì°¨ ë²•ë¥  ì „ë¬¸ê°€ AI'ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ [ë²•ì  ìœ„ê³„ê°€ ì •ë¦¬ëœ ì°¸ê³  ë¬¸ì„œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

[ë‹µë³€ ìƒì„± ì›ì¹™]
1. **ë²•ì  ìœ„ê³„ ì¤€ìˆ˜**: 
   - ë°˜ë“œì‹œ [SECTION 1: í•µì‹¬ ë²•ë ¹]ì˜ ë‚´ìš©ì„ ìµœìš°ì„  íŒë‹¨ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ìœ¼ì„¸ìš”.
   - [SECTION 1]ì˜ ë‚´ìš©ì´ ëª¨í˜¸í•  ë•Œë§Œ [SECTION 2]ì™€ [SECTION 3]ë¥¼ ë³´ì¶© ê·¼ê±°ë¡œ í™œìš©í•˜ì„¸ìš”.
   - ë§Œì•½ [SECTION 3: íŒë¡€]ê°€ [SECTION 1: ë²•ë ¹]ê³¼ ë‹¤ë¥´ê²Œ í•´ì„ë˜ëŠ” íŠ¹ìˆ˜í•œ ê²½ìš°ë¼ë©´, "ì›ì¹™ì€ ë²•ë ¹ì— ë”°ë¥´ë‚˜, íŒë¡€ëŠ” ì˜ˆì™¸ì ìœ¼ë¡œ..."ë¼ê³  ì„¤ëª…í•˜ì„¸ìš”.

2. **ë‹µë³€ êµ¬ì¡°**:
   - **í•µì‹¬ ê²°ë¡ **: ì§ˆë¬¸ì— ëŒ€í•œ ê²°ë¡ (ê°€ëŠ¥/ë¶ˆê°€ëŠ¥/ìœ íš¨/ë¬´íš¨)ì„ ë‘ê´„ì‹ìœ¼ë¡œ ìš”ì•½.
   - **ë²•ì  ê·¼ê±°**: "ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì œOì¡°ì— ë”°ë¥´ë©´..." (SECTION 1 ì¸ìš©)
   - **ì‹¤ë¬´ ì ˆì°¨**: í•„ìš”ì‹œ ì‹ ê³  ë°©ë²•, ì„œë¥˜ ë“± ì•ˆë‚´ (SECTION 2 ì¸ìš©)
   - **ì°¸ê³  ì‚¬ë¡€**: ìœ ì‚¬í•œ ìƒí™©ì—ì„œì˜ íŒê²°ì´ë‚˜ í•´ì„ (SECTION 3 ì¸ìš©)
   - **ì£¼ì˜ì‚¬í•­**: ê°•í–‰ê·œì • ìœ„ë°˜ ì‹œ "íš¨ë ¥ì´ ì—†ë‹¤"ê³  ê²½ê³ í•˜ê³ , ìµœì¢…ì ìœ¼ë¡œ ì „ë¬¸ê°€ í™•ì¸ì´ í•„ìš”í•¨ì„ ê³ ì§€í•˜ì„¸ìš”.

[ë²•ì  ìœ„ê³„ê°€ ì •ë¦¬ëœ ì°¸ê³  ë¬¸ì„œ]
{context}
"""

@dataclass
class RAGConfig:
    # API Keys (í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ì§ì ‘ ì…ë ¥)
    pinecone_api_key: str = os.getenv("PINECONE_API_KEY", "")
    upstage_api_key: str = os.getenv("UPSTAGE_API_KEY", "")
    cohere_api_key: str = os.getenv("COHERE_API_KEY", "")
    
    # Models
    embedding_model: str = "solar-embedding-1-large-passage"
    llm_model: str = "exaone3.5:2.4b"
    llm_temperature: float = 0.1
    
    # Index Names
    index_names: Dict[str, str] = None

    def __post_init__(self):
        if self.index_names is None:
            self.index_names = {
                "law": "law-index-final",
                "rule": "rule-index-final",
                "case": "case-index-final"
            }


# ==========================================
# 1. RAG Pipeline Class
# ==========================================
class RAGPipeline:
    def __init__(self, config: RAGConfig):
        self.config = config
        self._init_components()
        self.bm25_retriever = None  # ì¶”í›„ build_bm25() í˜¸ì¶œ ì‹œ ì´ˆê¸°í™”

    def _init_components(self):
        """ê¸°ë³¸ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” (Pinecone, LLM, Cohere, Kiwi)"""
        # 1. Embedding
        if not self.config.upstage_api_key:
             logger.warning("âš ï¸ UPSTAGE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        self.embedding = UpstageEmbeddings(model=self.config.embedding_model)
        
        # 2. Pinecone Stores (Dense)
        if not self.config.pinecone_api_key:
             raise ValueError("âŒ PINECONE_API_KEYê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.")
             
        pc = Pinecone(api_key=self.config.pinecone_api_key)
        self.stores = {}
        logger.info("ğŸ”— Pinecone ì¸ë±ìŠ¤ ì—°ê²° ì¤‘...")
        for key, name in self.config.index_names.items():
            try:
                self.stores[key] = PineconeVectorStore(
                    index_name=name,
                    embedding=self.embedding,
                    pinecone_api_key=self.config.pinecone_api_key
                )
            except Exception as e:
                logger.error(f"âŒ Pinecone ì¸ë±ìŠ¤ '{name}' ì—°ê²° ì‹¤íŒ¨: {e}")
        
        # 3. LLM (Ollama)
        self._generation_llm = ChatOllama(
            model=self.config.llm_model,
            temperature=self.config.llm_temperature
        )
        
        # 4. Cohere Client (Rerank)
        if self.config.cohere_api_key:
            self.cohere_client = cohere.Client(api_key=self.config.cohere_api_key)
            logger.info("âœ… Cohere Rerank í™œì„±í™”ë¨")
        else:
            self.cohere_client = None
            logger.warning("âš ï¸ Cohere API Key ì—†ìŒ. Rerank ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

        # 5. Kiwi Tokenizer (for BM25)
        if KIWI_AVAILABLE:
            self.kiwi = Kiwi()
            logger.info("âœ… Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ ì™„ë£Œ")
        else:
            self.kiwi = None

    # ---------------------------------------------------------
    # BM25 Management
    # ---------------------------------------------------------
    def kiwipiepy_tokenizer(self, text: str) -> List[str]:
        """BM25ìš© í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ í† í¬ë‚˜ì´ì €"""
        if self.kiwi:
            return [token.form for token in self.kiwi.tokenize(text)]
        return text.split()  # Fallback: ë„ì–´ì“°ê¸° ê¸°ì¤€

    def build_bm25(self, documents: List[Document]):
        """
        ì™¸ë¶€ì—ì„œ ë¡œë“œí•œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë¡œì»¬ BM25 ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        (ì„œë²„ ì‹œì‘ ì‹œì ì— ì „ì²´ ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì™€ ì£¼ì…í•´ì•¼ í•¨)
        """
        if not documents:
            logger.warning("âš ï¸ BM25 ë¹Œë“œë¥¼ ìœ„í•œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return

        logger.info(f"ğŸ—ï¸ BM25 ì¸ë±ìŠ¤ ìƒì„± ì‹œì‘ (ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ)...")
        self.bm25_retriever = BM25Retriever.from_documents(
            documents,
            preprocess_func=self.kiwipiepy_tokenizer if KIWI_AVAILABLE else None
        )
        # BM25 ê²€ìƒ‰ ê°œìˆ˜ ì„¤ì • (Denseë³´ë‹¤ ì¡°ê¸ˆ ë” ë§ì´ ê°€ì ¸ì™€ì„œ Rerankerì— ë„˜ê¹€)
        self.bm25_retriever.k = 10 
        logger.info("âœ… BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")

    # ---------------------------------------------------------
    # Retrieval Logic (Dense + Sparse)
    # ---------------------------------------------------------
    def get_full_case_context(self, case_no: str) -> str:
        """íŒë¡€ ì „ë¬¸ í™•ì¥ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        try:
            # Query must not be empty for Upstage embedding
            results = self.stores['case'].similarity_search(
                query="íŒë¡€ ì „ë¬¸ ê²€ìƒ‰", 
                k=50, 
                filter={"case_no": {"$eq": case_no}}
            )
            # chunk_id ìˆœ ì •ë ¬
            sorted_docs = sorted(results, key=lambda x: x.metadata.get('chunk_id', ''))
            
            seen = set()
            unique_docs = []
            for doc in sorted_docs:
                cid = doc.metadata.get('chunk_id')
                if cid and cid not in seen:
                    unique_docs.append(doc)
                    seen.add(cid)
            return "\n".join([doc.page_content for doc in unique_docs])
        except Exception as e:
            logger.warning(f"âš ï¸ íŒë¡€ í™•ì¥ ì‹¤íŒ¨ ({case_no}): {e}")
            return ""

    def triple_hybrid_retrieval(self, query: str, k_dense_law=3, k_dense_case=3) -> List[Document]:
        """
        [Hybrid Search Workflow]
        1. Dense Search: Law/Rule/Case ì¸ë±ìŠ¤ì—ì„œ ì˜ë¯¸ ê²€ìƒ‰
        2. Sparse Search (BM25): ì „ì²´ ë¬¸ì„œì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰
        3. Ensemble: ê²°ê³¼ í†µí•© ë° ì¤‘ë³µ ì œê±°
        4. Case Expansion: íŒë¡€ ì „ë¬¸ í™•ì¥
        5. Rerank: Cohereë¡œ ìµœì¢… ì •ë ¬
        """
        logger.info(f"ğŸ” [í†µí•© ê²€ìƒ‰] ì¿¼ë¦¬: '{query}'")

        # 1. Dense Search (Pinecone)
        docs_law = self.stores['law'].similarity_search(query, k=k_dense_law)
        docs_rule = self.stores['rule'].similarity_search(query, k=k_dense_law)
        docs_case = self.stores['case'].similarity_search(query, k=k_dense_case * 2)
        
        dense_results = docs_law + docs_rule + docs_case
        logger.info(f"  - Dense ê²°ê³¼: {len(dense_results)}ê±´")
        
        # 2. Sparse Search (BM25) - ë¡œì»¬ ì¸ë±ìŠ¤ê°€ ìˆëŠ” ê²½ìš°ë§Œ
        sparse_results = []
        if self.bm25_retriever:
            # BM25ëŠ” ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰
            sparse_results = self.bm25_retriever.invoke(query)
            logger.info(f"  - BM25 ê²°ê³¼: {len(sparse_results)}ê±´")

        # 3. Ensemble (Union & Deduplication)
        combined_docs_map = {}
        
        # Dense ê²°ê³¼ ìš°ì„  ì¶”ê°€
        for doc in dense_results:
            # chunk_idê°€ ì—†ìœ¼ë©´ content ì•ë¶€ë¶„ì„ í‚¤ë¡œ ì‚¬ìš©
            cid = doc.metadata.get('chunk_id', doc.page_content[:30])
            combined_docs_map[cid] = doc
            
        # BM25 ê²°ê³¼ ì¶”ê°€ (ì´ë¯¸ ìˆëŠ” ë¬¸ì„œëŠ” ìŠ¤í‚µ -> ì‚¬ì‹¤ìƒ Denseê°€ ìš°ì„ ìˆœìœ„ì´ë‚˜, Rerankerê°€ íŒë‹¨í•¨)
        for doc in sparse_results:
            cid = doc.metadata.get('chunk_id', doc.page_content[:30])
            if cid not in combined_docs_map:
                combined_docs_map[cid] = doc 
        
        combined_docs = list(combined_docs_map.values())
        logger.info(f"  - í†µí•© í›„ë³´êµ°: {len(combined_docs)}ê±´")
        
        # 4. Case Expansion (íŒë¡€ ì „ë¬¸ í™•ì¥)
        final_candidates = []
        seen_cases = set()
        
        for doc in combined_docs:
            case_no = doc.metadata.get('case_no')
            # íŒë¡€ì´ë©´ì„œ ì•„ì§ í™•ì¥ ì•ˆ ëœ ê²½ìš°
            if case_no:
                if case_no not in seen_cases:
                    full_text = self.get_full_case_context(case_no)
                    if full_text:
                        # ì›ë³¸ ë©”íƒ€ë°ì´í„° ìœ ì§€, ë‚´ìš©ì€ ì „ë¬¸ìœ¼ë¡œ êµì²´
                        new_doc = Document(
                            page_content=f"[íŒë¡€ ì „ë¬¸: {doc.metadata.get('title')}]\n{full_text}",
                            metadata=doc.metadata
                        )
                        final_candidates.append(new_doc)
                        seen_cases.add(case_no)
            else:
                # ë²•ë ¹/ê·œì¹™ì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                final_candidates.append(doc)

        # 5. Rerank (Cohere)
        if self.cohere_client:
            try:
                # ë¬¸ì„œ ë‚´ìš© ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
                docs_content = [d.page_content for d in final_candidates]
                
                rerank_results = self.cohere_client.rerank(
                    model="rerank-multilingual-v3.0",
                    query=query,
                    documents=docs_content,
                    top_n=len(final_candidates)
                )
                
                reranked_docs = []
                logger.info("ğŸ“Š Rerank ì ìˆ˜ (Top 5):")
                for i, r in enumerate(rerank_results.results):
                    if r.relevance_score > 0.10: # Threshold
                        doc = final_candidates[r.index]
                        reranked_docs.append(doc)
                        if i < 5:
                            logger.info(f"  - [{r.relevance_score:.4f}] {doc.metadata.get('title')}")
                            
                return reranked_docs
                
            except Exception as e:
                logger.error(f"âš ï¸ Rerank Failed: {e}")
                return final_candidates # Fallback
        
        return final_candidates

    # ---------------------------------------------------------
    # Context & Generation
    # ---------------------------------------------------------
    def normalize_query(self, user_query: str) -> str:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²•ë¥  ìš©ì–´ë¡œ í‘œì¤€í™”"""
        prompt = ChatPromptTemplate.from_template("""
        ë‹¹ì‹ ì€ ë²•ë¥  AI ì±—ë´‡ì˜ ì „ì²˜ë¦¬ ë‹´ë‹¹ìì…ë‹ˆë‹¤.
        ì•„ë˜ [ìš©ì–´ ì‚¬ì „]ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ 'ë²•ë¥  í‘œì¤€ì–´'ë¡œ ë³€í™˜í•´ ì£¼ì„¸ìš”.
        
        [ìš©ì–´ ì‚¬ì „]
        {dictionary}
        
        [ì§€ì¹¨]
        1. ì‚¬ì „ì˜ ë‹¨ì–´ê°€ ì§ˆë¬¸ì— ìˆë‹¤ë©´ ë°˜ë“œì‹œ ë²•ë¥  ìš©ì–´ë¡œ ë³€ê²½í•˜ì„¸ìš”.
        2. ì¡°ì‚¬ë‚˜ ì„œìˆ ì–´ë¥¼ ë¬¸ë§¥ì— ë§ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ìˆ˜ì •í•˜ì„¸ìš”.
        3. ì˜¤ì§ 'ë³€ê²½ëœ ì§ˆë¬¸' í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
        
        ì‚¬ìš©ì ì§ˆë¬¸: {question}
        ë³€ê²½ëœ ì§ˆë¬¸:""")
        
        chain = prompt | self._generation_llm | StrOutputParser()
        
        try:
            return chain.invoke({"dictionary": LEGAL_KEYWORD_MAP, "question": user_query}).strip()
        except Exception as e:
            logger.warning(f"âš ï¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return user_query

    def format_context_with_hierarchy(self, docs: List[Document]) -> str:
        """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë²•ì  ìœ„ê³„(Priority)ì— ë”°ë¼ ì„¹ì…˜ë³„ë¡œ ì¬êµ¬ì„±"""
        # Priority ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ (1ì´ ê°€ì¥ ë†’ìŒ)
        sorted_docs = sorted(docs, key=lambda x: int(x.metadata.get('priority', 99)))
        
        sections = {1: [], 2: [], 3: []}
        for doc in sorted_docs:
            p = int(doc.metadata.get('priority', 99))
            src = doc.metadata.get('src_title', 'ìë£Œ')
            title = doc.metadata.get('title', '')
            entry = f"[{src}] {title}\n{doc.page_content}"
            
            if p in [1, 2, 4, 5]: 
                sections[1].append(entry)
            elif p in [3, 6, 7, 8, 11]: 
                sections[2].append(entry)
            else: 
                sections[3].append(entry)
            
        context = ""
        if sections[1]: 
            context += "## [SECTION 1: í•µì‹¬ ë²•ë ¹ (ìµœìš°ì„  ë²•ì  ê·¼ê±°)]\n" + "\n\n".join(sections[1]) + "\n\n"
        if sections[2]: 
            context += "## [SECTION 2: ê´€ë ¨ ê·œì • ë° ì ˆì°¨ (ì„¸ë¶€ ê¸°ì¤€)]\n" + "\n\n".join(sections[2]) + "\n\n"
        if sections[3]: 
            context += "## [SECTION 3: íŒë¡€ ë° í•´ì„ ì‚¬ë¡€ (ì ìš© ì˜ˆì‹œ)]\n" + "\n\n".join(sections[3]) + "\n\n"
            
        return context

    def generate_answer(self, user_input: str, *, skip_normalization: bool = False) -> str:
        """
        ìµœì¢… ë‹µë³€ ìƒì„± íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        """
        # 1) Normalize
        if not skip_normalization:
            normalized_query = self.normalize_query(user_input)
            logger.info(f"ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: {normalized_query}")
        else:
            normalized_query = user_input

        # 2) Retrieve (Hybrid)
        retrieved_docs = self.triple_hybrid_retrieval(normalized_query)
        
        if not retrieved_docs:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë²•ë ¹ì´ë‚˜ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # 3) Context Formatting
        hierarchical_context = self.format_context_with_hierarchy(retrieved_docs)

        # 4) Generate Answer
        prompt = ChatPromptTemplate.from_messages([
            ("system", SYSTEM_PROMPT),
            ("human", "{question}"),
        ])
        chain = prompt | self._generation_llm | StrOutputParser()

        logger.info("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...")
        try:
            return chain.invoke({"context": hierarchical_context, "question": normalized_query})
        except Exception as e:
            logger.error(f"âš ï¸ ë‹µë³€ ìƒì„± ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë„ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


# ==========================================
# ì‹¤í–‰ ì˜ˆì‹œ (Testing Block)
# ==========================================
if __name__ == "__main__":
    # 1. ì„¤ì • ë° ì´ˆê¸°í™”
    config = RAGConfig()
    try:
        pipeline = RAGPipeline(config)
        
        # 2. [ì¤‘ìš”] BM25 ë¹Œë“œ í…ŒìŠ¤íŠ¸ (ì‹¤ì œë¡œëŠ” ì „ì²´ ë¬¸ì„œ ë¡œë“œ í•„ìš”)
        # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„° ìƒì„± (ì›ë˜ëŠ” CSV ë“±ì—ì„œ ë¡œë“œí•´ì•¼ í•¨)
        dummy_docs = [
            Document(page_content="ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì œ3ì¡°(ëŒ€í•­ë ¥) ì„ì°¨ì¸ì´ ì£¼íƒì˜ ì¸ë„ì™€ ì£¼ë¯¼ë“±ë¡ì„ ë§ˆì¹œ ë•Œì—ëŠ”...", metadata={"chunk_id": "LAW_001", "priority": 1}),
            Document(page_content="í™•ì •ì¼ì ë¶€ì—¬ ë° ì •ë³´ì œê³µì— ê´€í•œ ê·œì¹™ ì œ2ì¡°(ìˆ˜ìˆ˜ë£Œ) í™•ì •ì¼ì ë¶€ì—¬ ìˆ˜ìˆ˜ë£ŒëŠ” ê±´ë‹¹ 600ì›ìœ¼ë¡œ í•œë‹¤.", metadata={"chunk_id": "RULE_001", "priority": 3}),
        ]
        pipeline.build_bm25(dummy_docs)
        
        # 3. ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
        test_query = "í™•ì •ì¼ì ë°›ìœ¼ë ¤ë©´ ëˆ ì–¼ë§ˆë‚˜ ë“¤ì–´?"
        print("\n" + "="*60)
        print(pipeline.generate_answer(test_query))
        print("="*60)
        
    except Exception as e:
        print(f"ğŸ”¥ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")