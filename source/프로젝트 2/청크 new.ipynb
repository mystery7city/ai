{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff159fe4-6b2e-4eed-ae59-0703ad7c4f22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8ca07f-198c-45bb-8fb6-78748e3c6618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0afc8a15-4ee1-44b1-ac4e-abbbbdee170e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_response_info': {'raw_headers': {'connection': 'keep-alive',\n",
      "                                    'content-length': '151',\n",
      "                                    'content-type': 'application/json',\n",
      "                                    'date': 'Mon, 19 Jan 2026 05:42:43 GMT',\n",
      "                                    'grpc-status': '0',\n",
      "                                    'server': 'envoy',\n",
      "                                    'x-envoy-upstream-service-time': '37',\n",
      "                                    'x-pinecone-request-id': '7881142262978464916',\n",
      "                                    'x-pinecone-request-latency-ms': '36',\n",
      "                                    'x-pinecone-response-duration-ms': '38'}},\n",
      " 'dimension': 3072,\n",
      " 'index_fullness': 0.0,\n",
      " 'memoryFullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {},\n",
      " 'storageFullness': 0.0,\n",
      " 'total_vector_count': 0,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# .env ë¡œë“œ\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# í™˜ê²½ë³€ìˆ˜ ì½ê¸°\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "if not PINECONE_API_KEY:\n",
    "    raise ValueError(\"âŒ PINECONE_API_KEYê°€ .envì— ì—†ìŠµë‹ˆë‹¤\")\n",
    "\n",
    "# Pinecone ì—°ê²°\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# ì¸ë±ìŠ¤ í™•ì¸\n",
    "index = pc.Index(\"realestate2\")\n",
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5cc910-3a81-4d18-b9a5-32ce26fd8aba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff1c8721-6df9-44ff-bf42-5bf497177e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì¡°ë¬¸(article) ë³µì› ê°œìˆ˜: 119\n",
      "âœ… ì¬ì²­í‚¹ ê²°ê³¼ chunk ìˆ˜: 165\n",
      "                                     id category  priority    law_type  \\\n",
      "0  d9cab39a-c371-4c1b-9a16-a5b24bd6b212      law        50  ì‚¬ë¡€/íŒë¡€/í”¼í•´ìƒë‹´   \n",
      "1  73d58879-0530-4840-a732-2830bfb8982a      law        50  ì‚¬ë¡€/íŒë¡€/í”¼í•´ìƒë‹´   \n",
      "2  5b51377b-502a-43cd-84df-3d5a46545838      law        50  ì‚¬ë¡€/íŒë¡€/í”¼í•´ìƒë‹´   \n",
      "\n",
      "  article  chunk_id             source  \\\n",
      "0    ì‚¬ë¡€ 1         0  2025ì „ì„¸í”¼í•´ì§€ì›ì‚¬ë¡€ì§‘.pdf   \n",
      "1   ì‚¬ë¡€ 10         0  2025ì „ì„¸í”¼í•´ì§€ì›ì‚¬ë¡€ì§‘.pdf   \n",
      "2   ì‚¬ë¡€ 10         1  2025ì „ì„¸í”¼í•´ì§€ì›ì‚¬ë¡€ì§‘.pdf   \n",
      "\n",
      "                                                text  \n",
      "0  ì „ì„¸ëŒ€ì¶œ ì—°ì²´ ë¬¸ì œ ì£¼ìš” ì „ì„¸í”¼í•´ì£¼íƒì—ì„œ í‡´ê±°í•˜ê³ ì í•˜ë‚˜ ì„ëŒ€ì¸ì´ ì—°ë½ ë‘ì ˆ(ë˜ëŠ” ...  \n",
      "1  ë‹¤ê°€êµ¬ì£¼íƒ ì„ì°¨ì¸ì˜ ìš°ì„ ë§¤ìˆ˜ê¶Œ ì–‘ë„ 11 ìš°ì„ ë§¤ìˆ˜ê¶Œì„ ì–‘ë„í•˜ì§€ ëª»í•œ ì„ì°¨ì¸ (LHê°€...  \n",
      "2  â€¢ ê²½ë§¤ê°€ì • : LH ê°ì •ê°€ 6.5ì–µì›, ë‚™ì°°ê°€ê²© 5ì–µì›, ì„ ìˆœìœ„ ê·¼ì €ë‹¹ 4.5ì–µì›...  \n",
      "  ... upsert 64/165\n",
      "âœ… ì™„ë£Œ! ì¸ë±ìŠ¤: realestate2 | namespace: __default2__\n",
      "ğŸ‘‰ í™•ì¸: {'_response_info': {'raw_headers': {'connection': 'keep-alive',\n",
      "                                    'content-length': '187',\n",
      "                                    'content-type': 'application/json',\n",
      "                                    'date': 'Mon, 19 Jan 2026 05:44:35 GMT',\n",
      "                                    'grpc-status': '0',\n",
      "                                    'server': 'envoy',\n",
      "                                    'x-envoy-upstream-service-time': '35',\n",
      "                                    'x-pinecone-request-id': '1485144305525443631',\n",
      "                                    'x-pinecone-request-latency-ms': '34',\n",
      "                                    'x-pinecone-response-duration-ms': '36'}},\n",
      " 'dimension': 3072,\n",
      " 'index_fullness': 0.0,\n",
      " 'memoryFullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'__default2__': {'vector_count': 165}},\n",
      " 'storageFullness': 0.0,\n",
      " 'total_vector_count': 165,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# =========================\n",
    "# 0) ENV & CLIENT\n",
    "# =========================\n",
    "load_dotenv(override=True)\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY or not PINECONE_API_KEY:\n",
    "    raise ValueError(\"âŒ .envì— OPENAI_API_KEY / PINECONE_API_KEY ì„¤ì • í•„ìš”\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "INDEX_NAME = \"realestate2\"          # âœ… ìš”ì²­í•œ ì¸ë±ìŠ¤\n",
    "NAMESPACE = \"__default2__\"          # âœ… ê¸°ë³¸ namespace\n",
    "CSV_PATH = \"data/all_chunks.csv\"\n",
    "\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "EMBED_MODEL = \"text-embedding-3-large\"  # 3072-dim\n",
    "\n",
    "# =========================\n",
    "# 1) PRIORITY MAP (ë„ˆ ê¸°ì¤€)\n",
    "# =========================\n",
    "PRIORITY_MAP = {\n",
    "    \"ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²•\": 1,\n",
    "    \"ì‹œí–‰ë ¹\": 2,\n",
    "    \"ì‹œí–‰ê·œì¹™/ëŒ€ë²•ì›ê·œì¹™\": 3,\n",
    "    \"ë¯¼ë²•\": 4,\n",
    "    \"ì‚¬ë¡€/íŒë¡€/í”¼í•´ìƒë‹´\": 50,\n",
    "}\n",
    "\n",
    "def get_priority(law_type: str) -> int:\n",
    "    lt = (law_type or \"\").strip()\n",
    "    if lt in PRIORITY_MAP:\n",
    "        return PRIORITY_MAP[lt]\n",
    "    s = re.sub(r\"\\s+\", \"\", lt)\n",
    "    if \"ì£¼íƒì„ëŒ€ì°¨\" in s:\n",
    "        return 1\n",
    "    if \"ì‹œí–‰ë ¹\" in s:\n",
    "        return 2\n",
    "    if \"ì‹œí–‰ê·œì¹™\" in s or \"ëŒ€ë²•ì›ê·œì¹™\" in s:\n",
    "        return 3\n",
    "    if \"ë¯¼ë²•\" in s:\n",
    "        return 4\n",
    "    if \"íŒë¡€\" in s or \"ì‚¬ë¡€\" in s or \"í”¼í•´ìƒë‹´\" in s:\n",
    "        return 50\n",
    "    return 50\n",
    "\n",
    "# =========================\n",
    "# 2) TEXT UTILS\n",
    "# =========================\n",
    "def norm_space(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n",
    "\n",
    "def split_sentences_kr(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    ì•ˆì „í•œ ë¬¸ì¥ ë¶„ë¦¬ (ë„ˆê°€ ì“°ë˜ ë°©ì‹ ê¸°ë°˜)\n",
    "    \"\"\"\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    # 1) .,?,! ë’¤ ê³µë°± ë¶„ë¦¬\n",
    "    parts = re.split(r'(?<=[\\.\\?\\!])\\s+', text)\n",
    "\n",
    "    # 2) 'ë‹¤.' ê¸°ì¤€ ì¶”ê°€ ë¶„ë¦¬(look-behind ì—†ì´)\n",
    "    final = []\n",
    "    for p in parts:\n",
    "        sub = re.split(r'(ë‹¤\\.)\\s*', p)\n",
    "        buf = \"\"\n",
    "        for token in sub:\n",
    "            if token == \"ë‹¤.\":\n",
    "                buf += token\n",
    "                if buf.strip():\n",
    "                    final.append(buf.strip())\n",
    "                buf = \"\"\n",
    "            else:\n",
    "                buf += token\n",
    "        if buf.strip():\n",
    "            final.append(buf.strip())\n",
    "\n",
    "    final = [s.strip() for s in final if s and s.strip()]\n",
    "    return final\n",
    "\n",
    "def chunk_by_sentences(sentences: list[str], max_chars: int = 1400, overlap_sents: int = 2) -> list[str]:\n",
    "    \"\"\"\n",
    "    ë¬¸ì¥ ê²½ê³„ ê¸°ë°˜ ì²­í‚¹(ë¬¸ì¥/ì¡°ë¬¸ ì¤‘ê°„ ì ˆë‹¨ ìµœì†Œí™”)\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    buf = []\n",
    "    buf_len = 0\n",
    "\n",
    "    def flush():\n",
    "        nonlocal buf, buf_len\n",
    "        if buf:\n",
    "            out.append(norm_space(\" \".join(buf)))\n",
    "        buf = []\n",
    "        buf_len = 0\n",
    "\n",
    "    for s in sentences:\n",
    "        s = norm_space(s)\n",
    "        if not s:\n",
    "            continue\n",
    "\n",
    "        # ë‹¨ì¼ ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ë©´ ìµœí›„ìˆ˜ë‹¨ìœ¼ë¡œ ìë¦„(ë“œë­„)\n",
    "        if len(s) > max_chars:\n",
    "            flush()\n",
    "            for i in range(0, len(s), max_chars):\n",
    "                piece = norm_space(s[i:i+max_chars])\n",
    "                if len(piece) >= 30:\n",
    "                    out.append(piece)\n",
    "            continue\n",
    "\n",
    "        if buf_len + len(s) + 1 > max_chars:\n",
    "            flush()\n",
    "            # overlap ì ìš©(ì´ì „ ì²­í¬ ë§ˆì§€ë§‰ overlap_sents ë¬¸ì¥ì„ carry)\n",
    "            if overlap_sents > 0 and out:\n",
    "                prev_sents = split_sentences_kr(out[-1])\n",
    "                carry = prev_sents[-overlap_sents:] if len(prev_sents) >= overlap_sents else prev_sents\n",
    "                buf = carry[:]\n",
    "                buf_len = len(\" \".join(buf))\n",
    "\n",
    "        buf.append(s)\n",
    "        buf_len += len(s) + 1\n",
    "\n",
    "    flush()\n",
    "    return [c for c in out if len(c) >= 30]\n",
    "\n",
    "# =========================\n",
    "# 3) Embedding (batch)\n",
    "# =========================\n",
    "def embed_batch(texts: list[str]) -> list[list[float]]:\n",
    "    resp = client.embeddings.create(model=EMBED_MODEL, input=texts)\n",
    "    # OpenAIê°€ ë°˜í™˜ ìˆœì„œë¥¼ ì…ë ¥ê³¼ ë™ì¼í•˜ê²Œ ì¤Œ\n",
    "    return [d.embedding for d in resp.data]\n",
    "\n",
    "# =========================\n",
    "# 4) Load CSV -> Article ë³µì› -> ì¬ì²­í‚¹\n",
    "# =========================\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "required = [\"source\", \"law_type\", \"article\", \"chunk_id\", \"text\"]\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"âŒ CSVì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing}\\ní˜„ì¬ ì»¬ëŸ¼: {list(df.columns)}\")\n",
    "\n",
    "# chunk_id ì •ë ¬ìš©\n",
    "df[\"chunk_id_num\"] = pd.to_numeric(df[\"chunk_id\"], errors=\"coerce\")\n",
    "df = df.sort_values(by=[\"source\", \"law_type\", \"article\", \"chunk_id_num\"], na_position=\"last\")\n",
    "\n",
    "# (source, law_type, article) ë‹¨ìœ„ë¡œ ì¡°ë¬¸ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°\n",
    "articles = []\n",
    "for (source, law_type, article), g in df.groupby([\"source\", \"law_type\", \"article\"], dropna=False):\n",
    "    parts = [norm_space(t) for t in g[\"text\"].astype(str).tolist() if norm_space(t)]\n",
    "    full_text = norm_space(\"\\n\".join(parts))\n",
    "    if len(full_text) < 50:\n",
    "        continue\n",
    "    articles.append({\n",
    "        \"source\": source,\n",
    "        \"law_type\": law_type,\n",
    "        \"priority\": get_priority(law_type),\n",
    "        \"article\": article,\n",
    "        \"full_text\": full_text,\n",
    "    })\n",
    "\n",
    "articles_df = pd.DataFrame(articles)\n",
    "print(\"âœ… ì¡°ë¬¸(article) ë³µì› ê°œìˆ˜:\", len(articles_df))\n",
    "\n",
    "# ì¬ì²­í‚¹í•˜ì—¬ ì—…ì„œíŠ¸ ë ˆì½”ë“œ ìƒì„±\n",
    "MAX_CHARS = 1400\n",
    "OVERLAP_SENTS = 2\n",
    "\n",
    "records = []\n",
    "for row in articles_df.itertuples(index=False):\n",
    "    sents = split_sentences_kr(row.full_text)\n",
    "    if not sents:\n",
    "        continue\n",
    "    chunks = chunk_by_sentences(sents, max_chars=MAX_CHARS, overlap_sents=OVERLAP_SENTS)\n",
    "\n",
    "    for i, ch in enumerate(chunks):\n",
    "        records.append({\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"category\": \"law\",  # í•„ìš”í•˜ë©´ ì—¬ê¸°ì„œ ë°”ê¿”ë„ ë¨\n",
    "            \"priority\": int(row.priority),\n",
    "            \"law_type\": row.law_type,\n",
    "            \"article\": row.article,\n",
    "            \"chunk_id\": i,\n",
    "            \"source\": row.source,\n",
    "            \"text\": ch,\n",
    "        })\n",
    "\n",
    "out_df = pd.DataFrame(records)\n",
    "print(\"âœ… ì¬ì²­í‚¹ ê²°ê³¼ chunk ìˆ˜:\", len(out_df))\n",
    "print(out_df.head(3))\n",
    "\n",
    "# =========================\n",
    "# 5) Upsert to Pinecone (realestate)\n",
    "# =========================\n",
    "BATCH = 64\n",
    "\n",
    "def upsert_df(batch_df: pd.DataFrame):\n",
    "    texts = batch_df[\"text\"].tolist()\n",
    "    vecs = embed_batch(texts)\n",
    "\n",
    "    to_upsert = []\n",
    "    for vec, r in zip(vecs, batch_df.to_dict(\"records\")):\n",
    "        md = {\n",
    "            \"category\": r[\"category\"],\n",
    "            \"priority\": r[\"priority\"],\n",
    "            \"law_type\": r[\"law_type\"],\n",
    "            \"article\": r[\"article\"],\n",
    "            \"chunk_id\": r[\"chunk_id\"],\n",
    "            \"source\": r[\"source\"],\n",
    "            \"text\": r[\"text\"],   # âœ… ë°œì·Œìš© í•µì‹¬\n",
    "        }\n",
    "        to_upsert.append((r[\"id\"], vec, md))\n",
    "\n",
    "    index.upsert(vectors=to_upsert, namespace=NAMESPACE)\n",
    "\n",
    "# ì‹¤í–‰\n",
    "total = len(out_df)\n",
    "for start in range(0, total, BATCH):\n",
    "    batch_df = out_df.iloc[start:start+BATCH]\n",
    "    upsert_df(batch_df)\n",
    "    if (start // BATCH) % 10 == 0:\n",
    "        print(f\"  ... upsert {start + len(batch_df)}/{total}\")\n",
    "\n",
    "print(\"âœ… ì™„ë£Œ! ì¸ë±ìŠ¤:\", INDEX_NAME, \"| namespace:\", NAMESPACE)\n",
    "print(\"ğŸ‘‰ í™•ì¸:\", index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c3a6fe-c328-48a0-95ad-34b1a32a3bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b446b27d-48eb-454a-9b91-e802d519d2db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm(ipykernel)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
